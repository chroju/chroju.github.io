<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post on Arthur Dent</title>
    <link>https://you.github.io/post/</link>
    <description>Recent content in Post on Arthur Dent</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 20 Nov 2017 21:51:51 +0900</lastBuildDate>
    
	<atom:link href="https://you.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://you.github.io/post/test/</link>
      <pubDate>Mon, 20 Nov 2017 21:51:51 +0900</pubDate>
      
      <guid>https://you.github.io/post/test/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AlexaにAWSの請求額を教えてもらう</title>
      <link>https://you.github.io/post/hello_alexa/</link>
      <pubDate>Sat, 18 Nov 2017 23:23:00 +0900</pubDate>
      
      <guid>https://you.github.io/post/hello_alexa/</guid>
      <description>Amazon Echo Dotを書いたのでさっそくオリジナルスキルを作ったのだけど、アイディアに乏しいマンなので、「自分が使っているAWSアカウントの請求額を教えてもらう」という、あんまり夢のないスキルが出来上がった。想定される発話の一覧を作っているとき、「請求額を教えて」とか「支払いを確認して」とかいっぱい書いてて、若干つらい気持ちになったのだけど、取りあえず作るのが目的だったのでそこは気にしない。
ライバルであり、先に国内発売していたGoogle Homeは買っていないので、比較はできていないのだけど、Alexaの強みはLambdaを直接叩けることだと思う。API Gatewayを準備してHTTPSのエンドポイントを渡す、とかではなくて、LambdaのARNを指定して直接連携ができるのでとてもシンプル。既知の技術を活かせることもあって、開発の難易度は高くない。面倒なのはデバッグで、声に出して全機能試してみるというのはわりとまどろっこしくてだるい。
チャットボットフレームワークであるAmazon Lexや、音声変換技術であるAmazon Pollyが公開された時点で意図がわかってはいたけど、Alexaの開発にはバックエンドにAWSを存分に使ってくれということなんだろう。Google Homeの強みが、Google Assistantという高機能AIにあるとしたら、Amazonの方の強みは長年培ってきたクラウドサービスノウハウとのシームレスな連携にあるような気がしている。
Alexaの開発 Alexa開発はデモをしても音声なので、動画か音声ファイルを貼る必要があるのがちょっとつらい。一応アプリにも会話の記録が残るので、今回はそれを貼ることにする。こんな感じで、サービスごとの請求金額や、請求金額の合計を教えてくれる。

開発にまつわる技術的な話もしようと思っていたのだけど、Alexa Skill開発入門みたいなことは、書いたら長くなってしまったのでQiitaに投稿しておいた。そちら参照。
 Amazon Echo (Alexa) の開発に必要な概念について - Qiita  ここにはLambdaのコードとIntent Schemaだけ貼っておく。Lambdaの方はblueprintで用意されている「alexa-skills-kit-color-expert-python」を少しいじっただけなので、そんな大したものではないというのと、Python2.7のままですというのを断っておく。逆に言えば、用意するものはこんなもんだけでAlexaのSkillは作れてしまう。
 like a hubot ? ところで人間的な応答を返す会話インターフェースとなると、hubotを始めとするチャットボットを彷彿とさせる。hubotの登場後は、単純な雜談や情報連携から、CIとの連携、監視との連携、さらにはbotを使ったdeployなどもチャットで行われるようになった。ではAlexaはどうなのか。開発の現場でも似たように使えるんじゃないか。
Lambdaと連携できるということは、自社のAWSアカウントのAPIを叩かせることもできるわけで、例えば「Alexa, サーバー台数を100台に増やして」みたいな「ボイスオペレーション」も可能ではある。突然Yahoo!砲のような大量のリクエストが来てワタワタとオペレーションしているときに、片手間で呼びかけるだけで操作を肩代わりしてもらえるなら、ちょっと便利かもしれない。
ただ冷静に考えるとやっぱり無理かなという思いがある。ひとつは文字が使えないこと。チャットボットは文字で指示する、要はCLIの派生みたいなものだったので、記号的なidだとか、固有名だとかでも違えず引数として与えることができた。対してAlexaは音声を解釈するので固有名詞や記号的文字列はどうしても苦手であり、「AutoScalingグループのhogefugaを」などと指示しても、たぶん読み取ってはくれない。
もうひとつは権限の問題。強力なオペレーション機能を持ったhubotは、専用のchannelだけに置いたり、特定のユーザーだけに反応するように組むことで、「権限」の制約ができたのだけど、Alexaは声紋認証するわけでもないので、anonymousに命令を受け付ける。なのでさっき書いたような「起動サーバー台数を増やす」機能を持ったAlexaなど、オフィスに置いたら危なっかしくて仕方ない。てわけで、ボイスオーバーオペレーションはちょっとだけ夢見たのだけど、今のとこ無理そうかなという気がしている。小規模ベンチャーとかで限定的に導入したらテンション上がるかもだけど。
Alexaブーム来てほしい 繰り返しになってしまうけど、開発プラットフォームがAWSという既知の領域に開放されたことは結構大きいと思っていて、ぶっちゃけた話企業によるスキルの公開を待たなくても、巷のAPIと連携する形で必要なものはどんどん作れてしまう（もちろん、企業側とバッティングすれば審査で弾かれるんだろうけど）。スマートスピーカー、いざ使ってみると案外手の届かないことが多くてそのうち使わなくなる、みたいなことを想定したりもしていたのだけど、自分の好きなようにスキルが作れるなら、いくらでも使いようがあるなという気がしている。個人的にはAlexaスキル作成ブームが来ることをちょっと期待したい。お前も書けって話だと思うので、機会があればまた書きたいとも思う。</description>
    </item>
    
    <item>
      <title>Scrapbox.ioで思考を気持ちよく繋げる</title>
      <link>https://you.github.io/post/i_love_scrapbox/</link>
      <pubDate>Sun, 08 Oct 2017 22:31:00 +0900</pubDate>
      
      <guid>https://you.github.io/post/i_love_scrapbox/</guid>
      <description>Scrapbox.ioを最近使っていますが本当にいいのでいいという話をします。
そもそもScrapboxって何かという話から言えば、いわゆるオンラインでメモが取れる、ノートが書ける、スニペットが保存できるという類のサービスです。Gyazoを運営しているNOTA Incによる開発です。
何がいいかと言えば、これは記録するためのメモツールというより、思考のためのメモツールだと言うことです。NOTAのshokai氏のエントリーでも以下のようなものがあります。
思考をブロックする要素を減らしたい - 橋本商会 - Scrapbox
Scrapboxが「思考のためのツール」たる特徴はいくつかあるのですが、Wikiで良くあるシステムのように、本文中にブラケットで囲んだ単語を入れると、それがリンクとなる機能があります。それだけだとまぁ普通なんですけど、ノートを書いているときにこのリンクを作ると、リアルタイムにその単語に対してリンクしたページがバババッとページの下に羅列されるのがとても気に入っています。

こんな感じ。面白いのはリンク先のページのみならず、リンク先にリンクしたページ、つまり2ホップ先まで表示される点です。Christpher Nolanについて書こうとしたら、過去に彼の監督作で『ダンケルク』と『インターステラー』についても書いていたんだなぁってのが自動的に想起され、今回何かを書くにあたり、過去に何書いてたんだっけというのを見ながら編集ができたりとか、そういう「過去の自分との連想」ができる。
自分の場合は固有名詞はとにかく端からリンクにしてるんですが、そうすると自分でも忘れていた、過去に同じテーマに関して書いたメモがどんどんピックアップされてつながっていくんですよね。この手の「発想を繋げる」方法論、従来では情報カードを使い、机上でカードを広げてやる方式が梅棹忠夫氏の『知的生産の技術』に載っていたりして有名ですが、まぁ手間はかかるし、確実ではない。得てして脳の「想起」の機能はランダムなものだし、カードでそれを補強しても完全ではない。Scrapboxは自動で漏れなくそれを実現してくれるのがとてもいい。
メモの整理で「タグを付ける」という方法論自体も古くからあるものですけど、メタ情報として付けるのではなく、単に文章を書きながら目についた固有名詞を囲んでいけばいいだけなので、思考を阻害されることなくタギング出来るってのもだいぶ気に入っています。
自分はそれほどスペックのいい脳味噌を持ってないんで、思考と記憶が外部化されて、さらにそれらが「勝手に繋がる」のが気持ちいい。気持ちいいので現状プログラミング以外のことでもとにかくぶち込んで繋げて遊んでます。オススメです。</description>
    </item>
    
    <item>
      <title>CoreOSからECSへDockerを移行した</title>
      <link>https://you.github.io/post/migrate_coreos_to_ecs/</link>
      <pubDate>Tue, 26 Sep 2017 08:36:00 +0900</pubDate>
      
      <guid>https://you.github.io/post/migrate_coreos_to_ecs/</guid>
      <description>CoreOS（現状Container Linuxですけど、キャッチーじゃないのであえて）上でプライベートのDockerコンテナを動かしていたのですが、それをAmazon ECS上へ移行した話です。
従来の環境 従来は以下のような環境を使っていました。
 さくらクラウドでCoreOSを起動（イメージがデフォルトで用意されていて嬉しい） コンテナは個人的なログ用influxDB、Grafana、個人slack用hubotの3つ。 オーケストレーションツールは使っていない（docker composeで管理、systemdで自動起動） それ以外に運用っぽいことも特にしてない。  プライベートかつ、とりあえずDocker触って動かしたいという理由だけで作った環境だったので、正直扱いは雑でして、たまにコンテナ落ちてて、hubotに呼びかけてから気付くみたいなこともままありました。
それが地味にストレスだったのと、せっかくDocker使うならもうちょいオーケストレーションとかちゃんとしたいってことで移行を決断しました。
移行先の検討 CoreOSは従来であればfleetという独自のオーケストレーションツールを持っていましたが（今も正確には現役ですが）、このリンク先にもある通り、現状はk8s使えってことになってます。
なのでさくらクラウド続投してk8sで頑張るというのも手ではあったのですが、仕事ではAWS使うことが多いというのと、個人的にここ最近聴いたDockerのproduction利用の話がだいたいECSだったのとで、管理も楽そうだしECSでいいかなという感じでした。
ただ、k8sであればオンプレやら何やらといった環境を問わずに運用が可能だし、クラウドに載せたければGKEでそのノウハウ活かせるしという利点があり、長期視点で汎用性の高い技術選択、という視点だとk8sの方がいいのではと思ったりしてます。
ECSの概念 簡単に触れておきます。
cluster コンテナをホストするEC2インスタンス群。
task definition cluster上でコンテナをどう配置するかという定義。
task task definitionに基いて稼働するコンテナ群。
service taskを永続的なプロセスとして稼働させるECSの機能。
taskは「service」として動かすことで、コンテナが落ちても勝手に上げ直してくれるようなイイ感じのやつになります。serviceの定義の中でAuto Scalingの設定したりとかもできるらしいですが、今回そこまでやってないです。要らないし。
移行作業 ぶっちゃけ、すごく簡単でそんなにやったことはないです。
はじめはTerraformで環境作るかとか思ってたんですが、tfファイルをゼロから書くのがだるくなったので他にも手段があるんじゃないかと調べていたところ、ECS CLIというAWS提供のコマンドツールがありました。
で、これがtask definitionとしてdocker-compose.ymlがだいたいそのまま使えるんですね。。なのでdocker-compose.ymlからTerraformへ定義を書き写すような面倒はかけず、移行が出来てしまうわけです。実行したコマンドはこれだけです。
# clusterを起動 $ ecs-cli up --keypair hoge --size 1 --instance-type t2.micro --capability-iam --port 8080 3000 8086 # docker-compose.ymlからserviceを起動 $ ecs-cli compose --file docker-compose.yml service up  これだけでもう、コンテナはほぼ永続的に稼働します。便利ですね。今回は個人用なのでt2.microを1台しか上げてないですが、複数台インスタンスを上げればコンテナはイイ感じに分散してくれるみたいです。
ただ、ecs-cli upで賄えるのはあくまでEC2の範囲なので、ALBを使いたいですとか、EFSで各インスタンス間のデータを共有したいですとか、そういう要望がある場合にはマネジメントコンソールなりTerraformを使う必要があります。あくまで「とりあえずコンテナを動かす環境が何かあればいい」ぐらいの目的で使うものかなと。
まぁ自分の場合はプライベート用途で複雑な要件もなかったので、ECS CLIで十分です。構成管理という面でも、先のコマンドを書いたシェルスクリプトとdocker-compose.ymlさえあれば環境再現はすぐできるので、拍子抜けするほど楽でした。</description>
    </item>
    
    <item>
      <title>AWS Certified Sysops Administratorを取得した</title>
      <link>https://you.github.io/post/aws_sysops_administrator/</link>
      <pubDate>Sat, 26 Aug 2017 12:04:57 +0900</pubDate>
      
      <guid>https://you.github.io/post/aws_sysops_administrator/</guid>
      <description>AWS認定Sysopsアドミニストレータという資格を取得しました。AWSには5種類の認定資格がありまして、レベル的には「アソシエイト」と「プロフェッショナル」の2段階に分かれるのですが、これは下位にあたる「アソシエイト」のものです。本当は「プロフェッショナル」にあたるDevOpsアドミニストレータがほしいなと思ったんですけど、受験資格としてSysopsが必要だったのでまずは、という感じで取りました。
準備したこと 試験要項には「モニタリングとメトリクス」とか「ネットワークの基本的な知識」といった抽象的な試験範囲は書かれているんですが、具体的にどのAWSサービスが出題されるかは書かれていないので正直勉強しづらかったです。ウェブで漁ればある程度受験記を書かれている方がいるので読んどいた方がいいのと、実際の問題に慣れる意味でサンプル問題は確認しておくこと、あと模試が2000円ぐらいで受けられるので受けとくのは必須です。サンプル問題も模試も先のリンク先から飛べます。
実際受けたところだと、Sysopsで出題されるサービスは以下あたりという感触。
 EC2 VPC RDS S3 Route53 ELB(Classic, ALBいずれも) Auto Scaling EBS Elastic Beanstalk ElastiCache Opsworks CloudWatch CloudFormation  ここに並べた各サービスについて、AWS クラウドサービス活用資料集 | AWSに存在するBlackBeltの資料を読み、その上でざざっと公式のドキュメントを眺めました。たまによく読まないと解けない引っ掛けはありますが、重箱の隅をつつくような変な問題はないので、きちんとサービスの仕様とプラクティスを押さえれば得点はできます。準備期間は1か月ぐらい？
ただ、自分の場合はこの大半のサービスを実際に使った経験があるので、それにより得点できた感は否めないです。逆に触ったことがないElastiCacheあたりは弱かったりしたので、触っとくか、それが難しい方ならAmazonが開催しているトレーニングを受講した方がたぶん無難です。
問題の傾向 実際に出る問題はユースケースを想定したものが多く、丸暗記でどうにかなる感じではないです。
 構成に関する情報が記載され、そこからシングルポイントを見つける問題  IGWやVGWやNGWとか、AWSマネージドの範囲がどう冗長化されてるか答えられないといけない  RDSやEC2のインスタンス障害時の想定動作 欲しいメトリクスをAWSサービスを使って取得、通知する方法の考案 ネットワークの疎通ができない際や、CPU等がサチってるときの対処方法を考えさせる問題 クライアントが保存するデータの高セキュリティな保管、読み出し方法を考えさせる問題 負荷上昇を見据えて、可用性と金銭的コストを踏まえた適切なスケーリング設計の作成 既存のオンプレミスデータセンターと連携した上で、高可用なシステムを組む問題  こんな感じですかね。漫然とAWSを使っているとわからないけど、EBSの種類をちゃんと吟味してるかとか、SPOFが無いかどうかつぶさにチェックしているかとか、そういう点で差がつく気がします。
今後 冒頭にも書いたとおりDevOpsも受けようと思います。が、この手の資格にはつきものの2年ごとの更新が面倒だなぁという気もしたりしてどうも。AWSは必要なサービスを必要なだけ使えればそれでいいものだとは思いますが、網羅的に仕様を押さえてどっぷりAWSを使いこなしたいという場合には、体系的な知識獲得に悪くない資格だと感じてます。</description>
    </item>
    
    <item>
      <title>builderson2017に行ってきた</title>
      <link>https://you.github.io/post/builderson_2017/</link>
      <pubDate>Sun, 06 Aug 2017 21:25:43 +0900</pubDate>
      
      <guid>https://you.github.io/post/builderson_2017/</guid>
      <description>builderscon Tokyo 2017に行ってきた。最初会場をSFCと勘違いしてましたが、慶應の日吉の方でした。日吉駅周辺、大学を前提に作られている放射状の道が美しいですね。
 会場になっていた協生館は大学施設というよりは少し街に開かれているのか、コンビニがあるのはもちろん、驚いたことにHUB（スイッチングじゃなくて酒飲めるアレ）があったり、ホールには全席電源とLANポートが完備されていたり、IT系カンファレンスには最適の場所じゃないかという声をTwitterで見かけたりしました。とてもいいとこでした。
ビルコン初めてでしたけど、名前通りにと言うのか、まさに何かを「build」しましたという発表が多かったり、テーマでもある「知らなかったを聞く」機会に多く恵まれてよかったです。自分はOps側の人間なので、それほどbuildするという機会が多くはないんですけど、あんまりそこにハードル高く考えず、なんかやってみたいなと思ったら稚拙でもいいから作ればいいんだなという感覚が強くなった気がします。
あと完全に私事ですが元同僚と遭遇したりして、前職と今とで全然やってること違うのにこういう機会があるってこと、ジャンルの壁を設けない横断的なカンファレンスってやっぱいいなと思いましたね。会社という枠が良い意味でそれほど強い意味を持たない、というのはこの業界の好きなとこです。
今回は以下のセッションを聴きました。いずれもbuildersconのページからスライド資料に飛べるはずです。
 横山三国志に「うむ」は何コマある？〜マンガ全文検索システムの構築  この週末だいぶバズったアレです。 コマの切り取り、OCR、全文検索、ウェブページのフロント、それぞれにOSSを的確に使っていて、発表自体もすごく参考になりました。 個人利用できて楽しいことできるAPIとか結構あるんやんと思った。APIに自分でも触ってみたくなるセッション。  Anatomy of DDoS  CloudFlareの中の人によるゲストセッション。 DDoSと一口に言ってもOSI7層それぞれに手法と対応があるとか、IoTでデバイスばらまかれるとそれだけセキュリティの穴も増えるよねみたいな基礎から丁寧な話。  RDBアンチパターンリファクタリング  今回のベストトーク賞。 RDBに限らずリファクタリング全般に言えるYAGNIの話、技術的負債のうちどれを優先して返済するのかみたいな視点が参考になった。  LT  QRコードは分割できるという話がマニアックながら何かに使えそう。 もうマジで何でもいいから作ればいいじゃんという感を覚えた。作るは正義。  サーバーサイドKotlin  Kotlinのnullable型がよいという話。  OSSで始めるセキュリティログ収集  osqueryの話 OSからログ、メトリクス情報を吸い出す手段って対象ごとにバラけてて面倒だなと思ってたんだけど、それに対する1つの回答になりそう。  OSS貢献超入門  がんばりましょう。。。時間つくりましょう。。。  Serverless Server Side Swift  Swiftはバイナリで動いてランタイム要らないのでAWS Lambdaで使えるって話。 Lambdaのランタイム制約しんどいって話はよく聞くけど、正直バイナリそのまま突っ込んでまで別言語使いたいという気持ちはいまいちわからない。  ここが辛いよサーバーレス。だが私は乗り越えた  サーバーレス実運用した上でのノウハウの話。 フェイルオーバーの実装とか、テストのときの環境エミュレートとか。つらいのとてもわかる。   </description>
    </item>
    
    <item>
      <title>Toilの地獄から抜け出す術を知りたい</title>
      <link>https://you.github.io/post/very_exhausted_because_of_toil/</link>
      <pubDate>Mon, 12 Jun 2017 23:35:55 +0900</pubDate>
      
      <guid>https://you.github.io/post/very_exhausted_because_of_toil/</guid>
      <description> 最近とても停滞感があってよろしくないので一度ブログに吐き出すなどします。
Toilが多すぎる 現状、自分はオペレーションエンジニア（決まった手順しかやらないオペレーターではなくて、問題解決や運用体制の構築とかやる感じの）という位置付けで働いていて、日々のシステム運用に関するタスクをもろもろ片付ける他に、長年澱のように蓄積された技術的負債とか、暗黙的な運用を改善していくことも任務だったりします。Googleが名付けたところで言えば前者がToilってやつですね、いわゆる。自動でできるはずなのにマニュアルで行われていて、サービスの増加に比例してどんどん業務を圧迫していくような類の仕事。
で、SRE本ではToilを日常の仕事の5割以下に収めろと言ってるんですが、全然自分の場合収まっていない。TodoistやTogglで最近はToilが1日に占める割合を計測しているのだけど、いずれもほぼ8〜9割。実感としても将来的な投資に費やせている時間ってのがほとんどないです。
実際のところ何がToilとして積み重なっているのかというと、手動でコマンドを打つような作業がほとんど。弊社では基本的に本番稼働中のシステムでオペレーションする人間を限定しているので、デプロイや設定の確認（ドキュメントがなかったりバージョン管理してなかったりするので、実機確認が必要になる機会が多々ある）等々自分らがやらねばならんのですが。しかし手動でやることがあまりに多い。数年に渡ってエンジニアのUNIXコマンドスキルに依存していた運用のツケが回ってきている。ある程度自動化すればいちいち手を動かさなくてよかったり、サーバーの状態、設定を確認するような作業であれば適切な可視化をすれば済む話であったり、解決策は見えているのだけど、手を打つ方に時間を回せていない状態。それなりに「どうすればいいか」がわかってはいるので、その分余計に「改善に時間が回せない」ことへの歯がゆさが強くてメンタルに優しくなかったりしている。
改善に必要なスキルは多方面に渡る DevOpsなんて言葉はもう枯れていて、SREという職種も珍しくなくなってきた昨今、先に挙げたようなありがちな技術的負債に対する汎用的な解決策は世の中に溢れていて、自分も見知ってはいる。でも、それを実際にどう現場へ適用していくかが本当の課題。
そもそもの時間が取れないってのはどしたらいいんですかね。『エッセンシャル思考』よろしく、本当に必要な依頼以外は「断る」というのは1つの選択かもしれない。うまいことやらないと怒られそうだけど。個人的な思いとしてはある程度残業時間増やしてでも時間確保するしかねーなってのと、あとは改善を進める効率を上げることなのかなと。例えばチームで使っているツールのAPIを叩くラッパースクリプトを書いたりとかよくしてるんですが、自分はお世辞にもプログラムを書くのが速い方ではないので、最近は1年ぐらいプログラマーとして働いてスキル上げたいなと思うぐらい。
時間が取れたとして、どうするか。改善は大きなことをいきなりやろうとしてもうまくいかないので「小さく始める」というのはよく言われることですが、小さく始めるにしても自分の業務だけが楽になるんじゃ意味がないから、自ずと他の人を何かしらは巻き込む形になる。なのでコミュニケーションスキル、と言わずともプレゼンテーションスキルは少なくとも必要。そう、技術的な解決策をいくら知っていてもそれだけではダメで、やはり組織でやっていく以上、当たり前の話として対人的なスキルは不可欠になるんですよね。自分が楽になるから、確実性が上がるから、ではなくて、双方ハッピーになれるという話をしなくてはならない。これが結構難しくて。単に設定をバージョン管理して可視化します、という話であっても、元々その設定を確認して開発に活用していた人たちの業務プロセスには変化が生まれる。それをメリットとして受け入れてもらえるか。改善を進めるというのを自分は十字軍にはしたくないと思っていて、そのへんのバランスには常々気を遣っていたり。
あとは自分の現場のように多方面に様々な課題が山積していると、その中のどの課題をまず解決するか、どのように解決するかという取捨選択も重要。局所最適にならず、なるべく全体最適で、さらにレバレッジを効かせられるようなことからどんどんやっていくべきで、些末なことに囚われるべきではない。どうしても課題をパラパラ並べていくと、手を付けやすそうなこととか、技術的に面白そうなことからやりたくなっちゃうんだけど、そこをグッと我慢しなければならない。
つわけで改善、と一言で言っても、求められるのは純粋なコンピュータに関する技術、だけではなくて、社会学や人文学がやっぱり必要になってくるなぁというのを実感する日々です。そしてそのどれもが自分には足りていないような気がして、強く閉塞感を覚えてしまっているのも事実であり。課題は一度バラして並べてあるので、それがなかなか崩れていかないのがとてももどかしくてしんどい。
あらゆる情報の流れを制御したい まぁしんどい話ばかりしていても仕方ないので具体的にやっていることも書きます。。だいたいは散逸してしまっている運用を統制していこうという向きが強い。
 時系列データベース（主にinfluxDB）を用いたメトリクスデータの集約。 あらゆるもののバージョン管理、コマンドによるオペレーションの推進、必要に応じてAPIをラップしたCLIツールの作成。
 Ansibleやfabricでの構成管理、デプロイの統一。 言語はだいたいPythonを使っているけど、クロスコンパイルしてバイナリ配布できるGoに興味がある感じ。 Infrastructure as Code記載の構成レジストリ（サーバー情報を集約したデータベース）やKVSによる設定情報の統制管理。 あとは人間のオペレーションをどうコントロールできるか、という視点から認知科学やデザインを少しかじったり。  とにかく暗黙知に頼った運用が多すぎて、結果として「運用でカバー」せざるを得なくなってToilがかさむ、という本当にしんどい循環をしてしまっているので、様々な情報の集約、可視化、管理をしたい感じ。たぶん自分が興味あるのは情報の流れをつくる、それに乗せることで人間の仕事を最適化していくということっぽいんですよね。それができるのはとても楽しくはあるけど、つらくもあり。結果技術というよりプロセスの話を最近多く学んでいるのでブログも書けてなかったり。そういや暗黙知つながりでポランニーの『暗黙知の次元』読んだりもしてるのだが、もはや自分が何屋かわからんな。
ちょっと正解が何なのかよくわからなくなってきたので、一旦立ち止まって冷静にならないとダメそう。
  </description>
    </item>
    
    <item>
      <title>O&#39;Reilly『Infrastructure as Code』読了</title>
      <link>https://you.github.io/post/infrastructure_as_code_book/</link>
      <pubDate>Fri, 05 May 2017 17:53:22 +0900</pubDate>
      
      <guid>https://you.github.io/post/infrastructure_as_code_book/</guid>
      <description> O&amp;rsquo;Reillyの『Infrastructure as Code』を読んだ。従来Ansibleの本、Chefの本といった各ツールの使い方にフォーカスした本はいくらか出ていたけれど、おそらく「Infrastructure as Code」の考え方や、システム運用への採用方法を包括してまとめた本は初めてということになるのかな。
ツールとプロセス 端的に内容を説明してしまうと、Infrastructure as Codeで使用するツールを
 ダイナミックインフラストラクチャプラットフォーム（AWS等パブリック、プライベート問わずプログラマブルな操作が可能なインフラプラットフォーム） インフラストラクチャ定義ツール（Terraformのような、構築するインフラ環境を定義するツール） サーバー構成ツール（Ansible、Chef等のサーバー内部の設定を定義するツール） インフラストラクチャサービス（モニタリング、サービスディスカバリ等の環境管理用のツール）  の4つに大きく分類して、
 インフラのプロビジョニング サーバーテンプレートの管理 サーバーのアップデート、更新 サーバーの破棄  といったライフサイクルを、各ツールを使ってどう管理するかを説いた本。後半で第10章をまるごと「インフラストラクチャのためのソフトウェア工学プラクティス」の説明に充てて、継続的インテグレーションやVCSの方法論を書いていたりして、DevOpsの名のもとにソフトウェア管理に対して導入されてきた手法を、インフラへも適用していくための本になっている。なのでサーバーを構築したらそれでおしまいではなくて、ダイナミックにサーバーが生成／廃棄される環境であれば、自動的にログは退避できるようにしておく必要があるだとか、システムマネジメント全体に焦点が当てられている。
ツールの使い方自体はドキュメントを読むなり、Qiitaを探るなりすればだいたいわかるんだけど、それをどう運用すれば効率的になるのか、より効果が得られるかという、プラクティスを知りたい思いが強かったので、大いに参考になった。
徹底的な自動化アプローチ 例えばプロビジョニングは「インフラストラクチャ定義ツール」でサーバーを構築し、その内部を「サーバー構成ツール」で設定するわけだけど、2つのツールを個別に手動実行するのではなくて、前者から後者へどう情報を受け渡して、自動的にプロビジョニングを済ませるかが肝になる。
Terraformであれば、EC2を立てた後に自動的にAnsibleを呼ぶようなことも機能でできるらしいけど、そういった連携の仕組みがツール自体に存在しなければ、構築したサーバーのIPアドレスやユーザーの情報を「サーバー構成ツール」へ渡す必要がある。1例として挙がっていたのが「構成レジストリ」と呼ばれる、サーバー情報を中央集権的に管理するツール。インフラストラクチャ定義ツールは立てたサーバーの情報をここに登録し、サーバー構成ツールが情報を引き出して、サーバー設定のデプロイを行う。具体的に「構成レジストリ」として使えるツールについてはそれほど言及がなかったけど、CygamesだとElasticsearchを使ってサーバー情報を管理しているという話を聞いたことがあって、実装としてはこれに近いのだと思う。
 あるいはAnsibleなどを導入したときによく聞かれるのが、結局手動でサーバー構成を変更してしまって、Playbookと実情に乖離が出てしまうという問題（本書ではこれによる構成差異の発生を「構成ドリフト」と呼んでいる）。これに対してはイミュータブルなサーバー管理をするのが最も良いとされる。なぜなら構成ドリフトを起こしても、定期的にイチから生成したサーバーに置き換えられていれば、そのたびに構成ドリフトもリセットされるから。
この本全体で貫かれているのは、とかく徹底的な自動化と、ワークフローのパイプライン化を行なって、各タスクからシステム管理者の手動オペレーションをなくすということ。これによりシステム管理者はインフラ構築、変更の「門番」としての役割から解放され、問題への対応といったより生産的なタスクへ移ることができるし、システムの変更によりサービスに対して発生する影響も低減できるとしている。
 自律的なオートメーションこそ、Infrastructure as Codeが確実に機能する秘訣である。チームがセキュリティを向上させる新しいウェブサーバーの設定オプションを見つけたら、それをオートメーションツールのなかに組み込む。チームは、もう1度そのことについて考えようとしなくても、現在及び将来のすべての関連サーバーにそれが適用されることを知っている。 (p.254)
 共通言語として利用したい 先述した「構成ドリフト」のように、本書ではいくつかあまり聞き慣れない用語（構成ドリフトを起こしたサーバー群を「スノーフレークサーバー」と呼んだり、イミュータブルに何度も破棄されては生成されるサーバーを「フェニックスサーバー」と呼んだり）が使われているのだけど、アンチパターンやグッドプラクティスを共有認識にする上では、名前がついているほうが話が通じやすいので、この本の用語が共通言語になっていくとうれしいなと思ったり。そういえば、プラクティスに名前を付ける手法は『Fearless Change』でも使われていた。
で、結論としてとても良い本だとは思うのだけど、これを全部導入するとなるとなかなかにパワーが必要になる。特に、すでに手動でのサーバー管理を絶賛実施中の環境に対して導入する場合には、インフラに対する考え方自体を、秘伝のタレのように継ぎ足して使うものから、外部で管理している設定と常に同期して使うもの、すぐに破棄と生成のサイクルを回せるものへと変えていかなくてはならないので、組織的な協力が必要になってくる。しかも必要なのは一部ワークフローの自動化ではなく、徹底的な自動化、インフラ環境の自律的な動作。
まぁ、とはいえインフラパイプライン全体をいきなり用意して導入するのも難しいのは確かなので、まずは頻繁に変更が入る設定など、Infrastructure as Codeの恩恵が大きそうなところから始めてみるべきかと思う。少しずつでも確実にこれは実現していきたいなと思える本だった。
Infrastructure as Code ―クラウドにおけるサーバ管理の原則とプラクティスposted with amazlet at 17.05.05Kief Morris オライリージャパン 売り上げランキング: 25,983
Amazon.co.jpで詳細を見る </description>
    </item>
    
    <item>
      <title>VimperatorからVivaldi &#43; Vimiumへ乗り換える</title>
      <link>https://you.github.io/post/vimperator_to_vivaldi/</link>
      <pubDate>Wed, 15 Feb 2017 23:30:43 +0900</pubDate>
      
      <guid>https://you.github.io/post/vimperator_to_vivaldi/</guid>
      <description>Firefox 51から、Vimperatorの一部機能が使用できなくなる状態が発生した。これについてはGitHub上でFixが進んでいるが、2017-02-15現在だとまだ修正版の公開までには至っていない。
 Issues · vimperator/vimperator-labs  もう数年間vimpを使っている身として、一時は修正版を待とうかとも思ったのだが、今年11月からWebExtentionに移行する際にも同様の混乱が様々ありそうだし、代替策の模索を始めた。
Why Vivaldi ? とはいえ、今の時代に代替となるブラウザはほぼChrome一択になってしまう。ChromeにもVimライクなKey configを実現するVimiumがあり、触れたこともあるのだが、自分はどうもChromeが苦手で移れずにいた。
一番大きいのがUIをカスタマイズする余地がないこと。例えばFirefoxだと多段タブをTab Mix Plusで実現しているのだが、Chromeだとこういったことはできず、大量のタブを開くとファビコンのみの表示に縮小されてしまう。またアドレスバーが上部で固定されているのも、個人的には邪魔くさい。
そこでVivaldiを採用しようと考えた。VivaldiはChromeエクステンションが互換動作するのでVimiumも使える上、UIがかなりフレキシブルにカスタマイズできる。

多段タブこそできないものの、タブバーを上部ではなく横に表示することで、ファビコンだけに縮小されることは免れた。アドレスバーも要らないと言えば要らないのだが、SSLの確認用途もあるので下部表示へ変更している。
Vimperator to Vimium まずそもそもなのだが、VimiumではVimperatorの完全な代替には成り得ない。Vimperatorの真髄はjavascriptを介してOSの挙動を骨の髄までコマンド操作に置き換えるところにあったわけだけど、Vimiumでコマンド操作可能なのは、あくまでエクステンション内部で定義されているものに限られる。Vimperatorのような好き勝手はできない。
なのでVimperatorで自分にとってのキラーとなっていた機能だけでも移植を検討した。
Key mapping key mappingは自由に替えられるので、自分の.vimperatorrcに合わせる形で変更した。
なおctrlキーを交えたマッピングを行う場合、ブラウザデフォルトのショートカットと衝突する恐れがあるが、Vivaldiはショートカット定義を自由に替えられるため、キーバインドを解除すれば衝突しなくなる。
unmap d map d removeTab unmap T map b Vomnibar.activateTabSelection map ;t LinkHints.activateModeToOpenInNewForegroundTab unmap X map u restoreTab map @ togglePinTab map O Vomnibar.activateEditUrl map l nextTab map h previousTab unmap &amp;lt;c-h&amp;gt; map &amp;lt;c-h&amp;gt; moveTabLeft map &amp;lt;c-l&amp;gt; moveTabRight  quick marks Vimのmarkっぽい機能として、VimperatorではURLにmarkを設定できるquickmarks (qmark) という機能があって重宝していた。例えばqにQiitaを設定しておくと、goqでカレントタブ、gnqで新しいタブにQiitaが開く。</description>
    </item>
    
    <item>
      <title>サーバーレスっぽいインフラをどう管理していくか</title>
      <link>https://you.github.io/post/try_on_serverless_architecture/</link>
      <pubDate>Tue, 07 Feb 2017 15:37:51 +0900</pubDate>
      
      <guid>https://you.github.io/post/try_on_serverless_architecture/</guid>
      <description>きっかけ  VPSで設置していたプロフィールサイト（静的HTML）をサーバーレスで動かした方がコストダウンになるのではないかと考えた。 とはいえ完全に静的なサイトだとS3だけで事が済んでしまい面白くない（）ので、自分のブログの最新エントリーを動的に書き加えるような処理を与え、動的に生成されるページをLambdaで作ることにした。 サーバーレスの時代にいくつかデプロイフレームワークが出てきていたので、それを使えればと思った。  ……という感じでサーバーレスっぽい何かを作った。いや作ろうとしたってレベルか……。
実は個人で使っているクラウド上のサーバーが、今年1月の時点で3台ありまして。1台はプライベート用途のContainer Linux @さくらクラウド。1台は先に書いたパブリック用途のウェブサーバー @さくらVPS。んで踏み台として、必要なときだけ起動するEC2を1台。ああ、もう1台Herokuもhubot用に使ってたか。これはDockerにしちゃって消しましたが。
まぁさすがにちょっと無駄に多いから整理したいなという思いがきっかけとなった。VPS、数年前だとエンジニアは1人1台持ってnginxとMySQL運用しろみたいな気風もあったけど、なんだかそんな時代も去った感もあり。どちらかと言えばどこかのPaaSでなんか作る方が今風なのかも。
詳細な設計はQiitaに上げましたので、そちらをご参照ください。
使用したツール サーバーレスなデプロイツール これらのフレームワーク群をまとめてどう呼べばよいものか。。。「サーバーレスフレームワーク」がしっくり来るんだけど、これはすでに使ってしまっているプロダクトがあるから使えない……あの名前、汎用的過ぎてちょっとどうかと思うんだけど。
今回せっかくなので、フレームワークを使ってみることにしました。試したのは以下3種類。
   名称 カバー範囲 感想     Serverless Framework 中でCloud Formationを使っているのでわりとなんでも。 多機能で何でもできる印象。CFn直接使うのと学習コストを比べたい感じがする。   chalice API Gateway, Lambda レスポンスがjsonに限定されるなどまだ機能は少ないが、その分簡単なAPIならサクッと作れて楽。何よりIAMポリシーを自動生成してくれるのが最高。   Apex Lambda Lambdaを手作業でzipしてデプロイするのが面倒なときに使いたいツール。GolangでLambdaを書けたりする機能もある。    試用の結果、今回の目的はHTMLの返却だったのでchaliceは一旦諦めて、Serverless Frameworkでもよかったんだけど、ちょっと今回払う学習コストとしては見合わなさそうな印象があったので、Apexを使うことにした。
触った印象としてはApexも非常に学習コストが少なくて良いのだが、今後に期待したいのはChalice。awslabsが作っているので、というのもあるけれど、おそらくサーバーレスアーキテクチャーとして多い用途の1つであろう、API GatewayとLambdaを使ってREST APIを返す、という使い方においては必要十分な機能が満たされているように思います。
HTML5 UP! 以前は自分でせこせこHTMLやCSS書いてたんですが、カッコよいデザインがいいので今回はHTML5 UP!を使いました。やりたいことに注力して、専門外の部分は外から力借りるってとても大切だし、今はあらゆる側面でそれができる時代になりました。以下の記事のおかげです。ありがとう。
 「プログラマでしょ？ホームページ作ってよ！」を1日で対応する - Qiita  サーバーレスの管理問題 冒頭に挙げたQiitaの記事に書きましたが、だいーぶ四苦八苦しました。かれこれ2週間ぐらい？かかった？
例えばRDSやEC2ならオンプレで使っていたLinuxなりMySQLなりを単に置き換えたものとして扱えるわけですけど、API Gatewayなどは抽象化されたインフラなので、どんな機能を持っているのかは全部AWSの采配次第なわけです。従って当然ながらできることとできないことがあり、それをきちんと把握した上で、低コストでアーキテクチャーを設計するというのはなかなかにサービス仕様への理解が必要になります。AWSを持ち上げすぎることは、ベンダーロックインとどう違うのかという意見もありますが、まさにそれです。
使えるようになると確かにとても楽ではありますが、一方でこれに依存し切るのもどうなんだろうな、という思いがあるのも事実で。要所要所でLambda + API Gatewayを限定的に使うならまだしも、3つ4つのサービスを繋がるピタゴラスイッチになってきたら管理コストの方が高くなる可能性が強そうです。先に挙げたApexやChaliceが非常にシンプルなデプロイツールになっているのは、構成自体もシンプルに仕上げるためだと思います。Serverless Frameworkのように守備範囲の広いツールは一見便利ですが、その分カオスにもなりやすい。</description>
    </item>
    
    <item>
      <title>Baroccoで分割キーボードデビューした</title>
      <link>https://you.github.io/post/barocco_review/</link>
      <pubDate>Sun, 05 Feb 2017 23:34:24 +0900</pubDate>
      
      <guid>https://you.github.io/post/barocco_review/</guid>
      <description>20代がまもなく終わってしまうということもあり、今年はある程度健康維持、環境維持というところに注力して、快適かつ健全な肉体と精神を保とうと一応考えてます。んでその一環として1日の半分近くを費やすキーボード入力の身体への負担を改善すべく、 Barocco を買いました。
昨年10月に国内で販売開始、その後さまざまなレビューは出ているので、懸念点は織り込み済みで、半ば流行り始めである分割キーボードの将来に投資する意味合いを込めて購入に至った感覚です。まぁErgoDoxに投資するほどガチでは今のところないかなとか、あまりに通常のキーボードからかけ離れると他のキーボード使いづらくなりそうとか考えた結果、Baroccoが妥当ではという判断に落ち着きました。よく聞かれる不満としてHHKBとの配列や機能的差異があったんですが、幸い？にもHHKB未経験なのでその点は問題なく。
Impression 使い始めてまだ1週間も経っていないので、慣れが必要ではあるし、判断には時期尚早ではあるけど、今のところ悪くないなという感じです。
キー配列 キー配列はHHKBとの比較、というのはわからないものの、確かに少ししんどいところがあって、例えばバッククォートがFn + ESCという変則仕様でだいぶつらい。
また自分はアローキーをよく使うんですが、これもFn + j, k, l, iで少々微妙。キー配列を替えているのは今のところ以下。
 アローキーはデフォルトの機能で「右下Shift , Fn, Pn, Ctrlをアローキーに変更できる」というものがあったので、これを使用。 アローキーに変更して埋もれたFnは、右Altに担わせる。 Aの隣のCapsLockをCtrlに変更（ただし、これは元から使っていたソフトウェア側の機能）。  タイプ感 あんまりメカニカルを使ったことがないし、キーボードマニアでもないんですけど、打ちやすいし不満はないです。CHERRY MXなので期待を裏切ることはないかと。メカニカルは黒軸と青軸しか経験ないんですが、今回も黒軸にしました。赤軸がオーソドックスな気がするんですけど、店頭で試しに触ってみてもどうもあの軽さに馴染めなくて。
分割キーボード 肝心の「分割」という点ですけど、真ん中に位置するYやBは両手どちらも使って打っていたので、たまに空振ったりしてます。でもそれを除いてはそれほど問題なく打てています。
変わったのは姿勢ですね。弊社はアーロンチェアが支給されていて、この椅子って基本的に深く腰掛けるのがベストな座り方だと思うんですけど、これまではついつい前かがみになってしまい、いまいち活かしきれてなかったんです。Barocco導入後は深く腰掛けて、胸を開き、両手を軽く開いた場所にキーボードを配置できるので、アーロンチェアに合った座り方ができるようになった気がしてます。両手のポジションを自由に決めてタイピングができるというのは思っていたより快適でした。いいんじゃないでしょうか、分割キーボード。
結果 よい買い物だったと思います。ただメカニカル、ちょっとやっぱり音がするなぁという感じで、購入するときに店頭物色してて触った静音のキーボードも気になってしまったりして、キーボード沼にはまらないかだけ不安です。HHKBやRealForceの分割も出て、分割キーボードの選択肢が増えるといいんですけど。
Barocco現在は品薄のようで、Amazonで2万円近くになったりしてますが、割高なので時期を待った方が良さそうです。私が買ったのはツクモのネットショップですが、黒軸と青軸だけが17000円ぐらいで残ってました。あとは新宿西口のヨドに赤軸18000円を見かけましたが、まだあるかどうか。</description>
    </item>
    
    <item>
      <title>2016年総括 - 技術者って何なのかやっと理解した</title>
      <link>https://you.github.io/post/looking_back_2016/</link>
      <pubDate>Wed, 28 Dec 2016 18:05:19 +0900</pubDate>
      
      <guid>https://you.github.io/post/looking_back_2016/</guid>
      <description>2016年も終わるので振り返ります。ポエムです。今年はなんというか、技術者として自分が何をすべきなんだろっていうのがやっとなんとなくわかってきたかなという年でした。
元々SEとして4年間、小規模なシステムのおもりをしてExcelと格闘する日々を過ごしていた自分なので、技術的な部分でだいぶコンプレックスがあったし、とにかく勉強しなきゃみたいな焦りをずっと抱いていたんですけど、転職して1年半ぐらい経過してようやくそういう焦りが落ち着いた気がします。今年は業務上必要なことを遅延評価で学習することが多く、あまり自分のやりたいものを作ったりなんだりっていうクリエイティブっぽい感じにはなれなかったんですけど、いわゆるシステム運用ってなんなんだ、エンジニアってどこまで知識や技術を広げるべきなんだっていう全容がやっと見渡せて、自分の位置づけがわかってきたように思う年でした。
とか言ってしまうとだいぶ抽象的ですけど、具体的にやったのはMySQL中心にRDBMSの理解、Linuxでよく使われるBINDやsquidのようなOSSの習熟、AWSの利用とホワイトペーパーからのベストプラクティス習熟、Infrastructure as Code等DevOps周りの導入検討といったところでした。6年目でやることかこれみたいのも入ってますけど、Windowsだけの世界でずっと生きていたというのもありますし、遅かったかもしれないけどなんとかこう、スタートラインには立てたのかなと。学習はほぼほぼ本を読む、ブログを読むというところに特化していて、図書館もよく使いました。都内だと広尾の都立中央図書館がすごい技術書の蔵書量ですね。オライリーに限れば日比谷図書文化館にも結構ありますが。あとは勉強会を必要に応じて月に1回ぐらいのペースで。手を動かすより読む場面の方が多かったかもな、というのは若干反省点。基礎的な内容だろうと恥ずかしがらず、習得したらアウトプットしたいものです。
でー、基礎的、具体的な技術の土台ができてくるとメタな視点を持てるようになるもので、先の見通しがクリアになってきたかなと。『情熱プログラマー』の「1つのテクノロジーに投資するな。ベンダー中心にキャリアを考えるな」という記述にだいぶ影響受けまして、これまでは「技術学習」となるとプロダクトと相対する形がほとんどだったわけですけど、技術は手段に過ぎない、何のためにその技術を選ぶのか、みたいなことをやっと考えられるようになってきました。ミッション的には自分はビジネス的価値を直接生み出す、いわゆるコーディングをする人間というよりは、ビジネス的価値を産んでくれるコードをつつがなく動かす、運用効率を最適化するというところにフォーカスしたいという思いが強くなってきていて、そのためにどんな技術を選べばいいのかなというのをよく考えます。よく言うDevOpsとか、今年流行ったSREに思いとしては一番近いでしょうか。まだまだ未熟なので、そういう方面でバキバキバリュー発揮したいですね。特にDevOpsとかは文化的、コミュニケーション的な側面が強く、純粋に技術極めりゃいいってより組織論とか社会科学的なアプローチが必要だったりするし、そのへんもっと頑張りたいなと。。
あとあと、これまでは技術をインプットするという方向性が強かったんですけど、技術者という枠組みの中にいるのであれば「提供する」側にはそれなりに寄与したいなとも思うようになってきました。なんか今まではLTをする（そういえば今年やっと初LTしました）、GitHubでプルリクを出すみたいなのは自分の価値向上のためだとばかり思ってたんですけど、見方を変えれば技術者コミュニティというもっと広い場所に対する価値向上の意味もあるわけだよなと。特にOSSへのプルリクやらコミットというのは、誰かがそれをしてくれなくてはこれだけ便利な世の中は現状存在していなかったはずというわけで、これまで受けてきた恩恵を少しでも返す努力したいなと。そのためにはコードを読めて書けなくてはならない、もっと手を速くしたい。会社への寄与、自分の価値向上だけではなくて、業界全体に対して自分がやれることというのを意識したいなと思う。
はい、というわけで技術者として冷静になれたというのが今年だろうと思います。自分が何のために技術者やってんだってのをやっと考えられるようになった。ウェブを見てると新しいことやらなきゃいけない、かっこいいことやらなきゃいけないみたいに焦りますが、そうじゃなくて自分ができること、求められていることを的確に提供できるようになりたい、そのために必要な知識や技術を、的確な速度でキャッチアップしているエンジニアになりたいですね。来年はもっとコードを書く、それも汎用的に使える、きちんと公開して他の人の役にも立つようなものを書く、それが技術者としての責任でもあるかなと思います。</description>
    </item>
    
    <item>
      <title>esa.ioを個人利用している話</title>
      <link>https://you.github.io/post/esa_io_personal_use/</link>
      <pubDate>Tue, 27 Dec 2016 00:46:39 +0900</pubDate>
      
      <guid>https://you.github.io/post/esa_io_personal_use/</guid>
      <description>3か月ぐらい前からesa.ioを個人利用している。
esa.ioにした理由 何分調べること、考えることが多い職業ということで、メモをする環境というのは常に欲しいもの。家でも仕事でもその姿勢はシームレスだったりするので、可能であれば場所を選ばずメモが出来て、メモを参照できる環境がいい。
従来はDropboxにmarkdownを保存して、GitHub - glidenote/memolist.vim: simple memo plugin for Vim.を使ったりしていたのだけど、職場でDropbox同期をするのは気が引けたのと、古いメモが死蔵される率が高かったために断念。以前は古いメモも「grepできればOK」と考えていたが、そもそもgrepする単語を思いつかなければ古いメモは参照されないままになってしまうので、もう少し一覧性の高い仕組みが欲しくなった。
手を出したのがesa.io。課金は毎月500円/ユーザー数なので、個人であれば月々500円で使用できる。esa.ioを使っているのは、どこでも使えるという点もそうだし、もういろいろな人が言っていることだけど、「使っていて気持ちがいい」という点が大きい。
 Markdownで書くことが前提になっていて、例えば箇条書きを「* 」を使って書いていると、改行したときに自動で「* 」が入力されるなどの入力補助がある。 Markdownプレビューがシンプルで見やすい。 整理が簡単で、タイトルを「hoge/fuga/memo」とスラッシュで区切るとその構造でフォルダが作成され、「#」を含めるとタグが付与される。ドラッグ＆ドロップでノート整理をするような苦痛がない。 テンプレート機能がある。週報や読書メモのような、決まったフォーマットで書きたいメモを作りやすい。 各フォルダにREADME.mdを作成しておくと、フォルダのトップを表示したときにREADMEの内容がプレビュー表示される。そのフォルダ内の使い方を記したりするのに使える。 デザインかわいい。  esa.ioの用途 もっぱら使う目的は長文用途が強い。日常の中で長文を書く、一つのテーマについてとにかく書き留めていくという機会は多い。技術書を読めば概要などのメモはしておきたいし、勉強会に参加したときも、資料が後から公開されるにしても、手元にメモを残している。何か新しい技術を学んだり、課題を検討したりするときも、なるべく徒手空拳ではなくて思考のログを残したい。「書く」のは特別な道具 - naoyaのはてなダイアリーというエントリーでも触れられているけれど、インプットされた情報そのものではなく、それを自分の中でどう咀嚼したか、どう考えたかをロギングするツールというのが必要で、その点でesa.ioを活用している。というわけで、用途を並べるとこんなところか。
 読書メモ 勉強会、イベントメモ 何か1つのテーマを掘り下げるときのノート取り 個人週報  特に読書メモや週報あたりは特定の書式（KPTだとか、感想と不明点と次に読む本だとか）に則って書きたいという気持ちが強かったので、テンプレート機能のあるesa.ioがすごくマッチしている。
また自分が今注力している分野、短期的な目標等を見失わないため、一番トップのREADME.mdにはそれらを書き記している。いわゆるタスク管理ツールは短期的な行動指針にしかならないので、そのさらに先の方向性を定めるイメージ。『SOFT SKILLS』でも「目標をたてよう！」と言っていたし。
エンジニアと「書く」ということ それにしても自分はメモ環境を悩みすぎている嫌いがあって、今年の初めにも「Personal Knowledge Base」という括りでエントリーを2つ上げている。
 Personal Knowledge Base · the world as code Personal Knowledge Base 2 · the world as code  どうにも「どこかに書き留めることなく、頭の中だけでつらつらと思考を巡らせてしまう」癖のようなものがあって、1つのことを深く考えたり調べていったり、逆に様々な思いつきを将来役立てるために記録しておくことが得意じゃない。
なのでメモ術、整理術みたいなものはいくつも読んでいて、自分が影響を受けたものにはこんなところがある。
 外山滋比古『思考の整理術』。ド定番だけど、一度考えたことを「寝かせて」おいて、しばらくしてから「メタノート」に拾い上げる、考え直すみたいな方法は気に入っている。 PoIC。要は情報カードの使い方なのだが、発見、参照、記録、GTDの4種類にカードを分類するだけで、大掛かりな整理はしないシンプルな方法論。こちらも「メタノート」のように、何かを考えるにあたっては関連するカードを束ねた「タスクフォース」を作り、メモの掘り返しを行う。 GTD。言ってしまえば頭の中のことをすぐにメモをして、その内容はすぐ何かやるべきか、後でやるか、特に行動が要らないか分類して、頭の中を空っぽにしようという話。  これらに共通するのは、結局のところ思考を外部に記録することで、脳内のリソース効率を高めることと、蓄積された情報を後から見返すことで、新たな意味を発見できる可能性があるという二点になる。PoICの中で、情報の蓄積による「エントロピーの増大」という表現をしていたが、頭の中で考えているだけの段階だと見通しが悪くてあまり価値のある情報にはならないのだが、書き溜めていくことで自分が何を考えているのか、どういう方針を持っているのかが明確化されるという副作用がある。過去の自分に囚われたいわけではないが、自分がどういった方向性で進んできていて、それとブレていないかを定期的に確認する手段としてメモなりブログなりは作用している。
技術系の仕事をしていても同じことで、あるソフトウェアに関するエラーを調査していたはずなのに、徐々に重箱の隅を突き始めて、目的から逸れたことを調べていたりすることはそこそこある。「体系立てて考える」というのは簡単なように見えて案外難しくて、「外部脳」に頼ることはやっぱり必要だなと思う。それこそ研究者の文献管理方法とか参考にしてみたらいいのかもしれないと思ってもいて、一度togetterにまとめられているのを読んだのだが、案外というか、京大式カードとか使うものなんだな実際と思ったり。
 参考：文献読書中のメモの管理方法 - Togetterまとめ  こういう「メモをする」「ノートを取る」重要性は理解しているし、それに沿った方法論や道具も持っているはずなのに実行できていないというのは、まぁちょっとアプローチを考える必要があるんだろうなと思う。強制的かつ自動的にメモを取る、脳内ではなく「脳の外で思考する」ために「書く」という手段を使えたらなと思いながら、まだ絶賛模索中。</description>
    </item>
    
    <item>
      <title>エンジニアの問題解決力とは何か</title>
      <link>https://you.github.io/post/engineering_problem_solving/</link>
      <pubDate>Sat, 17 Dec 2016 12:43:17 +0900</pubDate>
      
      <guid>https://you.github.io/post/engineering_problem_solving/</guid>
      <description>今年度、特に下半期からいわゆる技術的負債の返済、特にDevOps方面におけるプロセス改善に深く携わるようになった。これまで依頼ベースの対応や、プロジェクトベースの仕事をすることが多く、要は「何をやるか」がある程度決まっていたわけだけど、改善系の業務は問題を見つけ、解決策、しかも場当たり的なものではなくてボトルネックを閉めるような策を講じていく必要があるということで、これまでと違う視点で仕事をする必要が出てきた。そこで何冊か「問題解決」にフォーカスした本を読んだ結果をまとめてみる。
問題とはなにか そもそも「問題」って何なのか。細谷功『問題解決のジレンマ』によれば、問題とは「事実と解釈の乖離」だという。
問題解決のジレンマ: イグノランスマネジメント:無知の力posted with amazlet at 16.12.17細谷 功 東洋経済新報社 売り上げランキング: 112,865
Amazon.co.jpで詳細を見る 客観的な「事実」があり、その関係性や論理構造を規定する「解釈」がある。事実というのは客観的なものなので変化することは少ないが、解釈は時代や文化によって在り方を変える。つまりは古くなってくるわけで、それによって事実と解釈が乖離を起こすことにより、問題が発生する。
システム運用という自身の立場で考えれば、例えばベンチャー企業においては「小規模なシステム」かつ「少人数の精鋭社員」という「事実」下では、「スピードが重要でドキュメントを書かずとも運用は可能」という「解釈」が成り立ちうる。これが年数が経過して「大規模なシステム」かつ「新卒等も含めばらつきのある社員」という事実に変わってくると、先の解釈通りにドキュメントなしでは運用が難しくなったりするわけで、事実と解釈の乖離が起きた状態が発生していることになる。
問題を見つけるために 人は日常的には己の「解釈」の枠組みの中で生きている。知識が増えるほど、自分の知る範囲内での最適化＝問題解決を図ろうとするようになるが、根本的な問題解決をするには、そもそもその「解釈」の範囲の外に問題がある、つまり自分の「解釈」がすでに形骸化している、ということを見つける必要がある。『問題解決のジレンマ』では、これをラムズフェルドがかつて言及したUnknown unknowns（未知の未知）という言葉で説明している。
とはいえそれは容易な話ではない。たいていの人間は自身の知識、先入観を抽象化して物事を考えるということを、簡単には実践できていないように思う。
自分がそもそも「わかっていない」ということを「わかっていない」ことに気付くには、安直に言ってしまえば視野を広げていくしかない。『問題解決のジレンマ』で挙げているのは「フレームワークの導入」で、物事を一般的なフレームワークに当てはめて考えれば、どの分野に対して視点が足りていないのかを探る手がかりになる。
また「知識がある」という状態がそもそもの「未知の未知」を見つける足枷になるわけだから、知識をリセットする、知識をフローとして扱って、不要になったら捨てていくようなプロセスがいいのではという。これは外山滋比古も「忘却の力」という形で扱っている概念だ。エンジニアは技術職だが、一つの技術領域にこだわりすぎると時代の潮目についていけなくなったりするので、この点は一理ある。
あるいは『SOFT SKILLS』の「学習」の項において、学習すべき事項を見つけるために勧めていたのが、「わからないことをメモしておく」ことだった。学習すべき事項というのは要は「問題」なわけで、これも問題発見には応用できるのだと思う。まぁ地道で「そりゃそうだろ」という話ではあるのだが、日頃見つかる小さな問題をスルーせずに、都度確実に書き留めておくことは必要と思う。そして個別の問題それぞれに対応するのではなく、ある程度蓄積された複数の小さな問題を並べて「ボトルネックは何か？」と考えていくことで、対応すべき大きな問題が見つかるのではないか。
問題を解決していく過程 見つけた問題を解決する過程については安宅和人『イシューからはじめよ』に詳しいが、内容としては要するにデカルトの『方法序説』に近い、一般的な科学的方法論だ。問題を見つけたら、それを解決可能な小さな単位に分解し、単純かつ具体的な観測から抽象へと認識を進め、全体の論理が沿うように再構成していく。GTDにおいても、目的と求めるべき結果を最初に定めて、その間に必要なタスクをブレインストーミングしていく「ナチュラルプランニング」という方法論があるが、どこか似通っている。
イシューからはじめよ―知的生産の「シンプルな本質」posted with amazlet at 16.12.17安宅和人 英治出版 売り上げランキング: 1,168
Amazon.co.jpで詳細を見る ある問題を端緒として改善活動を始めたとして、どうもエンジニアとしての性なのか、当初の目的からズレて技術的な面白さを求めてしまったりすることはよくある。サーバーのデプロイスピードが遅いことを発端として改善を始め、じゃあクラウドを使って改善しようという話になったが、どのクラウドサービスを使えばいいのかと言った別の問題で時間をかけてしまったり、結局手作業でEC2インスタンスを作っているのでオンプレのときとスピードが変わらない、みたいな話はありがちだ。問題から解決策への筋道が論理的な整合性を保っているかは、常に確認が必要になる。
この点は自分が特に出来ていないところで、プライベートで何か勉強を始めたはいいが、何を求めて始めたのかを忘れてしまって、細かいつまずきポイントでずっとハマったままになったりしていることが少なくない。
具体的実践 以上のような検討から、具体的にいろいろと実践してみようと思う。
 「わからないことメモ」をすすめる。よく使っている小さめのメモ帳があるので、それに1ページ1項目で「わからないこと」を記述して、週末に見直してみる。 何か勉強を始めたり、問題解決にあたるときは、個人契約しているesa.ioでノートを1つ作る。  まず問題を1文で書き表す。それを出発点として、ぶれないことを心がける。 調査の過程、問題解決を図るプロセスもすべて記録していく。常に全体の整合性が取れていることを確認しながらすすめる。 最終的に「解決」まで至ったノートは、ブログやQiitaに投稿して公開する。 実はこのエントリーもその方法に則ってesa.io上で昇華させた。  あまり特定の「技術」にこだわらない。  そもそも問題解決やビジネス的な成功という「目的」があり、それを達成するための手段が「技術」なのであって、それが何を採用するかこだわるのは本質ではない。 技術への固執は「未知の未知」を見えにくくする。多様な技術に対して寛容な理解を心がけることで、常に必要な技術へキャッチアップできるような気がする。 一方で低レイヤーの知識や、コンピュータ史への造詣は深める。先に挙げた「フレームワーク」にあたるのがこのような基礎分野だと思うので、基礎を固めることで現在の技術潮流をメタに判断できるようになる。   特に最後の「特定の技術にこだわらない」は重要だと思っていて、自分はどうしてもクールな技術、なんだかカッコよさそうなものがあると簡単に心惹かれてしまい、技術が目的になってしまう。もちろん、使っていて気持ちのいい技術を選択するのも大事なのだが、結局はどの技術もツールに過ぎないわけで、自社の「問題解決」に適切なハンマーなのかという点は念頭におきたいし、またその目的に適うならどんな技術だってクールと思うべきなんだろうと思う。
自分は謙遜せずに言えば頭の回転が速い方なので、どうしても考えすぎてしまう、頭の中でぐるぐる物事を捏ね繰り回してしまう傾向にあり、それを解消する意味でも「きちんとesa.io上で論理展開する」というのは良いだろうと思っている。『考えない練習』という本も、考え過ぎを抑制する助けになりそうなので読んでみたい。
考えない練習 (小学館文庫)posted with amazlet at 16.12.17小池 龍之介 小学館 (2012-03-06)
売り上げランキング: 5,008
Amazon.co.jpで詳細を見る また、ここまでは個人的な実践の話が多かったが、実際に職務上の問題解決を行うにはチームを巻き込む必要がある。人を動かす、チームの中で振る舞っていく方法論については、また別の課題としていきたい。</description>
    </item>
    
    <item>
      <title>多義化するOpsのミッションについて</title>
      <link>https://you.github.io/post/what_is_ops/</link>
      <pubDate>Tue, 25 Oct 2016 00:20:02 +0900</pubDate>
      
      <guid>https://you.github.io/post/what_is_ops/</guid>
      <description>今年度から運用の担当になったんですけど、最近消化不良を起こしつつあります。立場としては言われたことだけやるオペレーターという形じゃなくて、いわゆるインフラエンジニアっぽい感じだけど構築はやりませんよって感じです。日々のオペレーションや障害対応をやりつつ、一方でDevOpsの整備とかに注力している感じ。でも自分のミッションはそこまで社内で明確化されているわけではなくて、どうもそのあたりに消化不良の原因がある。
Opsの担うミッションの変化 そもそもにしてDevとOpsの境界ってどこなんだろうという定義はわりと曖昧な気もしますが、おそらくは「リリース」が境なんでしょう。基本的にはリリースまでを担当するのが「開発」で、リリース後のシステム稼働の面倒を見るのが運用、保守というイメージでいるのだけど、「運用」が担うミッションって最近かなり広がっているような気がするんですよね。
一番難易度の低い運用といえば、いわゆる手順書に沿った業務しかやらないオペレーター作業になるわけですが、そういう運用と開発の業務内容、職掌をガッツリと分けた状態って最近は主流ではなくて、いわゆるDevOps的に開発フェーズからデプロイされて本番運用へと移っていく過程がシームレスになっていくのが常識化しつつある。あと運用というのも、システムを単に放っておいてトラブルが起きたら対応、という受動的なものではなくて、能動的にメトリクスやログを解析して、SREのようにソースコードにまで手を入れて、安定運用のための方策を講じていくスタンスが生まれつつあります。
少し前だと、いわゆるインフラエンジニアが運用も担っているようなケースは特にWeb系だと多く見られていたし、Infrastructure as Codeの黎明期には「インフラエンジニアもコードを書けるべき」ということが盛んに言われていましたが、もはや時代は「運用の中でソフトウェアのコードを編集する」というところまで来ていて、だいぶ時代が移り変わったように見える。
Ops先鋭化へのジレンマ もともとインフラエンジニアが運用を担っていたパターンにおいて、SREのような業務を来月からやりましょうってのはまず無理だと思っています。スキルの畑があまりに違うので。やるのであれば人員の配置換えをするか、インフラエンジニアにかなりの学習コストを払わせることが必要になる。ので、ちょっとSREの話は先鋭的すぎるとして置いておくとしても、いわゆる「インフラエンジニアもコードを書けるべき」は求められる場面が多くなってきたように思う。
そもそも旧来の運用業務にしたところで、一切コードを書けない人間が担えたとは思えないんですよね。まぁ「手順書運用」は一切スキル不要なので別として、何か処理を自動化して効率化しましょうとなればスクリプトぐらいは書ける必要があるし、OSSの監視ツールを使いましょうとなれば、ソースを読む機会ぐらいはある。ただ、実際に運用業務に携わっていて思うのは、それを出来る人間というのは現実として限られてもいる。インフラエンジニアが運用を兼ねるようなケースであれば尚更です。
堅牢性を求めて保守的なシステム運用をしようとすれば、運用は「決められたことだけをやる」「引き継いだことだけをやる」という方向にシフトしがちなわけで、そうなってくるとコードを読み書きするスキルが必要とされる場面は当然ながら減ります。その場合のスキルはインフラ周りに重点が置かれて、コードの修正はソフトウェアエンジニアに任せることになる。手作業のオペレーションが煩雑であるならば、ソフトウェアエンジニアへ自動化を依頼するかもしれない。
どちらが良い、どちらが悪いという話ではなく、スキルと職掌による問題なんだろうと思っています。同じシステムを運用していても、コーディングスキルがあるエンジニアであれば自ら自動化をしよう、ソースコードを確認してみようという手の動かし方になるし、コードに携わるのは自分の職務ではないと認識していて、スキルも持ち合わせていないエンジニアであれば、誰かに任せればいいという思考になる。
（運用も担っていた）インフラエンジニアの役割が変わってきた、というよりは、スキルやマインド、スタンスの面で、コーディングスキルのあるOpsエンジニアが別の方向へ進み始めたというように思えます（繰り返しになりますが、どちらが良し悪しということではなく、決められた手順だけを実施させることで運用のコストを下げる、というのも一つの考え方だとは思っています）。
圧倒的技術力の必要性 ところで、DevOpsはさておき「システム運用」全体を網羅したような本ってあんまりないです。最近は少し出るようになっていて『インフラエンジニアの教科書』などはそれに近いですが、（サーバー）構築の話も入ってくるので、純粋な運用というとオライリーの『ウェブオペレーション』が一番ハマっているかなと。
インフラエンジニアの教科書posted with amazlet at 16.10.23佐野 裕 シーアンドアール研究所 売り上げランキング: 29,314
Amazon.co.jpで詳細を見る ウェブオペレーション ―サイト運用管理の実践テクニック (THEORY/IN/PRACTICE)posted with amazlet at 16.10.23オライリージャパン 売り上げランキング: 371,961
Amazon.co.jpで詳細を見る Amazonに在庫ないっぽいので絶版かもしれないですがいい本でした。監視の考え方から継続的デプロイ、障害対応に関してまで載ってる。技術書というよりはエッセイっぽいですが、運用ってどんなことやるんだというのがわかる本。
圧倒的な技術力が求められる職種だ /「ウェブオペレーション」を読んだ - kakakakakku blog
レビューを上げているブログを貼りましたが、このタイトルにもある通り、この本を読むと「圧倒的な技術力が求められるわ……」と呆然とします。2011年の本なので、SREのような先鋭化したスタイルに言及があるわけではないですけど、そもそも障害への対応にしたって、原因を切り分けるにはコンピュータアーキテクチャーのレイヤーから、OS、ミドルウェア、ソフトウェア、ネットワークと様々な視点を考慮する必要があるわけです。原因追求をせず、手順書通りに再起動や再実行をして「とりあえず」復旧したからOKとするのも運用と言えば運用ですけど、システムの安定性を保つためにどちらが望まれるかとなれば、大きな違いが出てくる。また運用や障害対応は組織で行うのが基本なので、うまいこと人員が動いて有機的に連携していくには、時に心理学や社会学の知識も役に立ったりします。
思うのは、Opsという領域が二分されてきているんじゃないかということです。運用を標準化して、決まったことをやることでコストを抑えた方式と、SREへ辿り着くような、漸進的にシステム改善を図りながら安定稼働を目指す方式。なんとなく、前者のエンジニアが後者へ移り変わっていくようなイメージを持っていましたが、そもそもスタンスとして前者を是とする人と、後者を是とする人の2種類に分かれてきているように思う。B to BのサービスでSLAを握っているのなら、多少システムリソースの使用率が逼迫していても、SLAの範疇で運用されていれば問題がないわけだし、そこでパフォーマンスチューニングをしてもお金が取れるわけではないので得策ではない。となればSREは必要ないかもしれない。といった具合に、スタンスの違いでミッションも変わってくる。
自分の場合は後者のスタンスを欲しています。SREのような高いスキルは現状持ちあわせていないけど、システムの安定した稼働、高効率な稼働に寄与して、エンジニアの対応コストを抑えていくというのをミッションにしたいなという思いが強くなってきている。が、それも会社が目指すOpsの方向性と噛み合っていないと無意味です。「運用」という言葉で括れる範囲は極めて広く、多義的になってきている昨今で、先に示したように「これ1冊！」という本を読めばOKというものでもないです。なので、まずはリリース後のシステムをどうしようか、というOpsのミッションをブレずに定義することが求められているのかなというのが最近の実感でした。
ちなみに自分が思うOpsの職掌としては、クックパッドさんのインフラエンジニアの職掌範囲がとてもしっくりきていますね。
インフラエンジニアの責任範囲と評価 - クックパッド開発者ブログ</description>
    </item>
    
    <item>
      <title>ServerlessConfとエンジニアの職掌に関して</title>
      <link>https://you.github.io/post/serverless-conf-tokyo/</link>
      <pubDate>Mon, 03 Oct 2016 23:52:14 +0900</pubDate>
      
      <guid>https://you.github.io/post/serverless-conf-tokyo/</guid>
      <description>ServerlessConf Tokyo
10月1日に開かれたServerlessConf Tokyoに行ってきました。会場で一度「サーバーエンジニアの方ってどれぐらいいますか？」という質問がありましたけど、そのときの挙手によればサーバー、インフラ寄りのエンジニアはだいぶ少数派だったみたいですね。まぁサーバーなくなっちゃってんだからそりゃそうだろって感じなんだけど、自分としてはインフラエンジニアの立場から見て「サーバーがない！」という状況はどう映るのか、どう向き合えばいいのかというのがだいぶ気にかかってました。それでなくてもクラウドの登場、特にAWSが当たり前のものとなってからはインフラエンジニア不要論が常に囁かれてはきたわけで。
インフラエンジニアの幸福論 · the world as code
改めてサーバーレスとはなんだろうと。いや、サーバーがないはずはないじゃんというのはよく聞く反論で、仰る通りAWS Lambdaといえど裏側では当然Amazon Linuxが、物理的なサーバーが動作しており、サーバーレスって言葉はあまりよろしくないんではみたいな話もあります。とはいえ運用するエンジニアの視点からすると、サーバーというスコープはごっそり消失しているような状態ではあるので、個人的にはあながち的を外しているとも思ってはいないです。しかしそうすると今度は、従来のherokuやAWSマネージドサービスだってサーバーレスだったことになるじゃん？というのが自分の疑問としてはあったのですけど、この点はだいぶ思い違いをしていたなと今回認識を改めました。
 端的にサーバーレスで受けられる恩恵をまとめたものとしては、このスライドが非常にわかりやすくてピンときました。これまでビジネスロジックはサーバーというモノシリックなエンティティの中に組み込まれ、サーバーを基本単位として扱われることが主だったわけだったですけど、サーバーレス、いわゆるFaaSの世界ではビジネスロジックが単一の機能＝関数を単位としたマイクロサービスへ分割されます。そして実行媒体は常時稼動が前提であったサーバーやデーモンから、immutableなライフサイクルを持ったコンテナへ移されるため、自ずとステートレスな状態が保たれる。FaaSの実行はイベントドリブンで必要なときのみに絞られるので、リソースの効率性も従来とは考え方がまったく変わってくる。サーバーレスというのは、こういったソフトウェアアーキテクチャーの全体的な転換点として捉えなければならないんだなというのがやっと掴めました。
確かに実行媒体であるはずのコンテナすらも隠蔽され、コードを書いて渡してしまえばそれだけで実行されるサーバーレスな世界というのは、インフラエンジニアという旧来の職種が入る余地はなさそうには見えます。でも自分の立場から見て、こういったサーバーレスの世界でやること皆無になるか？というとそうは思えなくて、ハードウェアリソースを如何に効率的に使うかだとか、リソース部分をすべて別の事業者に委ねてしまった状態で、可用性やパフォーマンスをどう担保していくかだとか、インフラエンジニアの視点から出来ることっていろいろあると思うんですよね。まぁそりゃ今までみたいにLinuxにSSHして云々だとか、ネットワーク機器の設定をどうのってのをやる機会はほぼ無いのかもしれないけれど、「インフラエンジニア」の定義を技術領域からミッションに変えることで、見えていく先はあるんじゃないかなと。
またイベント内でこれも何度も聴いたんですけど、当然ながらサーバーレスは銀の弾丸ではないです。ステートフルであることが相応しいソフトウェアもあるし、デーモンを常駐させる必要性も現在皆無になったわけではない。サーバーレスを至上として、ソフトウェアをどうすればサーバーレスに出来るか？と考えるよりは、ソフトウェアをより望ましい状態にするために、アーキテクチャーとしてサーバーレスを選択することが適切か？という考え方をするべきなのではないかなと感じました。
エンジニアの職掌は従来だと技術領域のレイヤーで隔てられることが多かったですが、技術分野の変容もさることながら、アーキテクチャーすら10年待たずにガラガラと形を変えていく時代においては、技術レイヤーへのフォーカスだけで職種を定義すると痛い目を見そうな気がしてきました。まずはミッション先にありきであって、そのための手段として技術やアーキテクチャーを選択する。そういう発想でいくと、「インフラエンジニア」と呼ばれる職種にも未来はある気がします。あるいは最近それがSREといった呼び方に変わったりしているのも、その証左であるんじゃないかなと。</description>
    </item>
    
    <item>
      <title>『SOFT SKILLS』と「やっていく気持ち」</title>
      <link>https://you.github.io/post/soft_skills_and_our_productivity/</link>
      <pubDate>Mon, 01 Aug 2016 22:30:41 +0900</pubDate>
      
      <guid>https://you.github.io/post/soft_skills_and_our_productivity/</guid>
      <description> SOFT SKILLS　ソフトウェア開発者の人生マニュアルposted with amazlet at 16.08.01日経BP社 (2016-06-02)
売り上げランキング: 223
Amazon.co.jpで詳細を見る 『SOFT SKILLS』読みました。印象としてはいわゆるライフハックな内容をエンジニア向けに落とし込んで解説している本、とういう感じ。ただ、単にテクニックだけを載せたペラペラした内容ではなくて、成功するためにはハードワークを避けるべきではない（第47章）ということもまた語っていたり、実情に即した書き方である点が好感が持てる。というか全体的にマッチョ寄り。
この手の「ソフトスキル」、つまりエンジニアやギークが「よく生きる」ための本というのがこれまで皆無だったわけではなくて、例えば『エンジニアのための時間管理術』や、『ヘルシープログラマー』に書いてあることと一部の記載は似通っている。本書のポイントとしては、プロダクティビティ、精神、肉体、金銭といった横断的なソフトスキルを幅広くまとめ、またそれらを「エンジニアとしてより良く生きるために、直接的なエンジニアリング以外の領域をどうすべきか？」という視点でまとめた本はあまりなかったのだと思う。自分はライフハックがわりと好きでかじりまくった時期があったので、ちょっと目新しさに劣る部分はあったのだけど、そういう分野を敬遠してきたエンジニアにとっては新鮮な話が多いはず。
行き先を定めてから、走るスピードを上げる 個人的にライフハックに対して感じていることがあって、タスクをどう処理するか？とかライフログがなんちゃらみたいな「目の前のことに対応する方法」は多いのだけど、それをすることで何を得るの？っていうのがあんまりなくて。早く帰れますとか生産性が上がりますでもまぁいいんだけど、それって本来「手段」となるべき部分で、生産性を上げて何を成したいのかというのが重要だよなとずっと思っている。
本書の構成はその点で言えば第1部が「キャリアを築こう」で行き先を定める部分にあたり、第2部「自分を売り込め！」でそのために最も必要なことを語り、第3部以降でいわゆるライフハック的な、プロダクティビティや肉体精神の維持といった話に入っていく。順序としてはやっぱりこれが正しくて、行き先を定めておかないと、何のために走力を上げたいのかという目的を見失ってしまう。
日常での実践 本書の内容で自分がいま実践できていることって、実際のところあまりなかった。目標を持とう、と言われてそういえばあまり具体的な目標ないなと気付いたし、そこまで自己アピールして「売り込み」ができてもいないし、投資はちょっと前ちょろちょろETFを買ってみたっきりだし、肉体と精神に関しては自信がない。
生産性に関しては少し前にrebuild.fmで取り扱っていたこともあり、自分は今ポモドーロを使っている。本書で紹介されているKanbanFlowは使っていないけれど、Trelloでざっくりした自分のプロジェクトの状況をダッシュボード的に管理して、細かい「やるべきこと」はTodoistに入れて、Chrome ExtentionのToggl Buttonでポモドーロタイマーを使っている。ポモドーロは25分間集中させることに意味があるように語られがちだけど、実際は1日の消化ポモドーロ数を見て自分の生産性を測るのに適していると書いてあって、そうそうそれだよ！という感じだった。タスクの消化数だと、タスクごとに工数が異なるのでなかなか生産性とイコールにはならないのだが、ポモドーロだとそれができる。あるいはアジャイルにおけるPivotal Trackerの考え方が生産性の計測には良いのかなと思っている。
その他の部分は取りあえずやれることから手をつけたいかなと。ただ冒頭にも書いた通りわりとマッチョ寄りな内容が多くて、例えば自分は精神力に本当に自信がないのだが、その改善策として「理想的な自己イメージを想像することだ」と言われてもなかなか難しい（文中でも「難しい」とは書かれているけど）。あるいはこういう記述。
 その上で聞きたいのは、あなたは負けるつもりなのかということだ。単純に自分は仕事に集中することができないと諦めるのか、それとも抵抗に立ち向かい、障害を乗り越えようとするのか。それはあなただけができる選択だ。単にしなければならない仕事をすると決心すればいいだけだ。
 いや厳しい。。。とはいえ言っていることもごもっともなので、「やるしかねえ」と腹くくるかなぁという感じです。いい刺激になりました。以下、やること取りあえずでメモ。
 ポモドーロでの生産性計測を継続する。 キャリア上の目標を定めてみる。 どっかでLTする。年内中を目標に。 瞑想を取り入れる。余裕あったら筋トレする。ランニングは見送る。 ハードワークを避けない。難しい課題、つまらない課題でも挑む姿勢。 何かを学ぶときは漫然と行わず、本書の10ステップに取りあえず従ってみる。 メモの頻度を上げる。自分の内面の状態管理に使う。 脂肪ではなくタンパク質の摂取。フルーツの摂取を心がける。 自己イメージを持つという話は納得できてないので、本書で紹介されてる『自分を動かす』読んでおく。  </description>
    </item>
    
    <item>
      <title>YAPC経験ないけどパチモンの方に行った</title>
      <link>https://you.github.io/post/yapc8oji_2016/</link>
      <pubDate>Sat, 02 Jul 2016 22:11:59 +0900</pubDate>
      
      <guid>https://you.github.io/post/yapc8oji_2016/</guid>
      <description>イベントレポートに見せかけたポエムです。
技術ブログとかQiitaとかやってると、更新してない間はあんまり成果出てないってことでバロメーターになっていいですね。先月はあんまり具体的な成果に繋がることは仕事でもプライベートでもしていない。。。何をやっていたかというと、
 AWS運用見直しのためにホワイトペーパーやユースケース読み漁り EFK(Elasticsearch + Fluentd + Kibana)スタックをDockerでやろうとしてまだ途中 セキュリティ知識つけるために『暗号技術入門』と「徳丸本」を読破。  あたり？ というわけでなんだか煮詰まった状態で久しぶりの勉強会というかイベントになったのがYAP (achimon) C::Asia Hachioji 2016 mid in Shinagawaでした。昨年まで開催されていたYAPC::Asiaの「続き」として開かれたカンファレンス。
本物は経験ない タイトルにも書いたけど本物のYAPCは一度も行ってないです。昨年転職するまではOfficeを主戦場としたSEだったのでアンテナ低かったし、タイトルにPerlって入ってるけど自分Perlエンジニアじゃないし、Perlを冠しながらあんなイベントだとは知らなかったし？ 結局内実をきちんと知ったのは昨年の開催中だったので時はめちゃめちゃ遅すぎた感じであった。RubyKaigiとかAWS Summitのような大きめのカンファレンスは経験あるにはあるけど、ビッグサイトで開催されていたという規模はちょっと想像できないですね。もったいないことをした。
技術スタックを掘り下げたい 今回いろいろとトークを聞いていて思ったのだけど、技術スタックをもっと掘り下げていきたいなという思いがした。こういう場で話される内容って、そのエンジニアの得意領域だったり経験だったりが活きるもので、ちょっとググって資料つくりましたとか、大して思い入れはないけどやってみたので喋りますとかそういうのじゃないんですよね。掘り下げがあるから話が面白くなる。実体験だから身にしみてわかる。そういうものだよなと。先月はAWS Summitにも行ったのだけど、今日の方が面白かった。それは企業がお膳立てした話じゃなくて、エンジニアがボトムアップに立ち上げたイベントだからだろうなと思う。
転職してちゃんとエンジニアっぽいこと仕事でできてるなーと感じ始めてから、自分はだいたい1年ぐらいなのだけど、この1年は業務についていくためと時代についていくため、とにかく広くいろいろ調べてやってみるというばかりで、あんま掘り下げってしてないんですよね。「インフラエンジニア」って肩書はよくあるものだけど、その守備範囲ってすさまじく広いので、DBなのか、AWSなのか、ネットワークなのか、セキュリティなのかみたいな、自分を位置づける技術スタックは持っておきたいなと思った。よくT字型とか言われるけど、広く浅い知識はもった上で、一部分で深掘りを進めていかないと、自分が何をすべきか見失いそう、というか見失ってるな最近と。
エントロピーの増大期間 突然話は変わるけど、自分はPoICという方法論がわりと好きで。要は情報カードに発見や記録をメモしていきましょうみたいな知的生産の話なのだけど、この中で面白いなと思っている話として、エントロピーに絡めた話がある。頭の中の乱雑な情報をカードに書いて書き溜めておくことで、そのストックの中でエントロピーを高めて生産性を生み出すという考え方。
これって外山滋比古のいう「メタノート」とか、あるいはジョブズのスタンフォードでのスピーチにあった「Connecting the dots」と話は似ていて、要はある程度蓄積されてこないと情報や知識って意味を成さなかったりするもんなんですよね。学習曲線も正比例のグラフを描かないことは有名だけど、何かを学んだり取り入れたりすることがすぐに成果を生むわけではなくて、ある程度価値となるまでには時間がかかる。
冒頭に書いたように、先月ぐらいって自分はあんまり成果出せなかったなっていうのがあって、停滞感にもやもやしていたんだけど、エントロピーの増大期間という考え方をすると、まぁそんな焦る必要もないのかなと。きちんと日頃から必要な知識に向き合って習得していくことで、徐々に掘り下げってできてくるのかなと思う。
で、エモい話だけしてイベントレポート終わるわけにもいかないので具体的な話も書きますが、今回MySQLの話4回ぐらい聞いていて、改めてDB弱いなと思った次第。
  とあるイルカの近況報告  from yoku0825 
弊社もMySQLがメインで使っているので、いい加減真面目に追いかけておきたい。正規形の話も数年前に受けた応用情報の復習みたいで勉強できた。
  MySQLと正規形のはなし  from yoku0825 
これは現地で聴いてなくて後から読んだスライドだけど、esa.ioの姿勢はとてもいいなと思う。頑張り過ぎないのあたりとかすごい大切だと思っていて、最初からベストなやり方するより、とりあえず導入した方がいいってことは多い。あと移行日記。移行じゃなくても技術的な日記書きたい。一つの技術を長期間追ってると徐々に目的見失ったり、そうだアレもやらなきゃみたいに脇道逸れたりしやすいんだけど、これぐらい砕けて感情的に書くと、それがなくなりそう。
 あと個人的にはHashicorp Vaultがとても気になりました（資料はちらとググったけど、まだ上がってない？）。クレデンシャル情報、どうしてもLDAPやIAMのような集中管理に載り切らないものが出てきてしまうけど、これなら集約的に管理できて、しかもAPIあるから取り回しも楽そう。
いろいろ考えることの多い1日でした。自分がどこに向いて進むのか、もっかい考えようと思う。</description>
    </item>
    
    <item>
      <title>Serverlessの時代とas code</title>
      <link>https://you.github.io/post/aws_summit_ansible_meetup_2016/</link>
      <pubDate>Sun, 05 Jun 2016 23:46:12 +0900</pubDate>
      
      <guid>https://you.github.io/post/aws_summit_ansible_meetup_2016/</guid>
      <description>先週Ansible Meetup in Tokyo 2016.06と、AWS Summit Tokyo 2016に行ってきたので軽めのレポートにします。
AWS Summit AWS Summitは初参加でしたが、会場を見渡したときのスーツ率の高さからいわゆるEnterprise系のイベントに近いのかと思いきや、DevConの方を中心にテクニカルな話題も多めで楽しめました。とはいえAWSサービス紹介にとどまるセッションや、タイトル通りの内容ではなく、各企業の内部事情を抽象的に話すだけで終わるようなセッションも少なくなく、セッションの選択はそれなりにコツがいるなとも思ったのですが。。
自分が受けたセッションで特に多く話されていたのは、Serverlessの話とDevOps、具体的にはCI/CDの話。いずれも要はこれまで複雑に運用していたシステムが、AWSのマネージドサービスを使うことで簡単に実現できるという話なのですが、特にServerlessに力を入れてる感じがしました。Lambdaの話がすごく多い。先日Serverless confで「サーバーを叩き割る」というパフォーマンスで話題になった、AWSのTim Wagnerも来てましたしね。
This is how you go #serverless – @timalleneagner pic.twitter.com/SpllWVz76u
&amp;mdash; Lars Trieloff (@trieloff) 2016年5月26日 
Lambda、簡単なジョブや解析処理であればEC2立てずとも実行が可能になるわけで、いわゆるアプリケーションが担っていた仕事、これまでサーバーを立てなければならなかった部分がごっそり持っていかれることになる。Lambda自体は発表が2014年なのでそれなりに経っているけれど、Pythonに対応したあたりから実用される動きが大きくなってきたような感触があります。とはいえLambdaだけですべて済むというわけでもなく、例えばそのエンドポイントとしてAPI Gatewayが使えたり、SNSと連携してメッセージのプッシュを行ったりと、これまで連綿と作られてきたAWS各サービスがあって、それらを繋げる部分に位置することで真価を発揮している。そう考えるとLambdaが単体ですごいというより、AWS全体を見回したときに足りなかった1ピースを埋めてくれたような印象がある。
Ansible そこに来てAnsible、というのはどういう位置付けになるのか。Ansibleの役割はServerlessとは対極、基本的にはサーバーの設定管理というところになります。仮にシステムをServerlessに置き換えていくのだとしたら、ひょっとしたら徐々に要らなくなってくるツールなのかもと思ったり。無論、AnsibleにもAWSモジュールがあるけれど、端々まで対応しているというわけではなく、AWSの設定を管理するのであればCloudFormationやTerraformを使う方が現実的かなという気がします（AWS Summit内で、CloudFormationにLambdaや周辺サービスの設定を書いて、サーバーレスのマイクロサービスをパッケージングする手法が紹介されてました。Lambdaのコードもそのjsonの中に含むので「つらそう」という声は多かったですが）
Cloud Modules — Ansible Documentationv
まぁ、とはいえ現状を鑑みてサーバーが一切なくなるというのはまずないとも思います。AWS Summitでの趣旨も別にサーバー全廃しろよと言ってるわけではなくて、Lambdaに肩代わりさせることでコストダウンしたり効率化が図れる部分が大きいよという点。だからポイントとしてはAWSのマネージドサービスを上手く使えないかというのが設計上第一に来て、困難な部分はEC2（やオンプレのサーバー）を使用するという発想の転換にあるのかと。
そしていずれにせよas codeであることが求められる。AWS上のマネージドサービスはもはや「インフラ」という言葉で括るにはふさわしくないように思いますが、システムおあらゆるレイヤーをcodeで管理し、アプリケーションと同じCI/CDのサイクルに載せてDevOpsで回していくことはもはや必須になる。そこで使うツールには選択の余地があって、Ansibleでもある程度AWSレイヤーをまかなえたりするし、一方でマネージドサービスはTerraformなどを使う方法もある。そのあたりの匙加減が難しい。
さらに言えば、まだツールが充実しきったとも言えない気がするんですよね。GitHubへのcommitをトリガーとしてサーバーの設定変更まで行うような、インフラCIの手法はまだ確立しきったとは言い難い（そういえばCodePipelineにOpsWorksが対応しましたね）し、Serverspecのようなインフラテストツールもより広がる必要がある。インフラの考え方はここ数年どんどん変化してますけど、まだまだ本格的な動きはこれからなんじゃないかという気がしています。</description>
    </item>
    
    <item>
      <title>インフラエンジニアなので第5回ペパボテックカンファレンス行ってきた</title>
      <link>https://you.github.io/post/pbtech_infra_engineers/</link>
      <pubDate>Sat, 14 May 2016 21:58:32 +0900</pubDate>
      
      <guid>https://you.github.io/post/pbtech_infra_engineers/</guid>
      <description>第5回ペパボテックカンファレンス〜インフラエンジニア大特集〜 - connpass
そもそもインフラ向けのイベントって相対的にはやっぱ少ない気がするし、かのペパボさん主催のイベントだし行く以外の選択肢はなかった。アプリ系のイベントだとだいたいが「Pythonエンジニア」みたいに言語ごとだったり、フレームワークまでテーマが絞られたりするなか、「インフラエンジニア」ってよくよく考えたらめっちゃ広いよねって感じだけど、実際の職務もまぁそんな感じだし、今回の話もだいぶ話題としては多岐に渡っていました。
自分の最近の関心事としては社内にAnsible導入させたいなと思いながら格闘しているのと、もっとインフラの低レイヤーの知識深めたいなと思っているのとがあるのだけど、その両面について話が聞けてとてもよかったです。
インフラのコード管理   前者はdrone.io上にdocker-on-dockerでインフラCI環境を作っているという話。どうもdrone.ioだとベースイメージのバージョンが切り替えられないらしい。自分は過去にAnsibleとServerspecによるCIをCircleCIで走らせる構成を作ったことがあるのだけど、これを社内で導入するなら確かにこうなるんだろうなぁという感じだった。
Ansible + Serverspec + Docker + circle ci によるインフラCI · the world as code
後者はあるある、というかAnsible導入検討のなかで自分としても浮かんでいた課題で、コード管理しててもそれを更新する「人手」が必ずしもパーフェクトではないので、コードと実機状態の乖離が出る問題。解決策としてはPuppet、Serverspecと実機の照合を毎日夜間に回して、差異にすぐ気づけるようにするというものでした。わかる。すげーわかる。懇親会でも話していたのだけど、sshして作業しちゃいたいのグッと我慢してPuppetちゃんと書き換えるの大事。個人的にはPuppetとServerspec両方回すのもなかなかしんどそうなので、Serverspec単体を構成変更監視みたいに使うのでもいいかなと思う。
他に印象的だったのは開発側もPuppetを触る、つまり自らインフラの設定変更に手を出せるという話。現状弊社だと、フォルダ1つ掘るのでも、ファイルのコピーでも開発から依頼を受けて運用でsshするみたいなテンション上がらない運用なので、開発にコードでそれを書いてもらえるのはとても効率的に思える。インフラをコード化する一番の意味って、開発と運用という垣根を超えて、インフラを語る共通言語を技術者全体にもたらせることなのだろうなと。まさにDevOps。
あとそういえば「Infrastructure as Code」って言葉を聞かなかった。単語として長いなぁとは思ってたから自分も今度から「インフラのコード化」って言います。
低レイヤー スライドまだ出ていないみたいだけど、ext4からxfsへの移行検討にあたって、結構ガッツリとベンチマークテストやったというLTがありました。クラウド全盛な今日このごろでも足回りは結局物理なんですよというのはまさにその通りで、自分は全然このへんの知識がないのだけどちゃんとやらなきゃなと。。。反省。。。
そして何よりペパボのプライベートクラウド&amp;rdquo;Nyah&amp;rdquo;の話ですねー。
 いやもうすげーわー。あこがれるわー。OpenStack使うにあたり社内別グループ会社に経験者がいたので知見を聴けたとか、一気に5バージョン飛ばして最新バージョンへ追随させるとか、技術に対する姿勢が会社も個人もとても前向きで素敵な話だった。真似しようとして簡単に真似できる話ではないけれど、姿勢や方針については本当に参考になった。
懇親会 インド人完全無視カレーおいしかった。トムヤムクン入れてるのか。なるほど。
インド人完全無視カレー | インド人のアドバイスを完全無視！－カラメル
ペパボの方と実際に話せてよかったです。なんか新しいもの社内に導入したり、広げていったりとなると「結局コミュニケーションだよね」という結論に至ったのでそこは頑張るしかないのかなぁ超ニガテ。あとKPIちゃんと出して比較しないと響かなかったりするってのは確かになという感じなので、自分も腐らず頑張らなきゃなと思い新たにしました。スーパーエンジニアすげーすげーと言っても始まらんし、やりたいならちゃんと自分でやらなきゃなーと。いやー行った甲斐がありました。ありがとうございました。</description>
    </item>
    
    <item>
      <title>#qpstudy 響け！アラートコール！行ってきた</title>
      <link>https://you.github.io/post/qpstudy_alert_call/</link>
      <pubDate>Sun, 24 Apr 2016 22:28:32 +0900</pubDate>
      
      <guid>https://you.github.io/post/qpstudy_alert_call/</guid>
      <description>#qpstudy 2016.04 響け！アラートコール！　本編 一般枠 on Zusaar
こちら参加してきました。アラートコール、というか監視運用をテーマとした勉強会。qpstudyことキューピー3分インフラクッキングについては、数少ないインフラ系の継続的勉強会ということで、気になってはいましたが初参加できました。
以前にも障害対応をテーマとした勉強会に行ったことがありますが、この界隈は闇が深い。。以前のそのイベントも今回のイベントも、いずれも参加者が互いの経験を話し合う場があったわけですが、やっぱりそういうのが一番効果あるかもなぁという気がしました。特に監視運用についてはイベント内でも触れられた通りノウハウや勘に頼っている部分も大きく、他社がどういうノウハウに頼っているのか？というのはとても気になるところ。例えばサービス運用全般のガイドラインとしてITILがあるように、客観的な基準があればいいわけなんですが。
で、監視の基準。これは確かにもう少し考え直した方がいいのかもなと思った次第。例えばメトリック監視ってよくありますけど、仮にCPUが90%使用率達したとして、それがすぐに何か異常に繋がるわけではないのですよね。だから「障害」として扱うべきは単純な閾値超過やエラーではなくて、システム的な動作不全であるはず。それと障害予兆にあたるようなワーニングメッセージは別で扱うべきであって、何が本当に必要な監視、アラートなのかというのは、どの会社でも洗い直すと結構ボロが出てきそうな気がしました。イベントではMakerelのような監視系のSaaSがフレームワークを提供してほしいという声もあったり。あとはAIによる判別。確かにメトリックやログの状態を機械学習させれば、障害予兆をAIで判断させることもできそうな気がします。
今年度に入ってからの自分の社内ミッションは、わりと自動化に重きが置かれているのですけど、イベントでは「自動化は目的ではなく手段」という話もあり。確かに自動化自体が楽しい作業なのでついついなんでも手を出すけど、何のために、またどういった効果があると考えられるから自動化するのか、あるいは自動化の手段には何を用いるのかというところはもうちょっと考えたい。「SaaSを使わない理由って何？」って話もあったけど、そういえばそうだなと。まぁ外部にメトリックやIP持たせるのが嫌、という理由で弊社の場合は通らないかもなぁという気もしますけど、SaaSの導入だって要は監視システムをDIYするプロセスを自動化しているわけで。一考には値するはず。
今回の勉強会はなにか結論をバーン！と提示してくれるものではなく、考えるきっかけを与えてくれるような形式だったので、明日以降ちゃんと社内に持って帰って再検討しようと思います。qpstudy、楽しいので次回もぜひ行きたいところ。
   Re: 運用に自動化を求めるのは間違っているだろうか  from Masahito Zembutsu</description>
    </item>
    
    <item>
      <title>Pythonに入門している</title>
      <link>https://you.github.io/post/entry-python/</link>
      <pubDate>Mon, 28 Mar 2016 22:29:39 +0900</pubDate>
      
      <guid>https://you.github.io/post/entry-python/</guid>
      <description>Pythonを学び始める 今年の 行動規範 でも書いた通り、Pythonに入門している。きっかけはAWS LambdaがPython対応しており、またAnsibleもPythonで書かれているということで、Pythonの読み書きが出来た方が今後良さそうだなと思うに至った。これまでRubyをよく書いていたけど、Linuxにデフォルトで入っているのはPythonやPerlという現実的な問題もある。
今までにやったこととしては取りあえず本を2冊読んだのと、一昨日は 入門者向けのPythonハンズオン に行ったりしてみた。基礎文法はだいたいさらって、requestsのようなポピュラーなライブラリは試してみた程度。
Pythonチュートリアル 第3版posted with amazlet at 16.03.28Guido van Rossum オライリージャパン 売り上げランキング: 24,079
Amazon.co.jpで詳細を見る Pythonエンジニア養成読本［いまどきの開発ノウハウ満載！］posted with amazlet at 16.03.28技術評論社 (2015-06-16)
売り上げランキング: 42,458
Amazon.co.jpで詳細を見る なおPythonチュートリアルはつい先日改版が出たのでそっちをリンクしてます。自分が買ったのは先月です（白目）
Pythonに対する小並感 自分はアプリ屋ではないので言語に対する知見は広くないのだけど、なんとなく感じているのはこんなところ。
 PerlのTMTOWTDIに対して&amp;ldquo;There should be one&amp;rdquo;という考え方が明確で好き。 難読な記法というのが今のところあまりない。発想した通りに書いてだいたい動く気がする。 インデントでブロック形成するのはcoding styleの戦争が起きなくていい。  ただし自分はインデント＝スペース2つ派だった。Pythonは4つの方が確かに見やすいけど。  バージョン2.x vs 3.xの話、外から聞いてはいたけどなにこれ面倒。 新参としては3.x学びたいけど、AWS Lambdaが2.7対応だし両方押さえようとしている。 pyvenvの環境の隔離の仕方がシンプルで好き。pyenvというのもあって紛らわしいが。 というか全体的にシンプルなコンセプトで作られている印象。  2.xと3.xの両輪を回さなくてはならないことを除いては、全体的にはシンプルだし書きやすくてよいなという感じがする。インフラ自動化便利ツールもそろそろなにか書いてみたい。
新しい言語の学習方法 あと言語学習ではいままで「とりあえず書く」というのを手法にしていたけど、複数言語を学んでみて徐々にわかってきた勘所が2点ある。
文法で押さえるべきポイントは決まっている 例えばelifかelsifかelse ifかとか、false判定されるのはnullなのか0なのか&amp;quot;&amp;quot;なのかとか、複数言語を並行して遣うときに迷うポイントはわりと決まっているので、そこさえ押さえればとりあえず書ける、というのがある気がした。
チートシートを自分用に作るのも漫然と端から文法を並べ立てるのではなく、こういうポイントに限ったものにすると効率がよさそう。
読むのも勉強 書くのではなく読むのも勉強。よく言われることではあるけれど、これまであまり意識していなかった。冒頭に挙げた通り、PythonではLambdaとAnsibleという明確にきっかけとなったツールが存在しているので、これらのコードをしばらく読んでものにしてみたいと思う。
あと個人的には最近インフラ界隈でもわりとgolangが話題で気になっているけど、それはまたおいおい（実はgo製OSSを修正して使いたくて、ちょっとだけかじってはいる）。</description>
    </item>
    
    <item>
      <title>Personal Knowledge Base 2</title>
      <link>https://you.github.io/post/personal-knowledge-base-2/</link>
      <pubDate>Thu, 17 Mar 2016 22:09:45 +0900</pubDate>
      
      <guid>https://you.github.io/post/personal-knowledge-base-2/</guid>
      <description>年初にPersonal Knowledge Base · the world as codeという記事を上げて、メモや知識の管理をする環境作りを進めていたのだけど、最近ようやく固まってきた。結論としては Trello と gollum を使っている。
Trello 近年よく名前を訊く、Kanban形式でのビジュアライズされたタスク管理を可能とするツール。タスク管理向けなので当初は目を向けてなかったのだけど、よくよく冷静に考えてみるとメモ管理にかなり適してそうだったので採用。
結果として、タスクに限らず、数多の情報を整理するツールとしてとても使いやすい。ポイントはいくつか。
 メモがカードの形で表示されて、パラパラと繰って一覧できる。 メモにラベルを付けると色で表示されるので視認しやすい。 全文検索が可能。 エクスポート機能あり（json）。 descriptionをMarkdownで書くことができる。 Android、iOSいずれもアプリあり。IFTTTも対応。  単なるテキストメモを保存する用途であればEvernoteの上位互換だと感じる。何よりも画面全体にメモを並べることができる一覧性の高さがいい。
今の自分はPoICのような使い方をしていて、とにかくメモを書き溜めては記録、発見、参照の3つのリストに取りあえず分けて、暇なときにつらつら眺めたりしている。分類の仕方はまだ模索中なので、今後変わるかもしれない。とにかく柔軟に使えるのがいい。
 「Trello」というアプリがおもしろい - iPhoneと本と数学となんやかんやと Using Trello for Your Personal Productivity System | Victor Savkin  GTDは「高度」の管理用といわゆるToDoの管理用に2つボードを設けて使ってみている。プライベートのタスクだけなのでそれほど厳密な期限管理などは必要ないし、これで十分。
gollum 散発的な思いつきをTrelloに入れる一方、体系的な知識管理はgollumを使う。GitHub Wikiの機能だけが単独でオープンソース化されているもので、さくらクラウドの2万円クーポンがあったのでとりあえずサーバー1台立ててホストしている。
技術的なメモ書き、読書メモ、あとは趣味で行く美術展の記録などはすべてここに溜めている。Wikiだと散逸的にページを作ってしまいがちなので、トップから最大2階層までの作成と定め、1階層目は各技術ジャンルのページ、2階層目に詳細記事として配置した。こうして体系立てたメモ環境を作ってみると、自分のスキルマップが出来上がっていくようで面白い。
gollumを建てるのに、技術的に難しいことはほとんどない。中身はSinatraとgitなのでカスタマイズもしやすく、とりあえず安易な認証機能ぐらいは追加してみた。作成した記事は個別にMarkdownファイルになってgollumのディレクトリ直下に直置きされるのだが、これはディレクトリを切ってgit submoduleとしてGitHub上に上げ、個別管理したいかなと思っている。
ナレッジ管理の必要性 ナレッジ管理が必要である、という潮流は昨今高まっているような気はしていて、Qiitaやesa.ioの登場あたりから特にそういう話はよく聞く気がする。
  情報共有から始めるチーム開発とキャリア戦略  from Takuya Oikawa 
個人であれ組織であれ、我々の仕事は（まぁ他の職種も同じだとは思うが）ナレッジを溜めて活用していくことにあるので、こういう取り組みは何がしか進めるべきだろうと思う。上記スライドにもあるように、暗黙知を形式化していくことで自分内でも、社内やコミュニティ内でも知識を記憶、伝達してさらに深めることが可能になる。
とはいえ昨今はツールの乱立はあって、ブログとQiitaの使い分けだったり、個人的なメモをどうするかだったり、やっぱり迷うことも多い。そこはある程度自分なりの使い方を定めておかないと、後々散逸したメモの山に途方に暮れたりはしそう。あと、単純に良いツールがあったとしても、メモする習慣をつけておかないと意味がないし、どちらかといえばそっちが大事なんではという気もする。些細なことでも漫然と調べず、記録するクセをこの2ツールで付けていこうと思う。
以下、他に比較検討したツール群を載せておく。
FAQT  比較した中ではだいぶ惹かれた。一時はこれにしようかと思った。 明確にKnowledge baseを唄ったサービス。Markdownで書いたメモがカード形式で表示できる。 Markdownプレビューが結構好みだし、外観はとてもよかった。 まだ立ち上がったばかりで、将来性はちょっと不安。 全文検索の不在が決め手になり不採用。  Simplenote  定番メモサービス。 Markdown対応がほぼ皆無なので不採用。  Quiver  最近少し話題になったMac用ノートアプリ。 タグとノートブックで分類するEvernoteっぽいMarkdownノート。 データファイルをDropboxに置いてクラウド同期が可能。 外観がクールだし、結構使い勝手はよかった。 とはいえMacでしか使えないので断念（持ち歩きPCがLinuxなので）  参考  The Personal Knowledge Management Saga: #1 — The Personal Knowledge Management Saga — Medium Ask HN: What do you use to organize your knowledge?</description>
    </item>
    
    <item>
      <title>JAWS DAYS 2016に行ってきた</title>
      <link>https://you.github.io/post/jaws_days_2016/</link>
      <pubDate>Sun, 13 Mar 2016 11:03:24 +0900</pubDate>
      
      <guid>https://you.github.io/post/jaws_days_2016/</guid>
      <description> 昨年に続き。
JAWS DAYS 2015でAWS童貞捨ててきた · the world as code
昨年JAWS DAYSのハンズオンでAWSアカウント作ったので、これでAWS歴丸1年。それにしては自分の力がしょっぱすぎて嫌になる。もっと活用したい。ただ今回「わからない」というセッションはなかったので（技術メインじゃないセッションをいくつか入れてたのは置いておいて）、それなりに知識はついているのかなぁという実感はできた。
サーバーレスアーキテクチャ、クラウドネイティブという言葉が極めて一般的になったなというのが今回の感想。事例を聞いていてもEC2並べてなんかやりますというのは当然ながらほとんどなくて、だいたいがLambda、Kinesis、API GatewayといったAWSリソースを繋ぎ合わせることでアーキテクチャを作り出している。もう従来的な言葉で言う「インフラ」というものは存在しなくなってしまった。VMwareやXenの時代というのか、あくまでサーバーという実体は変わらず、それの扱い方が変わるというだけの変化だったが、AWSがもたらしているのはサーバーそのものの消失。システムのアーキテクチャ自体の転換。何度かブログ内でも繰り返しているが、この全く新しい領域で勝負するにあたっては、アプリエンジニアもインフラエンジニアも関係ない。こういった状況下で自分が勝負できるとしたら、Opsになるのかなと思う。これまでインフラ屋が担ってきたOpsのスキルを、AWSへ適用していく。もちろん、コードをもっと書けるようにならなくてはならないといった、本来的な問題もあるが。
また今回全体を通して「コミュニティへの参画」というのが強調されていたようにも思う。最初のJAWS-UG代表である今春氏のセッションでも、Increments及川氏のセッションでも、社外のコミュニティに参加することの意義が説かれた。自分はお世辞にも積極参加ができているとは言いがたい状態なので、とても耳が痛い。
参加セッション JAWS-UGこれまでとこれから（キーノート）  先述した今春氏のキーノート。 京セラドームでもイベントを開いたことがあるというのはちょっと驚き。思い切ってますなぁ。 会場に出来る会社様募集中らしい。企業が無償で場所提供してくれる業界状況というのも非常に恵まれている。というか、そういう文化圏にいて良かったと思う。  AMIMOTO × サーバーレスアーキテクチャ  デジタルキューブ堀家氏。 先述の通り。サービスを疎結合にする。No EC2。 モバイル開発者じゃないからスルーしてたけど、mobile hub面白そう。 Serverless Framework気になる。 説明のあった「Lambdaをローカルからinvokeできる」ぐらいならAWS CLIでいいんだけど、他にも活用できそう。  外よりも中からの攻撃・事故がヤバイ、今やるべきクラウドセキュリティ対策  cloudpackのアイレット齊藤氏。今回一番ためになった。 セキュリティに終わりはない、完璧はない、だからきちんと監査して透明性を確保して、リスクを受け入れる。 監査は目的ではない。むしろ時代に追いつけてない監査基準もあったり。 パスワードは脆弱だからSAMLとKerberosでAD連携してSSOとかカッコイイ。 cloudpackでやっていることは公開されているらしいので後でチェック。  [Deep Dive] AWS IoT  mobile hubと同じく敬遠してたIoTだけどなんかやれそうな気になった。 とりあえずラズパイ入手かな。あとPython頑張る。  金融クラウド＆FINTECH最新動向　～AWSで金融のイノベーション！  AWS、MUFG、ウェルスナビの共同セッション。 MUFGがハッカソンとかやってるんだというのは全然知らなかったし意外オブ意外。 時代の潮目が確実に変わってきてるのかなと。 銀行システム、基幹系は今まで通り守らなくてはならない。フロントはスピードを速める必要があると。 その意味でSIerは失くならないと個人的には思っている。少なくとも金融系は。 ウェルスナビ、これまで米富裕層に提供していたような投資診断をモバイル向けで一般にも提供すると。 ITの役割が単なる自動化や効率化じゃなくて、新たな価値、市場を創造することにある。そういう仕事したい。  エンジニアのキャリアとアウトプットを意識した成長戦略  Increments及川氏。 話題になったIncrements入社の理由が「誰もやりそうにないから、差別化戦略として」ってのカッコよすぎた。 及川氏の最初のキャリアであるDECという会社、不勉強につき初耳。紆余曲折の末に現在はhpの一部か。。。 Googleの社是？ &amp;ldquo;Share everything you can&amp;rdquo; Qiitaにもっと投稿しようと思いました（こなみかん）  </description>
    </item>
    
    <item>
      <title>ブログをHugoに移行した</title>
      <link>https://you.github.io/post/convert-to-hugo/</link>
      <pubDate>Thu, 18 Feb 2016 20:51:20 +0900</pubDate>
      
      <guid>https://you.github.io/post/convert-to-hugo/</guid>
      <description>すでに流行りは一巡しているような気もするが、Hugoを導入してみた。もともと自宅iMacにOctopressを置いてブログ作業はしていたのだが、外でもブログ更新ぐらいできた方がいいなぁと考え、クラウド上の開発用端末にレポジトリ移しちゃおうということになり、じゃあついでだからと移行してみた。Go自体は最近使っているオープンソースがそれであったという試しがあり、すでに導入は終えていた（ただし書けない）。
随所で語られているように移行自体は大したものではなく、Markdownでいずれも互換性があるし、Front MatterもYAML形式であれば同一。Hugoレポジトリのcontent/post配下に記事ファイルを突っ込めば移行としてはおしまい。多少の差異については以下の記事が詳しい。
OctopressからHugoへ移行した | SOTA
ただ自分の場合はパーマリンクを前ブログから保てていない。というのも、Octopressで使っていた記事ファイル名がYYYY-MM-DD-foobar.markdownの形だったのに対し、今回は記事のパーマリンクをconfig.tomlで以下のように設定してしまっている。
[permalinks] post = &amp;quot;/blog/:year/:month/:day/:filename/&amp;quot;  従って/blog/YYYY/MM/DD/YYYY-MM-DD-foobar/という歪なパーマリンクになってしまっている記事がいくつかある。ほとんどの記事はfoobar.markdownに直したのだが、はてなブログ時代から移植した記事はYYYY-MM-DD-post.markdownという適当なパーマリンクにしていたので、一括して直すことができなかった。時間を見てこれらも意味のあるURLに直すつもり。
記事を公開する流れは以下のようになる。
# 記事作成 $ hugo new post/title.md $ vi content/post/title.md # ビルド $ hugo # commit $ cd public $ git add . $ git commit -m &amp;quot;new post&amp;quot; $ git push origin master  hugoコマンドでビルドするとpublicフォルダにサイト構成全体が吐かれるので、それをそのままgit pushして終わり。ただ実際にバージョン管理したいのはpublicというより、設定ファイルや元のMarkdownが詰まったHugoのレポジトリ全体ではないかという気もするので、後々以下の記事のようにレポジトリ全体でgit pushしてCIでビルドさせる形に変えたいと思う。
HugoとCircleCIでGitHub PagesにBlogを公開してみた - Hori Blog
なおテーマはとても悩みどころで、しばらくコロコロ変わるかもしれない。というか自分でカスタマイズしたいけどCSSなんて今更書けるのか。。。
（追記 2016-02-22 23:50） フィードのファイルパスがデフォルトだとindex.xmlになってしまうので、Octopressから変更がないようatom.xmlに直した。config.tomlで指定ができる。
rssuri = &amp;quot;atom.xml&amp;quot;  参考：What is {{ .RSSlink }}, exactly?</description>
    </item>
    
    <item>
      <title>Ops JAWS#3に行ってきた</title>
      <link>https://you.github.io/post/ops-jaws-3/</link>
      <pubDate>Sat, 30 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/ops-jaws-3/</guid>
      <description>その名の通り運用管理系の話題を中心としたAWSユーザーグループです。ハンズオンもあるということで行ってきた。
メインとなったのはconfig rulesのハンズオン。
  OpsJAWS 20160128  from hideaki yanase 
AWSリソース、使っているうちに無秩序になっていき、ルールの統一がはかれなくなったり、全体像が見えづらくなったりということはありがちですが、config rulesを使ってもうシステム的に制御しちゃいましょうというテーマ。例えばCloudTrailが有効化されていない場合にアラートを上げる、とか。監視結果が変化すると、それをトリガーにLambdaをinvokeしたりもできるので、それこそなんでもできる感じ。
やってみて気付いたけど、やっぱり自分は運用が好きなのかもしれない。システムによって、本来不確かであったり信用性に劣っていたりするはずの人間の動作を制御する、というのが好きなんだろうなと。SEやってた頃は運用の制御はExcel資料が元になることが多くて、それ自体は特に楽しくなかったのだけど、システム的に作りこんでいくのはわくわくする。これはいい気付きだったし、次回も是非参加してみたい。
問題としてはやはり、Lambda Functionを書くのにpythonかnode.jsを使う必要がある（いまさらJavaってのもなぁ）ので、次回参加するのであればそれまでにpythonをある程度やっておかねばなぁというところ。
その他、昨年とてもおもしろく読ませていただいたSDの特集『なぜ「運用でカバー」がダメなのか』を書かれた運用設計ラボの波多野氏がいらっしゃっていたりして、個人的にはテンション上がったりもしました。「運用でカバー」をググるとトップに出てくる、なんだか好評を得てしまった拙記事はこちら（あえて移行前ブログを貼るアレ）。
Software Design 2015年2月号『なぜ「運用でカバー」がダメなのか』読了 - そのねこが学ぶとき</description>
    </item>
    
    <item>
      <title>Personal Knowledge Base</title>
      <link>https://you.github.io/post/personal-knowledge-base/</link>
      <pubDate>Sun, 24 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/personal-knowledge-base/</guid>
      <description>昨年は転職のゴタゴタがあったりしてメモを取る習慣というものがどこかに消え失せてしまっていたので、年始にあたり生涯幾度目かわからないがメモ環境について再考している。本当に何度目だよ、と思うのだが、これはもう生涯模索しながらいくしかないし、最適解なんてのは時と場合によって変わるものだとは思う。
しかしオライリーから『エンジニアのための時間管理術』は出ているのに、『情報整理術』が出ていないのはなぜなのか。むしろそっちが職種的に重要じゃないのか。ブログやQiitaでもあまり見かけない。
Evernoteの呪縛 現代においてメモ、ノートというと真っ先に挙がってくるのがEvernoteであり、自分も確かに使ってはいる。使ってはいるが、正直に言って愛憎は入り混じっている。基本的には使いたくない。でも使わずにはいられなくて、ついついいろいろとぶち込んでしまう。
Evernoteの肝は、ありとあらゆるフォーマットの資料を何でも入れることができて、それがsearchableになることにあると思っている。昨日読んだブログのエントリー。1年前に契約したサービスの証明書。3ヶ月前に読んだ本の感想。そういったものをすべて並列に保存することができ、検索すればすぐに出てくる。放っておけば消えて無くなるような情報が「死蔵」されなくなる。だから取りあえず「いいな」と思ったものがあればEvernoteに入れてしまう。特に開発終了が発表されたClearlyが自分にとってはクリーンヒットで、良いなと思った文章があれば迷わずClearlyを使っていた。ググればまた出ると言われればその通りだが、同じ検索キーワードをまた思い出せるとも限らないし、ページが消滅することだってままある。
しかし実際に突っ込んだ情報をもう一度掘り出せるのかどうか。文字情報であれば確かに検索できるのだが、画像はどうか。音声は。またpdfは。それを防ぐためか、ノートブックやタグ付けという能動的な整理手法も用意されているわけだが、次々と投げ込んだ資料を1つずつ分類していくのは骨が折れるし、メールクライアントにあるようなオートフィルタリングはいまだにできない（サードパーティーのアプリならあるけど）。また実際に検索をかけたときも、ノートの読み込み速度はそれほど速いものではなく、特にモバイルアプリに関してはどうにもストレスフルだ。
結果的に何でも入れられるがサルベージが難しいゴミ箱、あれば安心感があるので離れられないけど、積極的に何か活用していこうとは思えない存在と化してきている。
何をメモするのか そもそも何をそんなに悩むほどメモしたいのか。改めて考えるとよくわからないなぁで思考が停まりそうになったが、いくつか挙げてみる。
 Tech関連でも日常の中の疑問でもそうだが、ググるのは簡単だがその知識はすぐ忘れてしまう。どこからどんな情報を得て、どんな結論に至ったのかは書き留めておきたい。 スニペットやチートシート。頭悪いのでコマンドや文法をすぐ参照できるようにしておきたい。 読書メモ。簡単な本の概要、感想、それを受けて何を実践するのか。 ポエム。客観的な事実や資料より感情より、主観よりのもの。課題に関する考えとか哲学とか。 チェックリスト。日常の指針になるような。定例作業の手順もそうだし、持ち物リスト、忘れがちなポリシー的なものとか。  こうして挙げてみると参照頻度、パブリックorプライベートといった軸で分類できそうな気がしてくる。またそれによって選ぶべきツールも変わりそうだ。例えばEvernoteは先に書いた通り遅い、分類が面倒という特徴を自分は見出しているが、逆にそれほど素早く引き出す必要のないもの、つまり参照頻度が低い「もしものためのメモ」などであればEvernoteでも構わないことになる。
スニペットやチートシートは本当に秒で出てきて欲しいし、シンタックスハイライトが効いてないと辛いみたいなところがあったりもするので、Evernoteには向かないことになる。この目的だと自分の中ではGistやKobitoが最近のヒットではある。
思うに、Evernoteや梅棹忠夫先生が掲げるような「すべてのメモを一箇所に集める」というのはちょっと厳しいのではないか。目的の違うメモであれば、適切なフォーマットも自ずと変わってくる。もちろん分散していろんなところにメモがある状況というのはわかりにくくはあるが、目的がはっきり定まっていればツールの選択に迷うことはない。自分はEvernoteにスニペットを格納したことはないし、日常生活で使うチェックリストをGistのPublicで保存したこともない。
Personal Knowledge Base ここでようやくタイトル回収するのだけど、海外ではこの手のツールをPersonal Knowledge Base(PKB)と呼ぶらしい。Wikipediaの記述の細かさを見ると、国内よりはだいぶホットな話題っぽく思われる。いろいろこのワードでググッてみたのだが、今のところうなずけたのは以下のあたり。
 Designing a Personal Knowledgebase – A Curious Mix Designing a Personal Knowledgebase | Hacker News How do you manage your knowledge base? - Programmers Stack Exchange  特に一番上の記事はだいぶ熱い。俺の理想とするPKBはこんなのだ！！！ってめっちゃ細かく書いているが、わりと同意できる内容ではあった。下2つのフォーラム系の記事を見ると、案外多いのが個人Wikiを使っている人。確かにフレキシブルな編集が可能という点では、2016年現在に至ってもWikiの優位性はかなり高い気がする。でもさすがに今更感あるなーということで手を出す気にはなれない。あるいはorg-modeが結構評判よくて、Vimmerじゃなければ手を出していたようには思う。
PKBの定義に関してはWikipediaの記事にあるが、情報の一次ソースそのものではなくて、そこから得られた知識をまとめるものということ。
 Its purpose is not simply to aggregate all the information sources one has seen, but to preserve the knowledge that one has learned from those sources.</description>
    </item>
    
    <item>
      <title>東京Node学園付属小学校1限目に行ってきた</title>
      <link>https://you.github.io/post/node-js-elementary-school-1/</link>
      <pubDate>Wed, 20 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/node-js-elementary-school-1/</guid>
      <description>【増枠！】東京Node学園付属小学校 1時限目 - connpass
東京node学園というNode.jsのユーザーコミュニティがありますが、その入門者向けバージョンが立ち上がったので行ってきました。
自分がNode.jsを扱った経験はAWS Lambdaとhubotで遊ぶために既存のスクリプトを少し触ったぐらいで、おそらくは小学校というより幼稚園レベルだったと思いますが、それ以前にやはりフロントエンドの集まりにインフラのエンジニアが行くというのはちょっと自分でも場違い感を覚えずにはいられず、さすがに提供できる話もないやろなってことで懇親会は出ずに帰ってしまいました（）。しかし刺激になったのは確かというか、フロントエンドの世界にちょっとだけ触れられる良い機会ではあったかなと。
そもそもにして自分にとって「言語」は最近手足になってきていて、Ansible使いたいからPythonやっておこうとか、Docker理解したいからgolangかなとかそういう選択ばかりするようになっていたのだが、フロントエンドにとって新しい言語というのは可能性の広がりなのだなーと当たり前のようなことに気付いた。例えば最近Kobitoの実装などで話題のElectronはNode.jsなわけで、サーバーサイドスクリプトであるNode.jsを学ぶことで、デスクトップアプリケーションを従前よりは容易に構築できる可能性になる。作れるものの幅が増える、やれることが膨らんでいくことはエンジニアにとってとても楽しい。
くっだらないものでもなんでも構わんから、とりあえず手を動かして「作る」ことが楽しいって経験をもっとしてもいいのかもなと思った。インフラのデリバリー、運用の効率化、そういうのも大切ではあるけれど、我々がそもそもビジネスとして提供しているサービス、システムとはなんぞやって部分をもう一度見返してみたい気がした。言語は単なるツール、ではないはず。だからこういう勉強会もいいけど、ハンズオンとかもっと行ってみるべきかなと。具体的に今回の勉強会で見かけた中ではMEANスタックが気になるのでやっておきたい。ちょうどMongoに手を付けたかったし。
以上、取り留めのない感想でした。</description>
    </item>
    
    <item>
      <title>2016年の行動規範</title>
      <link>https://you.github.io/post/manifesto-2016/</link>
      <pubDate>Sun, 03 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/manifesto-2016/</guid>
      <description>うまいことまとまらないのでつらつら。
問題意識  時間、お金の使い方がいまだに下手。  きちんと考えず浪費している機会が多い。 時間ならタスクシュート、お金ならZaimを使ったりした経験はあるが結局とまったりしている。 昨年は10年以上続いていた日記すらもとめてしまった。 何か忙しい課題が仕事なりプライベートなりに鎮座すると、他に回す手が一切なくなったりしてバランス悪い。  メモ環境の再考。  インプットが「ググって終わり」の場合が多く、次にその情報が必要になったときも結局ググってる。 読んだ本が本当に身になっている気がしない。 Evernoteがメモの死蔵場になっている。 デジタルメモはVimとmemolist.vimを基本にしているけど、書き散らして終わってる感。  技術探求の不足  やってるけどやりきれてないというか。 Qiitaで話題になっているのを見て、ザッと読んでなんとなく無理そうと思うと閉じちゃったり。 いつか使うかな？と思ってPocketにつっこんでそのまんまの積読があまりに多い。 流行ってる→やろうとか、こういうツールほしい→作ろうの瞬発力上げたい。 本読みたい。というか去年文化資本にあまりに触らなすぎた。  仲間がほしい  懇親会とか行ってもその場限りの話しかできなかったりでエンジニア仲間社外にあんまいない。 社内の技術課題解決するのに社外で話すってのも大切そう。 ていうかエンジニアとしてもうちょい知名度上げてみたい。   解消していくために  時間とお金の定量評価  お金は娯楽費にx万までみたいな予算持ってるので、時間も定量評価したい。コード書く時間を週に必ずx時間とか。 タイムロギングしたいけどTogglとか使うの面倒。とりあえずやってみるか。難しけりゃ1日の最後にノートに記憶から書き出すんでもいいかなと。 週ごとのノートにしたい。PDCA回すのにちょうど良いスパンだと思う。今週ダメなら来週帳尻合わせるとか出来るわけで。 そういう用途だと紙ノートよりEvernoteの方が良さそう。1ノートに対して1週間という形が取れるので。 じゃあ日記もここに載ってくる形でいっか。  メモをもっかいちゃんとする  技術テーマごとにちゃんとノート取る。ブログに上げるといった方がいいか。 ブログ記事にならないレベルのものはQiita。二番煎じ三番煎じでQiitaレベルに値しないものはGist。 でもGistあんまり使いやすくない……。 メモを見返す時間をちゃんと作る。週次レビュー的なあれ。金曜夜が第一候補。無理なら土曜。 というかそれぐらいの時間は取れるようなスケジューリングをする。去年はそれすら難しいぐらいアホほど予定入れてた。 頭のなかちゃんと棚卸するの大事。 手元のモレスキンはタイムライン的なリアルタイムメモ用にする。アナログは見返すの無理。ざざっと時系列で追う目的でしか使えない。  散逸的な勉強をしない  とりあえずおもしろそうなもの、役に立つはずのものに片っ端から手をつけるのやめる。 Pocketに記事を置いとくのはいいけど、1週間ぐらい経ったら躊躇なく消す（自動化できないかな）。 上述の通りブログに上げることを目的としてノートを取っていく。参照する記事はノート上で繰り回す。 スーパーマンになろうとしない（選択と集中、less is more）  技術的な瞬発力の向上  手足のように使える言語がほしい。Rubyかじったんだからちゃんとやり切る。 もう1個。デフォルトで入ってる言語だと楽なんだが。Pythonかなー。 LinuxディストリビューションにおけるPython 3デフォルト化の流れ - orangain flavor 実際の開発経験を積む。API叩くとかやる。動かす。 原則としてCLIで操作する。GUIに頼らない。コマンドでなんとかならないかとまず考える。  文化資本に触れたい  1クール3本のアニメ 1か月2冊の小説 1か月2本の映画  エンジニアとしての活動  なんかユーザーグループ入ってみたい。職種的に考えるとJAWS-UG？ これだけはという技術分野ほしい。Ansibleが今自分の中でキテるのでもっと。 GitHubをソーシャルにちゃんと使う。横断的な検索とかフォローとかプルリク出してみるとか。   挑むべき技術分野  上述の通り武器言語としてのRuby、Python。 hubotいじる上でnode.</description>
    </item>
    
    <item>
      <title>2015年総括</title>
      <link>https://you.github.io/post/looking-back-2015/</link>
      <pubDate>Thu, 31 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/looking-back-2015/</guid>
      <description> いろいろとチャレンジングに動いてみただけに、難しい年だった。
今年初めの「行動規範」で書いた通り、就職は、した。大手SIerからいわゆるベンチャー気質な企業に転職して、働き方はだいぶ変わった。毎日Excelとにらめっこしていた生活ではなく、業務上初めてsshを実行し、いまでは毎日実行するような生活になった。
承認と判子と指示が行動の軸だった状態から、個々人のスキルと瞬発力と経験が物を言うような状態になり、率直に言ってついていけてない感はある。どこまでを許可無くやっちゃっていいのかわからない（いわゆるDon’t ask for permission, beg for forgiveness的文化）し、技術検証に手をつけ始めると基礎スキルが低いのでやたら時間を食ってしまい、その間に他の人にタスクを取られたりする。
でもまったく手応えがないわけではなくて、個人開発で使っていたスキルで食い込んでいけるところも大きいし、自分に足りない、学ぶべきことは山のように社内に転がっているので、ひたすら旺盛に吸収していきたいと思う。というかそうしない限り、エンジニアとして生きる道がない。
興味領域としてはDevOps方面にかなりアンテナが伸びてきている。大企業でわりとカッチリ運用設計を認めていた自分が、創業からそれなりの年数が経ち、技術的負債の増えてきたベンチャーに入ったことによる必然とも言えるのだろうが、運用上の穴や非効率な部分がとても目についていて、ルールで縛るのではなく、システム的に運用の統括を図ろうというのが目下の課題となっている。それこそInfrastructure as Codeを使うなりDockerを使うなり、もっと低レイヤーにシェルスクリプトをガリガリ書くなり。だから技術的に磨いて実践していくことが本当に多いのだけど、一方で運用方法の改変ということは社内への浸透が必要になり、政治的な問題にもなってくるので、コミュニケーション力結局必要やんけってところで非コミュな自分は悩んでいる。社内政治ほんとやだ。
まぁ、総じて言えば楽しく仕事はできている。だけど大きな変化は副作用的に予期しない変化を別のところでもたらしたりするものでもあって、そのバランスを取ることがなんとも難しい。貪欲であることと、単に我欲を押し通すこととはまた違うわけで、もう少しコントールが必要だと思っている。リスクテイクしたのだからその分の負担の大きさを覚悟してはいたが、わりと想定以上なところはあってストレスは大きい。年齢も年齢なので、自分が「何をすべきか」という論調よりも、周囲、世の中にとって自分は「何であるのか」という視点で動いた方が良いのかもなという気がしてきた。もう少し、置かれた場所というものも大事にしたい。
抽象論についつい流れてしまったけど、具体的な技術的成果はQiitaを中心に流していこうと思っているので、ブログはポエミーにこんな感じで締めてみる。また来年。
おまけ：2015年定量評価 技術  Linuxのサーバー運用に従事開始。初歩的なコマンドからさらい直せてる。 CentOS7の業務利用開始。 AWSの利用を個人でも業務でも開始。ただしほぼEC2。 Ansible利用開始。 Serverspec検証開始。 Docker検証開始。  イベント  JAWS DAYS 2015 JAWS UG 初心者支部 デブサミ2015 Ansible入門イベント 他社の障害対応気にならNight 手羽の会（ハンズラボ） Serverworks Sonic! OSC東京 2015秋 Rakuten Tech 2015 RubyKaigi 2015 他  書籍  リーダブルコード プログラマが知るべき97のこと それがぼくには楽しかったから ハッカーと画家 UNIXという考え方 インターネットのカタチ Amazon Web Services パターン別構築・運用ガイド シェルプログラミング実用テクニック はじめてUNIXで仕事をする人が読む本 大規模サービス技術入門 Serverspec CentOS7実践ガイド Team Geak オペレーティングシステムの基礎 たのしいインフラの歩き方 他  </description>
    </item>
    
    <item>
      <title>Serverspecファーストインプレッション</title>
      <link>https://you.github.io/post/serverspec-first-impression/</link>
      <pubDate>Thu, 31 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/serverspec-first-impression/</guid>
      <description>秋ぐらいから個人開発で試してみて、最近業務でも使えないかとServerspecで試行錯誤している。はじめに言っておくと使用感もコンセプトもとてもしっくりきていて満足している一方で、技術的なハードルはAnsible等より上かもなと思っている。
サーバー構成の「仕様書」代わりとして 自分は当初Ansibleで構築したサーバーのあくまでテストツールとして使っていて、「こういう設定にしたい」という頭の中の設計書をAnsible playbooksとServerspecに同時に落とし込み、テストが通ることを確認していた。が、実際にじゃあこれを業務内でどう使おうかとワークフローを考えてみると、仕様書的な使い方がメインになりそうな気がしている。
Serverspecによるテストを実行するのはどういったタイミングか。構築完了時点での確認に用いるのは然り。その後サーバー設定を変更したときには、その内容をServerspecにも反映して再度テストを行うはず。つまりサーバーの仕様、設定の変更にServerspecは追従していく。逆に言えば任意のタイミングで仕掛けたServerspecがエラーを吐くことで、不意のサーバー設定変更を検知できる。サーバーの「正」とされる状態を管理する仕様書の代替として、Serverspecが活用できる気がしている。
中にはcronで監視チックに実行させている例もあるようだが、それもアリかなと思う。
導入は簡単だが探求にはRubyスキル必須 Ansibleが実質的にはYAMLを書くだけで使えてしまい、内部実装に用いられているPythonの知識をほとんど必要としないのに対し、Serverspecは徐ろにRubyスキルを必要とする。
例えば私が初めて書いたspec_helper.rbはこんな感じで、公式のtipsを反映したものとはいえ、デフォルト通りでは使っていない。
require &#39;serverspec&#39; require &#39;yaml&#39; properties = YAML.load_file(&#39;properties.yml&#39;) host = ENV[&#39;TARGET_HOST&#39;] set_property properties[host] set :backend, :exec  実際のテスト用のタスクを生成するのもRakefileである。もちろんデフォルトのままでも使えるには使えるのだが、ちょっと凝ったことをしようと思うとRubyが読み書きできていなくては難しい。これは「Rubyにより実装されたインフラテストツール」と理解するより、「RSpecをインフラテストに使えるよう拡張したもの」と捉えた方が正しいように思う。
自分は元々Rubyがある程度書けるものの、RSpecが理解しきれていないので、もう少し勉強しなくてはならなさそう。
国産OSSであるアドバンテージ Serverspecの何より大きなアドバンテージはここではないのか。開発者も国内にいらっしゃるので、Rebuild.fmで直接声が聴けるし、解説本もいち早くO&amp;rsquo;Reilly Japanから発行されている。特にオライリー本発刊時のRebuild.fmは本自体の補完にもなる内容で、開発コンセプトなどがよく理解できるので聴いておきたい。
Rebuild: 75: Book Driven Development (gosukenator)
Serverspecposted with amazlet at 15.12.31宮下 剛輔 オライリージャパン 売り上げランキング: 213,793
Amazon.co.jpで詳細を見る 結論として先述のようにRSpecの拡張的な位置付けであり、その他Infra as Code関連のツールと比べても実装が薄いことから、取り回しがしやすく、今後も継続して使いやすいのではないかと思う。Infratasterとも組み合わせられれば、よりテストの質は増しそう。</description>
    </item>
    
    <item>
      <title>『Team Geek』読了</title>
      <link>https://you.github.io/post/review-team-geek/</link>
      <pubDate>Mon, 14 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/review-team-geek/</guid>
      <description>Team Geek ―Googleのギークたちはいかにしてチームを作るのかposted with amazlet at 15.12.14Brian W. Fitzpatrick Ben Collins-Sussman オライリージャパン 売り上げランキング: 18,890
Amazon.co.jpで詳細を見る ひたすら技術ドリブンに仕事できるのであればそれはそれで良いような気はするが、現実にはちゃんとコミュニケーション取る必要はあって、何かやりたいことがあればいわゆる政治的な課題に悩まされることになったりもする。本を読むときはつい技術系のものや個人のハッカーマインドに関するものを読みがちだけど、歳も歳だし組織論もかじろうかということで読んだ。元々読むつもりはあったが、紀伊國屋書店新宿本店でオライリーカレンダーのプレゼントやってたので背中押された。
とても元も子もないまとめ方をしてしまうと、KAIZEN Platform, Inc. のエンジニア行動指針がだいぶ本書に影響を受けたと思われるものになっていて、これに全社員がコミットできている状態というのは理想的なのだろうなと思ったりした。本書の内容にはとても賛同できるのだが、「組織論」である以上は自分だけが納得していても仕方なくて、社内でこの内容を文化として定着させなくてはならない。そこのハードルがなかなかに高い。
本書でもそのあたりの話には「組織的操作の技法」として第5章をまるまる当てて触れられていて、例えば「道がないなら道を作る」＝草の根からツールの導入などを始めていく、「許可を求めるより寛容を求めるほうが簡単」、「安全なポジションまで昇進する」といったことが書かれている。結局はできることからやっていく、しかなくなってしまうのかもしれない。
個人のマインドに関する話は大変参考になって、HRT（謙虚、尊敬、信頼）を軸として、「コードの価値を自分の価値と同一視するな」というあたりもだいぶクるものがあった。技術職としては技術的価値の優劣が極めて大きな価値をもっていて、ともすれば「モヒカン」だとか「マサカリ」といった言葉が表すような事態になりかねないのだが、チームが円滑に動くためにはそういったものは障壁となりかねない。技術的に未熟であるメンバーについても、謙虚に対応していくべきだし、また自分の技術は粛々と磨いていくことが必要なんだろうなと。
こういう本は一人で読んでもやっぱり仕方がないところがあるので、チームで買ってシェアしたりもアリかもしれません。</description>
    </item>
    
    <item>
      <title>Dockerファーストインプレッション</title>
      <link>https://you.github.io/post/docker-first-impression/</link>
      <pubDate>Fri, 04 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/docker-first-impression/</guid>
      <description>前回上げたインフラCIを試みた際、CircleCIを利用する中で初めてDockerに触れたので、今更ながらのファーストインプレッション。
「仮想マシン」と考えるとDockerは理解しづらい Dockerを「仮想マシン」と称する文章も随所で見かけていたが、これを仮想マシンとして捉えると理解からは遠のく。自分自身、Dockerの概念的な理解にはかなり手こずっていて、OSがないのにどうやって「マシン」が動くのかわからなかったし、 chroot のようにファイルシステム上に仮想的なディレクトリツリーを設けるものなのかと思っていた。
Dockerは隔絶された名前空間上で展開されるプロセスに過ぎない。確かにコンテナはOSのような振る舞いを見せるが、そこにいわゆるVMwareやXenのような仮想「マシン」はない。あくまでホストOSの一部リソースを切り出して、仮想的に扱う技術に過ぎない。
Vagrantの代替？ 一度理解して、 docker run を叩けるようになると利便性は即座に理解できる。OSをブートさせるわけではないのでコンテナの起動は従来の「仮想マシン」と比べて格段に速く、CircleCIで使われている用途同様、テスト用にまっさらな環境が即席で欲しいときには大変重宝する。こういう用途にはもともとVagrantが適していたのだと思うが、本当にすぐ使い捨ててしまいたいようなOS環境であれば、Dockerを使ったほうが遥かに手軽に起動も破棄もこなせる。
ただあまりに簡単に起動、破棄ができるものの、作成したコンテナのイメージはコンテナ終了後も基本的に残存するので、調子に乗っているうちにいつの間にかディスクがかなり消費されていることが何度かあった。コンテナライフサイクルの把握と運用整備はマスト。
ポータブルなインフラストラクチャー Dockerを実用できる一例として、先日Traildashを採用する機会があった。
AppliedTrust/traildash
CloudTrailという、AWS APIへのアクセスログをjsonで吐いてくれるAWSサービスがあるのだが、それをElasticsearchで集計してKibanaでブラウザ表示してくれるツール。このツールはDockerイメージで配布されていて、自分のサーバー上にpullしてきて、AWS APIへアクセスするための環境変数をいくつか設定するだけで使えるようになる。自分はElasticsearchの運用経験はないのだが、実質的に docker run コマンド一発だけでそれが使えてしまう。（そのことの是非は置いておくとして）Dockerがアプリケーションサイドで実現することってこういうことなんだろうと。herokuが出たとき、ローカルからインターネットへのサービスのポータビリティが劇的に向上したわけだが、Dockerは稼働先を問わないわけで、ポータビリティはさらに拡大する。
これはインフラ側としても嬉しいところで、今までnginxやらDBやらというミドル的な部分はアプリとしての要求もあり、インフラとしての要求もあり、双方の要件がガッシリ絡んでしまっていて、設定を後から見返すと「これなにゆえにこうなったんだっけ？」ってことが少なくなかったり、構築分担が面倒だったりというのがあって。コンテナとしてアプリをデプロイするとなると、サーバーとコンテナが明確に分離される。疎結合になる。ミドルの調整はコンテナ内だけを気にして行えばよいので、サーバーはとりあえずDocker動いてくれればいいやみたいな状態になる。雑だけど楽だろうなという気がぼんやりしている。
Dockerの運用 とりあえず前述のTraildashはDockerによる本番運用（外に出すものではないので本番といえるか微妙ではあるが）の発端にはなりそうなものの、いわゆるアプリ、サービスを本番稼働させるのがどんなもんなのかってところは自分自身見えてない。これをきちんと本番で扱うには可用性やら信頼性やらを担保しなくてはならないわけで、クラスタ構成に用いるDocker Swarmを導入するだとか、いわゆるインフラとしてのお仕事はやっぱり必要になる。そのへんどこかで試せればなぁとは思うので、ひとまずは自分の http;//chroju.net をDocker化しようかなどと。この前OSCでさくらのクラウド2万円クーポンもらったし、Dockerによる個人PaaS的なものでも作ってみようか。
テストとしての利用には申し分のないところで、先日記事で上げたがAnsibleとServerspecのテストに使えるまっさらなOS環境としてDockerは重宝している。Infra as Codeと大変相性がよくて、よくこのタイミングで出てきてくれたなという感じがする。時代の要請なのだろうか。</description>
    </item>
    
    <item>
      <title>Ansible &#43; Serverspec &#43; Docker &#43; circle ci によるインフラCI</title>
      <link>https://you.github.io/post/ansible-serverspec-circle-ci/</link>
      <pubDate>Wed, 18 Nov 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/ansible-serverspec-circle-ci/</guid>
      <description>CircleCIでDockerコンテナに対してansibleを実行しserverspecでテストをする - さよならインターネット
この記事に書かれている内容を実際にやってみた。Ansibleを一旦は触ってみたところから、Circle.CIどころかCI経験が一切ない、ServerspecとDockerも使ったことがないという出発点だったので、得られるものはだいぶ大きい経験だった。完了したレポジトリは以下。
chroju/ansible-ruby-devs
Ansibleにテストは必要か？ AnsibleはPlaybookに書かれた設定通りにサーバーをセッティングしてくれるツールなのだから、傍証としてのテストは必要ないし、そもそもそれはAnsibleに対する信頼の問題だという話がある。（かのオライリーのServerspec本でも「Serverspecの必要性」を状況に応じて説明した章がある）が、自分は以下の理由からAnsible実行後のテストは必要と考えている。
 Playbookの書き方が間違っている 確かにPlaybookに書いた内容通りにサーバーは組まれるのだが、そもそもPlaybookの書き方がおかしくて、想定通りの実行結果にならない可能性はある。そのレベルであればコードレビューで気付くべきではないかという話もあるが、こういう趣味の個人開発では難しかったり、レビューで漏れがあったりというのも有り得るわけで、自動テストに任せられるならその方が確かかとは思う。
 冪等性の問題 特にshellモジュールを用いたときなどは冪等性が維持されない可能性があり、複数回の実行で想定外のサーバー状態になる可能性はある。
  テストツールの選定 普通にServerspec。Ansibleで定義したインベントリファイルやrolesをServerspecと共有してくれるansible_specというツールもあり、当初はこちらを使おうとしていた。が、前述した「Ansibleの書き方自体が間違っている可能性」をテストするとなると、できるだけAnsibleとテストツールは疎結合とするべきと考え、ファイルや設定は一切共有しない形でServerspecを使っている。
Circle CIの利用 繰り返しになるが初である。インフラエンジニアがCIをすることはまぁない（なかった）。そんな頻繁に設定を変えるわけでもなし。インフラCIが可能かつ必要となったのは、Infrastructure as Codeの台頭と、クラウドネイティブ化によりImmutableかつ極めて速いライフサイクルでサーバーインフラが更新されるようになったことによるもの。
で、Circle CIでググってもそんなに使い方みたいな初歩的な記事は出ない。どうもCIツールの使い方なんてのはJenkins登場の頃に身につけてて当然だろって感じの扱いっぽい。実際使いながら自分なりに理解したのは「レポジトリをpushすると、それを使って自動的にテストやデプロイを回してくれる」ツールということで、Circle CIについてはこんな感じに認識してるんだがあってんのかなぁ。
 レポジトリの使用言語やファイル構成を見て良きに計らって勝手にテストしてくれる。 もちろん自分でテストコマンドを書いてもOKで、Circle CIにやってほしいことは circle.yml というYAMLファイルに書いてレポジトリの第一階層に置いておく。 GitHub連携を前提としており、連携したレポジトリの push をトリガーとして動作する。 動作としてはCircle CI上でDockerコンテナ（ubuntuベース）を起動→レポジトリを git clone →circle.ymlを読んで実行  実装 実際のcircle.ymlはこうなった（といってもほぼ丸のまま冒頭記事のものを使っているが）。Dockerイメージのキャッシュには以下の記事も参考にした。
CircleCIでDockerイメージをキャッシュするのに、実はちょっとした工夫が必要な件 - tehepero note(・ω&amp;lt;)
machine: timezone: Asia/Tokyo services: - docker dependencies: pre: - if [[ -e ~/docker/docker_ansible_image.tar ]]; then docker load --input ~/docker/docker_ansible_image.tar ; else docker build -t centos_ansible ~/ansible-ruby-devs/ ; mkdir -p ~/docker ; docker save -o ~/docker/docker_ansible_image.</description>
    </item>
    
    <item>
      <title>オープンソースカンファレンス2015 Tokyo/Fall行ってきた</title>
      <link>https://you.github.io/post/osc-2015-tokyo-fall/</link>
      <pubDate>Sun, 25 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/osc-2015-tokyo-fall/</guid>
      <description>戦利品、ConoHaちゃんかわええ。
オープンソースカンファレンスに初めて行ってみた。2015 Tokyo/Fallです。だいぶ奥地の方でやってるなぁという印象があってなかなか行きづらかったのだが、実際行ってみると自然に囲まれて静かで建物綺麗で過ごしやすそうないい大学ですね。ちょっぴり休憩でもくもくしたりしてみたけどだいぶ捗ったし、もう少し近所なら作業環境に使いたい感じが（）
ぼっち参戦かつ初参戦かつコミュ障な故、ブースがんがん回ってがんがん自分から話しかけるみたいな度胸はなく、だいたいセッション聴いてました。ので、セッションごとにちょっとまとめる。
はじめてのオープンソース・ライセンス オープンソース自体の考え方だとかは知ってはいたのだが、ライセンスがMITとかApacheとかそういういろいろがあるのがよくわかってなかったので。話の中で教えていただいたGithubによる、オープンソースライセンスの選び方 | オープンソース・ライセンスの談話室というページが確かに詳しそうなので後で読まなければなと思った。ギッハブ使ってるのに全然これ理解してなかった。
実録！Hinemos導入経験者が語る、実運用でのあるある話 最近実務でHinemosを使っているもので。監視設定をグループ（スコープ）単位で作ってしまうと静観するときに設定変更がしんどいだとかっていう本当にあるあるな話と、Hinemos Utilityが便利だという話など。Hinemos Utility、設定のインポートエクスポートがあるので、GUIポチポチの面倒臭さから救われそうな気はした。あるある話の方は他の監視ツールでもわりと似たところあるので、結局アーキテクチャーってどんなツールでも大して変わらんのかなぁ、そのへんどうにかしたツールないかなぁとか思った。
そういえばセッションはTISの主催だったのだが、同社といえばZabbixの池田氏の印象が強いので、Zabbixと比較して同社としてどう考えてるのかってあたりも聞きたかった。
[飲食OK]（発表者募集中！）1日目-ライトニングトーク（by OSCスポンサー） なかなかカオスにライトニングトーク。飲食OKでしたけど学食せっかくなので使いたかったので無飲食で。言及してるとキリがないので割愛。
Ubuntuの最新情報 Ubuntu使ってない。てかDebian系ほとんど触った経験ないんで触らなきゃなと思いました。
ZabbixでDockerも監視してみよう 最後の質疑で出た話で、新陳代謝の高いコンテナの監視に既存ツールの分単位での監視感覚が役に立つのか？っていうのがあったのだけど、わりとそれに同意した。自分はコンテナを実サービスで運用した経験がないのでアレなのだが、その性質からして既存サーバーより速いスピードで起動停止を繰り返すことは有り得ると思うし、むしろアーキテクチャーの考え方がスピーディーなものに変わるためには監視ツールの在り方も変わんなきゃならないんじゃないかなぁと思った。答えは出てないけど。
コンテナ(Docker)時代のインフラ技術・運用管理に迫る！ Docker最新事情という感じで、Dockerの概要もそこそこに周辺ツールや開発状況をいろいろ舐めていく感じのセッション。Docker触り始めたばかりの自分にはとてもありがたかった。Docker machine、Docker Swarm、docker-composeだとかなかなかにワクワクする話。
aozorahackの今までとこれから ～インターネット電子図書館「青空文庫」をエンジニアリングで支える～ ここから2日目。春に青空文庫アイディアソンが開かれて話題になりましたが、そのときから興味があったので話を聞いてみた。青空文庫の誕生が1997年、オープンソースという言葉が生まれたのが1998年ということで、オープンソースより古い歴史を持つ青空文庫がOSSライクな発想をしていたはずもない！という出発点だったようなのだが、それを変えていこうという試み。もともとサードパーティ的にビューアやコンバータを作る動きは周辺にあったわけで、それをまとめて今風の開発をしていくとなると面白そうだなと思う。自分は業務エンジニア＋趣味エンジニアでしかないけど、こういうボランティアというか、自らの意志で参画していくエンジニアリング活動というのがOSSの在り方なんだとここで初めて腑に落ちた気がした。
Solaris ZoneとPuppet、Serverspecでインフラ CI Solarisわからないけどインフラ周りの知識手広くしたいなということで行ってはみたがやっぱりわからなかった。知識って広くて浅いか狭くて深いかの二択だと勝手に思ってたけど、実際それなりに出来るエンジニアってそこそこ手広く平均点取れる人が多い気がしていて、例えばこのセッションであればUNIX（Solaris）の知識にコンテナ（Zone）の知識、んでPuppetはRubyだし、ServerspecもRubyというかRSpecなんですよね。エンジニアとしての幅、ちょっと見直したいなと思わされたセッション。
【パネルディスカッション】今こそ語るエンジニアの幸せな未来 ～OSC東京編～ 春に行ったJAWS DAYSでも同様のパネディスを聴いてはいたので実質第二回というか。こういう話題が定期的に持ち上がるようになったのって、さくらインターネットが15周年迎えたこともあるようにエンジニアの高齢化（家庭環境の変化）があるような気はする。働き方はライフステージによっても、世の中の技術動向やビジネス動向によっても変わるので、結局時間と金銭的余裕のあるうちに勉強して、常に自分が働きやすい場所にいられるようバリューを磨いておくしかないのだろうなと思う。そういう話でした。
以上、9セッション。知識のザッピングとしてこういうセッション形式のイベントはやはり良いなと。いわゆる勉強会だとだいたいが自分の興味関心のあることだけに集中してしまうのだが、こういう機会だとせっかくなのでってことで脇道に逸れたりしやすいので、知識の幅増やす機会にはなりやすいですね。OSS、TISのようにビジネスとして取り扱っている人たちもいれば、aozorahackのような草の根の動きもあったり、有り様はいろいろであって、んでGitHubで取りあえずソース読むところからいつでも手を付けられる時代にあるので、何かしらやってみると面白いのかもしれない。尻込みしてるのではなく。
テロ #osc15tk pic.twitter.com/OEa2JYMezY
&amp;mdash; T.Kabu (@disco_v8) 2015, 10月 24 
懇親会、TRIGGER ANIMATION EXPOに行けるチャンスがここしかなかったのと、200人という大所帯にぼっち参戦する勇気がないのと（あと、さすがに薄い話しかできなさそうであまり意味はないかなと思った）で行かなかったんだけど、生ハム原木はうらやましかった。</description>
    </item>
    
    <item>
      <title>インフラエンジニアの幸福論</title>
      <link>https://you.github.io/post/eudaemonics-of-infrastructure-engineer/</link>
      <pubDate>Tue, 20 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/eudaemonics-of-infrastructure-engineer/</guid>
      <description>1年前、インフラエンジニアは死んだ。
遅れにも遅れをとって今年からAWSに触れているけど、これは触れれば触れるほどインフラエンジニアとしての自分の価値に疑問を抱かせてくれるサービスで、インフラエンジニアとして今後自分はどのように幸せになれるのかなんて考えたくなってくる。
どうもLambdaが出たあたりからサーバーレスアーキテクチャという言葉が取り沙汰されてきているようで、アプリやDBを動かすための基盤としてサーバーが必要だという前提はすでに崩れている。AWSを触れる前はVPSを触っていたので、どうもAWS＝EC2というイメージから抜け出せずにいたが、実際には「EC2を使ったら負け」なんて言葉も目にする時代にある。ここ最近Circle CIを触ってみていても、テストの実行基盤となるサーバーなんて考え方をする必要はなくて、テスト用の環境はyamlで書けてしまうし、別のインスタンスが欲しければDockerで済ませられる。まぁherokuあたりからすでにそういう風潮だったよなという気もするが、単純にアプリをデプロイしてしまって実行基盤は全部お任せという状態から、LambdaとS3とCognitoを組み合わせて云々みたいな柔軟なアーキテクチャを採用できる状態にまで変化してきている。
AWSを管理する人間が旧来のインフラエンジニアである必要性を当人としてはあまり感じていないし、実際に昨今のWeb企業あたりだとアプリエンジニアがAWSエンジニアを兼ねている場合も少なくないとは思う。規模が大きくなれば構成設計にインフラエンジニアの視点が必要になったりもするのかもしれないが、そこで必要とされるスキルは必ずしも旧来のインフラスキルとは直結していない。1000万円のLBとCiscoのスイッチとDELLのサーバーを買ってきて配線して起動して設定していくスキルと、ブラウザ上でELBやEC2のセキュリティグループを設定するスキルは明らかに異なるもので、故に2015年におけるインフラ＝AWSの領域においては、旧来のインフラエンジニアと非インフラエンジニアが同じ土俵で戦えなくもなかったりする（さすがに言い過ぎ感あるか）。これまで培ってきたインフラスキルというものは、必ずしも2015年に戦える武器にはなっていない。
もちろん一方でEC2もオンプレの環境もまだ健在ではあるし、これが10年後に撤廃されるかというと、現状のMFのように残り続けるとは思う。特に金融のような特殊領域ではどうしてもクラウド移行が難しいということもある。だから旧来のインフラエンジニアが死に絶えることはないのだろうが、それでもパイが小さくなることは事実だし、物理環境の障害だとか5年ごとのリプレースだとか、テンションの上がらない類の仕事に携わり続けることを余儀なくされる。
テンションの上がる仕事がしたいと言うと軽薄になってしまうが、誰だって夜中にタクシーでデータセンターに駆け付ける機会は極力少なくしたいと思うわけで、これまで注力していたいわゆる「オープン系」の需要が狭まる中で、インフラエンジニアの「幸福論」のようなものは求められつつある気がする。より少なくなる、かつ結構しんどい椅子に座り続けるのは個人的に嫌なので、Docker、ServerSpec、Ansibleあたりの領域でガッツリ存在感を示せるようになるか、あるいはそれらを生み出す側、より下のレイヤーで技術的に研ぎ澄まされていくかの二択なのかなと最近は思いつつある。とはいえ後者はどう考えても狭き門であり、現実的には前者を日常的な業務としつつ、要はRubyエンジニアがgem書くような感覚でツール作ったりOSSにイッチョカミしたりもたまにやれるぐらいの力があるといいのだろうなと思う。
何はともあれやはり「勉強する」以外に道がないことは今も昔も変わってないし、ある意味で過渡期にある技術として、いまインフラは面白いところにあるとは思っている。これについていけるかついていけないかっていうシンプルな問いでもあって、自分の希望としてはついていきたい。今からアプリに鞍替えする気はなく、カーネル書けるかって言えば書けないだろうし、かといって今のまま障害対応で夜に起こされるのを続けるわけにもいかない。だったら2015年におけるインフラというものを学んでいくしかないわけで、幸いなことに、学べば「物理ハードウェアからの開放」というある程度の報酬が待ち受けていることは確実になっている。どれだけ頑張ってもサーバーのファームウェアのバグで泣かされるような時代ではなく、ある意味でインフラエンジニアが「インフラを学び直す」ことは美味しい選択肢ではある。また場合によっては、インフラエンジニアとしてより良い環境へ適時シフトしていく（惰性でずっとオンプレ使う方針の会社との喧嘩は早々に諦める）こともまた必要になるのだと思っている。
とかなんとか書きながら考えていたら、若手インフラエンジニア現状確認会とやらで似たような話が上がっていた。個人的な実感としてはITエンジニア100人の企業であれば5～9人ぐらいがインフラかなと思うので、若手インフラが少ないというよりは全体的にインフラエンジニアが少ないのだと思っているが、その分情報交換とか大切にしなきゃなと思う。エンジニア仲間増やしたい。</description>
    </item>
    
    <item>
      <title>個人開発環境にGithub Flowを適用する</title>
      <link>https://you.github.io/post/individual-github-flow/</link>
      <pubDate>Sun, 04 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/individual-github-flow/</guid>
      <description>Github、joinしたのは2013年で作ったものは軒並みちゃんと突っ込んではいるんだけど、単に一区切りついたらadd =&amp;gt; commit =&amp;gt; pushしているだけでちゃんと使っていなかったので、個人開発ではあるがGithub Flowを取り入れてみた。
What is Github flow ? Githubを用いた開発作業を進めるにあたっての指針みたいなものです。基本的にはmasterブランチ上では作業せず、作業工程ごとにブランチ作って、終わったらプルリクしてmasterにマージしてもらうことでデプロイとしましょうね、というものだと理解している。至ってシンプルではあるけど、これを取り入れるだけで従来やっちゃってた「masterで作業してるのでデプロイしても動かないレポジトリがGithub上にある」みたいな状態が防げて良さそうだと思った。
ちなみにGit-flowというのもあるようだけど、こちらは全然別個のツールらしく理解していない。Git-flowの問題解決としてGithub Flowが提唱されたようだが、そもそも開発工程の制御のためだけにツールを追加したくはないなと思ったのでGithub Flowを採用した。
Github Flowの理解にはこの文章が良さそう。なお、dotfilesのような大した更新のないレポジトリにはさすがに適用していない。
GitHub Flow (Japanese translation)
実際の開発工程 あくまでGithub Flowに沿う形という程度なので、そのままそっくり適用できてはないとは思うが。
開発開始 ブランチを切る。ブランチ名は機能追加等の開発要件であればdev_hoge、バグフィックスであればhotfix_hogeとする。
$ git checkout -b dev_hoge  開発中 普通であればレビューを依頼するタイミングなど、開発の切りがついたところでpushしていくのだろうが、分散して開発しているわけではないので、1日の開発が終わる段階でpushしている。そもそも開発に使っている環境が複数あるので、Github上のdevelopブランチも常に最新化していつどこでもfetch可能にしたいなという思いがある。従来はDropboxで各環境間の同期を取っていたが、プラグインの有無やbundleなどで度々不具合もあったので改めた。
$ git add -A $ git commit -m &amp;quot; ... $ git push origin dev_hoge  git add .ではなく-Aなのは、そちらじゃないとgit rmしたファイル等が含まれないとこちらの記事に書いてあったゆえ。
開発終了 開発が終わり、masterへのマージを必要とする段階に来たらプルリクを出す。プルリクって別のコミッターからしか不可なのかと思っていたが、自分のレポジトリに自分で出すことも可能だったのでそうしている。本来であればテストツール等走らせるべきではあるのだろうが、今のところはプルリクに対して特にレビュー等なく（自分のコードだし）そのままマージしている。
後述するがバグや開発課題の管理にはGithub issueを用いているので、マージの際はissueのナンバーをコメントに入れている。これでGithub上のリンクとして働いてくれるので便利。

参考：Gitコミットメッセージの7大原則 - rochefort&amp;rsquo;s blog
マージ後 作業ブランチを消して、ローカルのmasterを最新化する。
マージにはgit mergeを使用し、git rebaseは使わないことにしている。そもそもrebase完全に理解してないというのもあるが、要するに歴史改変にあたるような操作があまり好めないというのが強い。個人の開発においては作業ブランチの変更中にmasterに更新が入ることは少ないので、このやり方でおそらく不都合はしないと思っている。
$ git checkout master $ git branch -a $ git branch -d dev_hoge $ git push --delete origin dev_hoge $ git fetch $ git marge origin/master  参考 GitのRebaseによるBranchの運用 ｜ Developers.</description>
    </item>
    
    <item>
      <title>開発環境のためのansibleを出来るだけベストプラクティスでまとめた</title>
      <link>https://you.github.io/post/ansible-in-nearly-best-practice/</link>
      <pubDate>Thu, 24 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/ansible-in-nearly-best-practice/</guid>
      <description> 自分は今まで開発に使うマシンとして家では据え置きのiMac Mid 2010（古い）を、出先ではVAIO Proに入れたArch Linuxを使っていて、レポジトリの同期にはあろうことかDropboxを使っていたのだが、インストールされているツールが微妙に違っていたり、Dropboxでbundleとかまで同期してしまうのはあまりよろしくなさそうだなというのもあったりして、EC2上に開発環境を置いて各端末からはSSHでつなぐことにしてみた。で、せっかくなのでと思いansbileで環境構築を行っている。
なぜEC2？ 少し前に開発環境としてDigitalOceanを使うことを勧める記事を書いたことがあったが、仮想マシンを停止しても課金が発生してしまうのが少々つらいのと、リージョンがすべて国外で、開発に使うにはさすがにレイテンシーが厳しいので断念した。EC2であれば停止中は課金されないので、常時起動が必要ない開発環境として使う分には課金額は少なくなりそうかなと考えている。今は自分のアカウントだとまだ無料期間にあたるので、t2.microを無料で使える状態にあり、実際の課金額がどうなるかは確かめていない。
ベストプラクティス構成の意識 出来上がったplaybooks（という言い方でいいのか？）はGitHubに上げてある。
chroju/ansible
あらゆるサーバーで共通のcommonというロールと、開発環境用のdevelopというロールを用意している。現状、開発言語がRubyでシェルにはzshを使っているので、完全に自分仕様のplaybookにはなっている。dotfilesも自分のレポジトリからgit cloneしているし。
ansibleには公式ドキュメントにディレクトリ構成のベストプラクティスが上がっていて、なるべくこれに沿うようには作っている。が、完全に当てはめてしまうには開発環境1サーバーだけのためのansibleには荷が重すぎるので、あくまで部分適用ではある。自分の解釈ではベストプラクティスの考え方はこんなところかと。
 playbooksは同時に実行すべきtaskをroleとして分割する ansibleの適用対象サーバーはWeb、DB等の役割ごとにグループで分割する グループごとに実行するroleと変数（group_vars）を紐つける  taskはroleに分割され、それらroleをwebservers.ymlやdbservers.ymlがincludeし、さらにsite.ymlがすべての*servers.ymlをincludeするというのが公式の勧めです。確かにこれなら全体に適用したい場合はansible-playbook site.ymlでよいし、一部グループだけに適用したいならansible-playbook *servers.ymlとすれば良いのだから合理的。さらにインベントリファイルもstagingとproductionに分けて、それぞれにwebserversとdbserversのグループを作っているのだから、stagingとproductonで別々に適用することも可能になると。またtagをtaskにつけておけば特定のtask群だけ実行することも容易になる。。。とまぁ、とにかくいろんな手段を使って分割実行できるようにしているわけですな。
なのでそこまで大規模な構成管理をしないのであれば、このあたりどこまで取り入れるのは自由ではないかと。自分の場合はサーバー1台が今のところは相手ということもあり、*servers.ymlにあたるインベントリファイルは作っていないし、tagも個々のtaskに対しては設定していない。今後さらに範囲を広げるようであれば、後付で設定していけばよいかと思っている。
ただ、少なくともroleに関しては分けておくべきと個人的には推しておきたい。普通にアプリケーション用のコード書くときにも関数やメソッドは分割しますよね？ってことで、全体の見通しを良くする意味でもroleへの分割は必須と思う。
ansible、ファーストインプレッションはとにかく「楽そう」だったんだけど、複雑なことをやろうとすればするほどドツボにはまっていきそうな気もする。ある程度早い段階でベストプラクティスに目を通したうえで、自分が使うときにはどういったディレクトリ構成が最も有効であるかを模索した方がよい。自分も理解できるまでは少し苦労したけど、一度やり方をハメてしまうと今後長く使えそうで満足感がある。
このレポジトリでやっていること ソース読んでもらえればわかる話ではありますが。
 common  hostname設定 localeとtimezone設定 sshd_config設定 authorized keys設定 /etc/aliasesの設定 DenyHostsインストール logwatchインストール iptables設定  develop  development tools、zsh、git、vim、jqのインストール dotfilesの配置 デフォルトシェルをzshに変更 rbenvの設定   もっとも苦労したのはrbenv周りで、git clone直後はpathが通ってない~/.rbenv/bin配下のコマンドをどう実行すべきかとか、それなりに悩んだ。当初インベントリファイルでsudo: yesとしてしまっていたので、rbenv関連のタスクも全部root権限で実行されて、軒並みフォルダやファイルがrootの所有になってしまうという事故があったのだが、sudo:ないしbecome:の設定はタスク単位で考えたほうが良いと思う。また本来であれば~/.bash_profileあたりにrbenvのpath追加等の設定を書き込むところまでタスク化すべきかと思うが、自分の場合はdotfilesにすでに設定が入っているので、そのタスクは作っていない。
もうひとつの悩みとしては、いずれのroleもsudo権限のある開発用ユーザーでの実行を前提に考えているのだが、EC2の場合はec2_user、その他VPSの場合はrootがデフォルトのユーザーなので、デフォルトユーザーで一度入って開発用のユーザーを作るところもタスク化すべきかなと言うこと。その場合はそのroleだけ別ユーザーで実行する形になるわけで、そういった構成がそもそも可能なのか？というところからわかってないのだけど。
その他技術的な話 細かい技術的なtipsは後ほどQiitaに上げるつもり。現状の疑問中心に一旦取りまとめます。
 変数名の命名規約。代入はgroup_varsで行うことになるが、ここに一挙に集めたときにどれがどこで使われているのかわかりにくいので、roleに紐ついた名前とすべきであろうか。。 ntp.confの設定。templatesでいい気はするのだが、どんなサーバーでも共通の設定で問題ないのか勉強不足でわかっていない。 EC2には開発ツールがないのでDevelopment toolsでまとめてインストールしてしまったが、本当は個々に切り出したい。 ファイルの一部上書きではなく、追記に良い方法がないものか。下手にやると何度も追記してしまって冪等性がなくなる（現状は一度grepかけて、その内容でtaskの実施要否を分岐してるが汎用性がない）。  </description>
    </item>
    
    <item>
      <title>ハッカーマインドと3冊のエッセイ</title>
      <link>https://you.github.io/post/hackers-mind-and-their-essay/</link>
      <pubDate>Sun, 16 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/hackers-mind-and-their-essay/</guid>
      <description>ハッカー3大エッセイとは自分が勝手に呼んでいるだけなのだが、『ハッカーと画家』『UNIXという考え方』『それが僕には楽しかったから』の3冊のことである。しかし『それがぼくには』は重要な一冊だと思うんだけど、なんでまた絶版なんですかね。そんな古い本でもないのに。仕方なく図書館で借りたけど。
いわゆるハッカーマインドを描いた本としていずれも似たような印象を抱きがちだが、実際に読んでみるとスタンスはだいぶ異なる。『ハッカーと画家』はコンピュータについてあまり詳しくない人に対して、ハッカーというのはこういう人種なのだと切々と説いた本であり、故にそれほど挑発的な印象は受けず、すらすらと読み進めていくことができる。もっともこれがハッカー以外に理解できるかというとかなり疑問ではあるが、ハッカーが比較的客観的に自らを解き明かした本として参考にはなる。著者のポール・グレアムのエッセイはnaoya_t氏による和訳がいくらか読めるので、これを読んで興味をそそられたら読んでみるのでもいいかもしれない。あと、Lispめっちゃ推してる。
『UNIXという考え方』は、ハッカー向けにハッカーマインド、というかUNIX哲学を説く本なので、これは3冊の中では最も「読むべき」本だと思った。プログラムの移植性が重要であることだとか、ソフトウェアのレバレッジを効かせて効率性を最大限に高めていくべきだとか、我々がコードを書いたりシステムを作る上で重視すべきことがいくつも盛り込まれている。
『それがぼくには楽しかったから』はまさにエッセイ、リーナス・トーヴァルズの半生を描いたもので、ハッカーマインド云々というよりはだいぶ読み物チック。終盤で著作権やOSSといった概念に対するリーナスの考え方が少し語られるが、ほとんどはLinuxがいかにして生まれたのか？を描いた物語と言っていい。自分はリーナスというハッカーをこれまで詳しくは知らなかったのだが、案外柔軟な人物であるという印象を受けた。OSSの考え方自体は肯定しながらも、それは押し付けるべきではない、具体的に名前を挙げてリチャード・ストールマンのやり方は強引に過ぎるとしていたり、自分は聖人君子ではなく、大金が舞い込んだときには当然喜んでしまったこともあるよなんて語っていたり、彼の人間性がとても良く出ている。まぁとはいえ、自分が否とみなしたものに対しては、それなりに厳しい批判を飛ばす人物ではあると思うが。
こうした本に書かれた「ハッカーマインド」なるものは、我々が仕事をする上で必須のものではないと思うし、行き過ぎるとリーナスが言うような宗教戦争チックにもなりかねない。また技術に傾倒しすぎた単なるオタクが仕事の上でも重要な人物足りえるかというと、そういうわけでもない。リーナス・トーヴァルズは偉大なハッカーの1人であろうが、彼は同時にLinux開発者という立場での活動を行うにあたり、社会性を身につけたりもしてきたわけで。単にGeekであること自体が良いこととも自分は思えない。
とはいえ、まだ生成されてまもなく、業界標準なんてものがあるんだかないのだかもわからない、進化の速いこの業界で仕事をしていくには、多少なりともハッカー的なマインドは必要だとも思うのだ。というか、じゃないと仕事が面白くならないんでは？　惰性で同じ技術をずっと使い続けたり、効率の悪い方法を繰り返したりしていてもお金は入るのだろうけど、それが必ずしも収入に結びつかないとしても、なんかカッコイイことやってみたいとか、楽しそうな新技術にトライしてみたいだとか、そういう感覚がないとエンジニアをやっている意味がないなと思う。エンジニアが会社を選ぶにあたって重要なのは、案外このポイントなのではなかろうか。
残念ながら求人票からハッカーマインドは透けてこないし、転職面接の数分でそれを読み解くことも難しいだろう（自分は以前、面接でArch Linuxの話でたまたま意気投合する機会があったりして、そういう面接が出来たら話が別なのだろうけど）。その点、最近GitHubやQiitaでエンジニアたちが企業名を出して活動していることがあるが、あれは求人票やウェブサイトでは見えにくいその会社のハッカーマインドを、外部に知らしめていく良い手段だと感じる。ビジネス的に何を成して、社会をどう変えたいのかというよりも、エンジニアとしてどういったカタチで技術にコミットしていくかの方が自分には重要だ。そういう視点で仕事をしていけたらどんなにか幸せだろうし、またそれは茨の道でもあるのだろうなと思っている。
ハッカーと画家 コンピュータ時代の創造者たちposted with amazlet at 15.08.16ポール グレアム オーム社 売り上げランキング: 6,887
Amazon.co.jpで詳細を見る UNIXという考え方―その設計思想と哲学posted with amazlet at 15.08.16Mike Gancarz オーム社 売り上げランキング: 44,838
Amazon.co.jpで詳細を見る それがぼくには楽しかったから (小プロ・ブックス)posted with amazlet at 15.08.16リーナス トーバルズ デビッド ダイヤモンド 小学館プロダクション 売り上げランキング: 71,563
Amazon.co.jpで詳細を見る </description>
    </item>
    
    <item>
      <title>Qiitaを使うということの意義</title>
      <link>https://you.github.io/post/qiita-commoditization-of-engineer/</link>
      <pubDate>Sun, 09 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/qiita-commoditization-of-engineer/</guid>
      <description>Qiitaにいくつか記事を上げてみて思ったことを。
承認欲求が満たしやすい ブログのような個人の場ではないのに承認欲求がってのもどうなんだという話はあるが、反応を得やすい。Qiitaでは各エントリーに必ずタグを設定することになり、ユーザーは興味のあるタグを登録して新着記事をチェックするわけだが、記事が上がってくるスピードは1日に数えられる程ではあるので、上げればほぼ必ず誰かしらの目には留まる状態にある。なので自分が上げたのは基礎的な記事ばかりだという思いはあるのだが、それでもすべて漏れなくストックされていた。
もちろん、100ストックなどを目指すとハードルはぐっと上がってくるわけだが、こういった個人ブログで誰が見てくれているかわからない状態と比べて、記事投稿へのモチベーションは保ちやすいように感じた。なお、はてなでも同様のエコシステムは働いていて、例えばこのブログもはてな時代はそこそこブクマされていたわけだが、github.io化した後のブクマは見事にゼロである。
誤り修正と議論の活性化 ほぼすべての記事が誰かしらの目に触れるということで、（自分は未経験だが）コメントにより間違いの修正が入ることも多い。特に特定のタグに関してはその道の有名な方がだいぶ監視しているっぽいなぁという場合もあり、ちょこちょこコメントが付いている。
またコメントで長々と議論が続くのもよく見かける。単なるハウツーよりは何らかの設計思想を書いた記事に多いように思うが、派生した内容として興味深く追えることも多い。
技術のコモディティ化 で、ここからが本題なのだが、QiitaによってIT技術者の知識というのはある程度コモディティ化されそうだなぁと思う。
Qiita以前ははてななどがエンジニアのアウトプットがよくストックされる場所ではあったが、Qiitaほど体系だってまとめられていたわけではない。Qiitaでは「タグ」を追うことで、その分野の新しい話題も古い話題も、基礎も応用も知っていくことができる。逆に言えば、Qiitaに書いてあることぐらいは誰だってすぐ追って身につけられる状態にある。
技術書のような網羅性の高い知識パッケージとはさすがに性質を異にはするが、先に上げたコメントなどによって適宜内容が改訂され、より正しい状態に近づいていき、また必要な情報、新たな情報が次々と追加されるという意味では、動的な知識パッケージとして果たす役割は大きいのではないか。 まぁ要はブログやSNSの黎明期に言われたようなことが、Qiitaという専門性の高い1サービス内で圧縮的に再現されているというだけの話ではあるのだが、「Qiitaをやっている」というレッテルが、ある一定の知識レベルを有することと同義になる日も来そうだなという思いがする。問題点としてはQiita外と同様、やはりWeb系、OSS系の知識に内容が偏っていて、有償製品等のノウハウはそれほど多くないことだろうか。これはQiitaの問題ではないのだけど。</description>
    </item>
    
    <item>
      <title>Kaminariの実装をしてみた</title>
      <link>https://you.github.io/post/exhibi-update/</link>
      <pubDate>Sat, 08 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/exhibi-update/</guid>
      <description>久しぶりに稼働させているExhiBiというサービスの機能を少し更新した。といってもそれほど大した話ではないですが、一応書き留め。
kaminari ページネーションでデファクトスタンダード状態であるkaminariを使ってみました。
amatsuda/kaminari bundlerでインストールすればほぼ設定とかなくても使えます。最初のローンチのときに入れなかったので、viewを結構いじらなくちゃいけなくて大変かなーと思っていたのだけど、そんなことはなかった。主に変更は2点で、まずはcontrollerで#indexのようなリソースを拾ってくるアクションに.pageをかましてやるようにします。
# もともとはExhibition.all.order... def index @exhibitions = Exhibition.page(params[:page]).order(&amp;quot;start_date DESC&amp;quot;) end  あとはviewでページネーションを表示するためのヘルパーを1行追加すれば終わり。以下はslimの場合。
= paginate @exhibitions  なお、実装当初はundefined method &#39;deep_symbolize_keys&#39;などというちょっと関係ねーだろこれって感じのエラーが出たりして焦ったのは秘密です。原因はconfig/locales/ja.ymlが一切インデントされてなかったことなんですけど、そんなのがここに波及するんですね。。。てかyamlの書き方よくわかってねーわ。
もちろん、1ページあたりの表示数とかページャーの表示の仕方だとか、いろいろ細かく設定はできますが、とりあえずこれだけでページャーは実装されます。あーこりゃデファクトスタンダードになるわなという簡単さ。早く入れればよかった。なお、本当にまだ入れただけなのでCSSとかぜんぜん調整してないです。
id以外の要素でmodle#showにアクセスする 例えばExhiBiの場合は美術館ごとのページにアクセスするには、これまでmuseums/2みたいなURLになっていたわけですが、カッコ悪いし使い勝手も悪いのでmuseums/motなど、英名でアクセスできるよう変えました。参考にしたのは以下ページ。
Railsで、URLにIDでなく名前を入力して、アクセスする方法 - Qiita やってることはなんともシンプルで、Museum.find(n)で呼んでいたところをMuseum.find_by_name_en_or_id(hoge)と出来るようにしただけですね。#to_paramでサービス内のリンクもすべて英名表記URLに変更できています。こういう柔軟さはRailsやっぱりいいですね。
ただ自分の場合ちょっと問題があったのは、これまでテーブルに英名表記のカラムを入れてなかったので、新たに追加する必要がありました。まぁ普通にbundle exec rake g migrationしてからrake db:migrateするだけなんですけど、ローカルで開発しているときに何故かこれが通らず、一旦rake db:migrate:resetしてから改めて打つハメになったりした。このへんの話は以下記事がちょっと詳しかったり。
rake db:reset と rake db:migrate:reset の違い | EasyRamble 自分はインフラエンジニアなので、Railsを実務で使うってことはほとんどこの先皆無だとは思うんですけど、自己表現手段としてやっぱりRailsぐらい使えておくと良さそうだなと改めて思います。例えばインフラの勉強でサーバー運用してみようとなっても、上で何か動いてないとあんまり勉強にならなかったり。自分がどんなことをしているのか？を外にアッピルする意味では、こういうの1つぐらい持っとくといいのだろうなと思います。yamlの勉強しなきゃとか、今回そういう派生効果もありましたので。近々作れたらもう1個サービス作ってみようと思ってます。</description>
    </item>
    
    <item>
      <title>Qiitaはじめました</title>
      <link>https://you.github.io/post/start-qiita/</link>
      <pubDate>Sat, 25 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/start-qiita/</guid>
      <description>chroju - Qiita 気分でQiitaはじめてみた。Kobitoをちらちら使って簡単なメモを残していたりしたのだけど、そこから一発で上げられるのはやっぱ楽かなと思って。あと先日の暗号化に関する記事みたいなまとめ記事、tips系はやはりQiitaの方がフットワーク軽くて使いやすいような気がする。更新した場合にも履歴が残るし。
ブログとの使い分けが難しそうな気はするが、いわゆる勉強メモみたいな頻繁に見返すものをQiitaに上げて、ブログの方はもっとガッチリとした長文、たとえば勉強会の記録だとか技術に対する考え、あるいは何かを作った系の記事などを上げたらいいのではと思っている。まぁこのへんはあまり縛られず、あくまで中心に据えているのは自分用メモとしての役割なので、自分が使いやすいようなやり方でやれればいいかなと思っている。
ブログは多くとも週2回程度の更新だったが、Qiitaはもっと高い頻度でいろいろ貯めこんでいきたいし、そうできるような仕事をしていきたい所存。</description>
    </item>
    
    <item>
      <title>AnsibleとDigitalOceanでどこでも使える開発環境を作る</title>
      <link>https://you.github.io/post/ansible-digitalocean-vps-dev-env/</link>
      <pubDate>Mon, 20 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/ansible-digitalocean-vps-dev-env/</guid>
      <description>個人開発環境としては自宅にiMac 2010Mid、モバイルでVAIO Pro 11に入れたArch Linuxを使っているのだが、メインとしてはiMacの方を利用していて、デプロイしたりなんだりは自宅からしか出来ない状態にある。じゃあVAIOに移せばいいやんとも思うのだが、こちらも会社PC（なぜかこちらもVAIO Pro 11）と二重になってしまうので始終持ち歩きたくはなく、平日フラフラしてるときにサッとbash入りたいなみたいのが出来ずにいた。
結論としてVPSを開発環境として扱い、最悪iPad miniからいつでもSSH接続してbash叩けるだけでも幸せかなというところに至った。これまでhttp://chroju.netをさくらVPSで運営していたので、特に考えずさくらをもう1台追加したりもしたのだが、ちょっと調べてみるとDigitalOceanが最近流行りつつあるようだったので、他社サービスも使ってみると面白そうだってことで新規契約してみた。
DigitalOcean すでに他所で言われてはいるが、利点としてはこんなところかと思う。
 月額課金ではなく時間課金なので、使いたいだけ払えばOK 安い アプリケーションやSSH鍵が最初から組み込まれたイメージを作れる REST APIでだいたいのVPS操作ができる  要するに使いたいときに使いたい環境をバチコンと作れちゃうというのが一番のメリットなので、今回のような永続的に使う開発環境より、一時的なテストなんかに使った方が良いのだと思う。とはいえ時間課金上限が月あたりで定められており、現状最安プランだと月5ドルが上限になっていたりもするので、永続的にマシンを上げておく分にも安いのは確か。なお、課金はイメージを作った時点で開始されるので、不要なマシンはhaltではなくdestroyしておく必要がある。まぁ無料のスナップショット機能もあるから、リカバリできると思えばdestroyしてしまうこともそこまで難しくはないかなと。
REST API提供ということで、CLIから落としたり上げたり壊したりなんだりも全部できるのだが、だったらひょっとして誰かがアプリとか作ってんじゃねーかなと思ったら、やっぱりすでにあった。
DigitalOcean Swimmer Android制作: Hannoun Yassir評価: 4.4 / 5段階中価格: 無料 (2015/7/11 時点)
posted by: AndroidHTML v3.1 このアプリさえあればGUI操作はほぼ全部できる。
ちなみにこんなことでハマる人はほとんどいないだろうと思うが、自分がハマったポイントとしてauthorized_keysの件がある。Digital OceanではあらかじめWeb GUIで公開鍵を上げておき、VPSをcreateするときに最初から任意の鍵を入れておくことができるのだが、当初はroot以外のユーザーがいないため、当然ながらauthorized_keysのパスも/root/.ssh/配下となる。構築用には別のユーザーを設けることになると思うが、その際にはauthorized_keysを/home/user配下へ持ってきて、アクセス権の適切な設定などもしなくてはssh接続できないので注意。
Ansibleによる初期構築 巷ではVagrantと連携して、vagrant upでDigitalOceanにマシンを上げるのが流行ってるらしい。
 vagrantではじめるクラウド開発環境（DigitalOcean編） - Qiita VagrantとSSDなVPS(Digital Ocean)で1時間1円の使い捨て高速サーバ環境を構築する - Glide Note - グライドノート  とはいえ自分は冒頭に書いた通り、最悪iPad miniでもいいので外から繋ぐという運用をしたかったので、Vagrantからの起動は使えない。なので初期構築には最近学び始めたAnsibleを使ってみた。
インフラ管理系のツール、使ったことがあるのはChefぐらいで、Puppetは概念だけ知ってはいるが、Ansibleの特色はやはりハードルの低さ、学習コストの低さだと思う。エージェントレス、knifeのような特殊なコマンドもほとんど覚える必要がなく、ansible-playbookコマンドさえ覚えておけばとりあえずなんとかなってしまう。
 エージェントレスなのでpipで手元のマシンにansibleを入れればすぐ使える。 設定はyamlによるplaybookに書き出すので、文法も比較的容易。 1個1個のタスクは定められたモジュールを用いて書くことになるが、やりたいことを公式DocsのModule Indexで探ればわりとなんとかなる。 ディレクトリ掘ったりknifeみたいなコマンドいっぱい覚えなくても、とりあえずyaml1つとコマンド1つあれば始められる。  pip経由でのインストールが必要なので非pythonista的には若干戸惑いもありましたが、学習コストの低さはハンパないのでインストールから1時間もあれば一旦サーバー建てられました。ノウハウもQiitaはじめ随所に落ちてはいるけれど、正直公式ドキュメントがかなり充実していて、YAMLのシンタックスガイドまで付いていたりするので、下手にググってやるよりもドキュメントちゃんと読んだ方がいいと思う。まぁ、Ansibleにかぎらずなんだってそうではあるが。ただ、複数台管理だとかアプリのデプロイだとかをやろうとすると当然ディレクトリ構成も複雑になって、既存のプラクティスが必要になってくるので、あくまで「導入の学習コストが低い」という感じだが。
書いたPlaybookはとりあえずGitHubに上げた。こちらを参考に、いわゆるVPS作るときの初期設定だけまとめている。ただしわりと俺用（dotfiles引っ張ってきたりとか）。Ansibleについてはまた別の記事でまとめようと思う。
chroju/ansible
iPadからのSSH接続 クライアントソフトがいろいろあるのは知っていたが、ここまでのレベルと思わんかったなーというのがPrompt2。</description>
    </item>
    
    <item>
      <title>暗号化とハッシュ化に関する基本的な事柄まとめ</title>
      <link>https://you.github.io/post/encryption-hash-at-first/</link>
      <pubDate>Mon, 20 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/encryption-hash-at-first/</guid>
      <description> セキュリティスペシャリスト持ってるはずなのに曖昧な理解で誤魔化してたので自分用まとめ。また書き足すかも。なんか書き足し書き足ししていくようなノートの整理には、編集履歴が見られるQiitaの方が便利なのかなとか思うけど。
 暗号化とハッシュ化は違う。暗号化はデータの秘匿を目的としており、適切な鍵を用いることで復号が可能。ハッシュ化はデータの置換がそもそもの目的であり、ハッシュ関数により一定のフォーマットへ不可逆の変換を行う。 ただし、衝突耐性を持つことなどにより、セキュリティ用途に適する「暗号学的ハッシュ関数」というものもあるらしい。デジタル署名やメッセージ認証符号への使用を目的とされており、逆にチェックサム等に使用するには計算が「重い」。  暗号 主なアルゴリズムをざっと。
RSA  公開鍵暗号。素因数分解の計算難度を根拠としたもの。サマウォで解いてたアレもたぶん素因数分解暗号だが、暗算で解かれたらたまったものではない。 SSHログイン時の鍵認証やSSL認証など、広く使われる。 秘密鍵生成コマンドとしてopenssl genrsaがある。SSH鍵認証ではssh-keygenを用いる。  DES  共通鍵暗号。鍵長54bitのブロック暗号。 鍵長が短すぎるため、現在では安全ではないとされるが、暗号化復号化処理を3回実行するトリプルDESという形で主に実用されている。 openssl genrsaでの秘密鍵生成時に、パスフレーズによるトリプルDESでの暗号化を施すため、-des3オプションが用いられる。  AES  共通鍵暗号。DESの安全性低下に伴い開発された、鍵長128bit超のブロック暗号。  ハッシュ ソルト ハッシュ化前に対象文字列に付加するランダムな文字列。同一文字列のハッシュ化時に衝突が防げる、レインボーテーブルによる探索に対する妨害になる、といった利点がある。
フィンガープリント SSH初回ログインで表示されるやつ。公開鍵のハッシュ値。~/.ssh/known_hostsに記述され、次回以降のログインで公開鍵の変更有無の確認に使われる。変更があると、サーバーなりすましの危険性もあるため警告が表示される。ssh-keygen -lfでも表示可能。
md5  出力128bitのハッシュアルゴリズム。ファイル配布時のチェックサムなどに用いられる。 安全性は高くないことが判明しているため、日米ともにSHAの使用が推奨されている。 コマンドはmd5sumあるいはopenssl md5を使用する。 なおパスワードハッシュ化でよく用いられるopenssl passwdはmd5による実装。  sha  Secure Hash Algorithm。暗号学的ハッシュ関数の一つ。 SSL、SSH等で用いられる暗号化アルゴリズム。 SHA-0,1,2,3が存在しており、SHA-1には脆弱性が存在するため、SSL証明書はSHA-2への全面移行が進められている。すでにGoogle ChromeではSHA-1による証明書に対して警告が表示される。 SHA-2は鍵長によりSHA-224、SHA-256、SHA-384、SHA-512といったバリエーションが存在する。 上述の通りopenssl passwdはSHA非対応だが、grub-cryptがSHA-2によるハッシュ化に使える模様。 参考: CentOS6.5でランダムSalt付きSHA-512のシャドウパスワードを生成する - ひろうぃんの雑記  </description>
    </item>
    
    <item>
      <title>Windows開発環境を整える話と、メモアプリとして最強であるwasaviの話</title>
      <link>https://you.github.io/post/development-environment-for-windows/</link>
      <pubDate>Sat, 04 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/development-environment-for-windows/</guid>
      <description>職場のWindows PC、いろいろと開発用に整備を進めてはいるが、やはりWindowsだとツラミある。
Chocolatey Windowsでのパッケージ管理。不可欠かというとそうでもないとは思うのだが、アップデート含め一括管理が可能なので、精神衛生上良さそうだという意味で使ってみている。どうでもいいが名前がかわいい。
インストール自体はPowershellからワンライナーを叩くだけなので難しくはないのだが、よくわからないエラーで止まることが多くて、現状使い切れてない。Proxyの設定はしたし、powershellの権限もRemoteSignedにしたのだけど、何がいけないのか。。。
 Unable to index into an object of type System.IO.FileInfo. 発生場所 C:\ProgramData\chocolatey\helpers\functions\Write-ChocolateyFailure.ps 1:24 文字:8 + throw &amp;lt;&amp;lt;&amp;lt;&amp;lt; &amp;quot;$failureMessage&amp;quot; + CategoryInfo : OperationStopped: (Unable to index...em.IO.FileI nfo.:String) []、RuntimeException + FullyQualifiedErrorId : Unable to index into an object of type System.IO .FileInfo. The install of clover was NOT successful. Error while running &#39;C:\ProgramData\chocolatey\lib\Clover\tools\chocolateyInstall.ps1&#39;. See log for details.  参考 windowsの開発環境は一瞬で整うwith chocolatey - Qiita
コンソール cmdだと貧弱貧弱ゥ！なので、とか、bashコマンド試し打ちしたい場合が多いとか、そういう理由でコンソールは入れ直す。
Gitを入れると自動的にGit bashが入るので、コマンド環境としてはこれを使っている。ターミナルアプリは最近ちょっと話題になっていたのでConEmuを入れてみた。もちろんChocolateyで。</description>
    </item>
    
    <item>
      <title>すべての障害対応を、生まれる前に消し去りたい！ #障害対応きにならNight</title>
      <link>https://you.github.io/post/how-about-your-troubleshooting/</link>
      <pubDate>Fri, 03 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/how-about-your-troubleshooting/</guid>
      <description>エンジニア交流会〜他社の障害対応きにならNight!〜 on Zusaar
改めて見るとすげー名前のイベント……行ってきた。
障害対応は嫌いです。ていうか好きな人がいるならお目にかかってみたいもんですが、しかしシステムを動かす以上障害は避けられないし、それならばなるべく負担を軽減したいというのが人の、いやエンジニアの性。つわけでよりよいソリューションを探す目的で行ってきたイベントだったんですが、結局のところ より深い闇を知るだけの結果に終わった。
世の中闇だらけですわ。闇しかないですわ。自分なんかぜんぜん甘いなっていうか闇とすら呼べないんじゃないかっていう。詳しくは書けませんけど世の中運用者って苦労してんなって認識新たにしました。まぁだからって闇を甘受していいわけじゃなくて、だからこそやることあるんだけどさ。
得た知見をザクっとまとめちゃいますけど、
 明文化と記録は何事も大事。顧客との契約にせよ、手順や構成にせよ、障害記録にせよ。 ただ記録するんじゃなくて探しやすいようにとか考えないと意味ない(Wikiに書き散らしても役には立たない) 日頃からの点検などによる障害の抑止も重要。障害訓練とか。 スーパーエンジニアだから治せるって状態は脱したいのでスキルの底上げは必要。 電話かかってくるのウザいけど必要。確実に対応しなきゃならない障害なら絶対電話。別にTwilioとかでいいので。  障害対応って辛くないはずはないのだが、だったらより辛くない方法を探さねばなと思う。アラートの対象は極力絞ったり、自動復旧でイケる事象はスクリプト組んでおいたり。
あと自分はもともと金融系SEで、運用に用いてたのもJP1やTivoliみたいな商用製品が多かった故、会場で交聞いたnagiosやらcactiやらCloudWatchやらを学ばねばというところ。顧客とビジネスモデルが変わっただけで、見える技術領域もほんとに変わるものだと思う。
こういうopsやインフラに絞ったイベント、なかなかない気がするので良いですね。</description>
    </item>
    
    <item>
      <title>SIerからサービス系の会社に転職して1か月</title>
      <link>https://you.github.io/post/one-month-from-job-changing/</link>
      <pubDate>Sun, 28 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/one-month-from-job-changing/</guid>
      <description>SIerからサービス提供型の会社に転職して1か月ぐらい経ったので、感想とか今後のこととかまとめておく。立ち位置としてはインフラエンジニア、まぁ要は運用担当者になるのだが、BtoB、BtoCで動いている提供サービスすべてと、社内インフラまでひっくるめたありとあらゆるインフラの運用管理を受け持っている。なのでそれこそ雑用めいた仕事から構築にイッチョカミしてサーバー立てたりAWSいじったりみたいなところまで、見渡すべき範囲はアホほど広い感じ。
日常のストレスは減った とはいえストレスは減った。だいーぶ減った。受託開発だと当然ながら顧客側のシステムポリシーだとか会社方針が第一にあり、それが要件になって、マネージャー同士での合意、契約に流れ、そこから上司の指示のもとで設計構築というように自分の動き方を規定するピラミッドがでっかくそびえるのだが、これがごっそりなくなった。インフラ関連で必要な設定作業や修正があれば誰かがRedmineにチケット上げるので、それを期日の早いものからパカパカと片付けていく感じ。もちろん粒度のデカすぎるチケットなんかは上位者が分割して割り振りをするわけだが、制約がんじがらめの中で仕事をこなしている状態からはだいぶ脱している。
とはいえ当然ながら責任とのトレードオフではあるわけで、チケット内の課題解決のためにスクリプト組んで実装するのか、ワークアラウンドでどうにかしちゃうのかはエンジニアの技量次第になったりするし（もちろん後者だと後から追及される可能性があるが）、そもそもスキル範囲外の話はチケット拾えなくて仕事できないとかご迷惑な場合も多々あり、なんとかせねばなという感じがある。
綺麗な運用ってどこに落ちてるんだろう 10年程度存続している企業なので、まぁ必要悪と言っていいのか、属人化してたりブラックボックス化してたり暗黙知化していたりなんて部分は様々見られ、アカンやろなと思うし、またそういう意識がチーム全体の根底にも流れてはいる。前職でもこの辺の課題はあったのだが、逆に綺麗で整った運用ってどんな会社がしているのか興味ある。というかそういう会社の話を聞いてみたいなと。
暗黙知化しているものについては手順化したりスクリプト化したりしていきたい。私は「すべての手作業を生まれる前に消し去りたい」と「人間は信頼性の面でコンピュータに劣る」をモットーとして掲げている人間なので、じゃかじゃかコンピュータ殿が勝手にやってくれる運用に切り替えていこうかなと思う。何年この会社にいるかなんて正直わからないけど、いなくなるまでにそれができれば本望かなと。
ただインフラエンジニアが運用エンジニアになっていく現状に対して疑問をもたなくもない。Infrastructure as Codeなんざの流れもあり、インフラ構築も含めたDevはすべてアプリ開発者が担えるようになりつつあり、一方でOpsはインフラエンジニアが担うというような流れにあるが、本当にOps以外にやることないんですかねえ？っていう。DevOpsはバズワードとして聞き流していた節があるので、昨今の潮流とかきちんと押さえて反映しようと思う。
エンジニアの就労環境って 旧来のIT企業と新進気鋭のところとで何が一番違うか？ってこれだと思うんだけど、弊社の場合もよくある事例ではあるがアーロンチェアが支給されていたり、開発運用に必要な物品はそれなりに気前よく買ってもらえたり、お菓子や飲み物が豊富に用意されていたり、オフィスがなんかシャレオツだったりみたいな感じである。旧来のSIerなんかでこういうの導入している会社はほとんどないのではないかと思うのだが、エンジニアを大事にしているか否かってことになるのだろうか。正直とても助かるし、転職の条件としては今後ちゃんと考えるようにしたい。
あと晴れて例の関東IT健保に入れたので、どっかで寿司食いには行きたいと思う。
やろうと思えば仕事は多いけど思わないとない 利益をあげる方法って売上増やすかコスト下げるかだと思うんだが、運用エンジニアの仕事って基本的に後者である。ただ当然ながらサービスとして稼いでいる前者の仕事がそもそも必要なので、後者、要するに効率化だとか運用改善の部分に上手いこと手が回らず、効率の悪い運用をいつまでも続けている、なんてことには陥りやすい。
前者、売上を上げるべき仕事というのは黙ってても降ってくるので、仕事はある。でもエンジニアとしてそれでいいんですか？というと、やっぱり良くないよねというか、エンジニアリングしてこそだよねと思うので、売上増やすためのタスクはさっさと終わらせて、改善や効率化にじゃんじゃん時間割きたい。そのために試行錯誤している時間ってそんなにはないので、手持ちの武器を増やす方向で進めていければと思う。取り急ぎシェルスクリプトとAWSかなぁと。</description>
    </item>
    
    <item>
      <title>Ansibleの入門イベント聞いてきた話</title>
      <link>https://you.github.io/post/hika-labo-ansible/</link>
      <pubDate>Thu, 25 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/hika-labo-ansible/</guid>
      <description>これ行ってきた。ざくっと感想を箇条書きで。
 ChefやPuppetに相当するインフラの設定管理ツールだと勝手に思い込んでいたが、対象範囲はそれらより広い。要はCapistranoがやるようなことまでまかなえる。 それどころかHomebrewの管理あたりも可。一時期brewfileとか流行ってたけど、冪等性とか考えるとAnsible管理の方がいいかも。  最近そんな記事がちょうど上がってた。→ 【要するに】osxcでMacの環境の構成を記述管理する【MacでAnsible】 ｜ Developers.IO  一番のポイントはエージェントレス、だと個人的にも思う。導入ハードルが低い。個人のVPSとかならChef-Zeroとかよりも気楽で良い。 yamlだからインフラ担当者でも読みやすいって点はそれほど惹かれないというか、Rubyぐらいインフラ屋でも昨今は読み書きできるべきではって気がする。  あとRubyの方がぶっちゃけ処理ベタ書きしちゃえるって点で安心感はある。Ansibleで細かいとこに手が届かないとき、どういうワークアラウンドがあるかはわかってないが。  しかし、とはいえやっぱ楽そう。Vagrantと組み合わせて開発環境立てるみたいな小さなことからやってみて、イケると思ったら本番展開ってのもアリかもしれない。かも。 この手のツールが出たときに「こんなこともできる！すごい！」ってなりがちなんだけど、実は再発明された車輪で成り立ってる部分もあって、それshでできるよ？ってなることは結構ある（brewfileがわりとその気配あった）ので、その点に関してはきちんと見極めがしたい。 あとやっぱり構成管理ツールはインフラエンジニアから完全にdevを奪いにかかるツールではあるので、インフラ屋はスケーリングとかネットワークとか（そういえばネットワーク周りのas codeなかなか流行りませんね）障害対策とか、本気でops特化が求められるのかもなとか思った。  </description>
    </item>
    
    <item>
      <title>ファッション無職になった</title>
      <link>https://you.github.io/post/i-am-neet-now/</link>
      <pubDate>Sat, 16 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/i-am-neet-now/</guid>
      <description>退職エントリーに対して「で、誰？」っていうレスが付くのがもはや定番化しているような気もするけど、ブログなんてだいたいはチラ裏なんだから「誰？」なんて退職エントリーに限らずなんだってそうやんけ、と思ったりはしてます。誰だかわかんない人の文章読むのがネットの醍醐味なんだから、あえて煽らなくてもいいじゃんねー？というネタにマジレス。
というわけで退職、自体は実際にはまだなのだが、すでに現職はフェードアウトしてファッション無職期間に入っていたりする。半月以上の休暇なんて大学以来だからちょっとワクワクしてるし、大学4年の春休みに「こんなに休めることはもうないんやで」とか言われたのは嘘だったんだなーとも思ってる。んで、インフラ系SEを辞めて、来月からSaaS事業者でインフラエンジニアをすることになる。転職にあたってそもそも何がきっかけになったのか、忘れないよう書き留めておく。
 自分は技術的な素養がまだ乏しいと感じているのに、会社側はマネジメントラインへ自分を進ませようとしており、方針の違いが大きくなってきていた。 =&amp;gt; もっと技術畑でやっていきたい。 下請け、外注、アプリとインフラでの分割受注等により、システム全体像が見えないことが多く、システム設計や運用設計の上で歯がゆいことが多かった。 =&amp;gt; 自社開発を重視したい。 怠惰に流れやすい。リスク回避の意味で「変わらない」ことが是とされることが多い。 =&amp;gt; もっと革新的な方針の会社で働きたい。 趣味的に追っている技術動向のスピードと、自分が業務で携わる技術変化のスピードの乖離が激しすぎる。 =&amp;gt; 世の動向をきちんとキャッチアップできるエンジニアでありたい。 というか率直に言ってインフラエンジニアとしての武器を増やしたい。  こんなところか。一言で言えば「焦り」である。就職から4年経ってなお、自分がインフラエンジニアとして技術的に成長できてない、と思う焦り。それを社内でやりくりしてどうにかすることも多分できたと思うし、それを上司に掛け合わないわけではなかったのだが、異動のタイミングは半年に一度しかなかったし、それを4回近く却下されたりしてきたので、じゃあもう自分で動いてしまった方がいいかなと思うに至った。年齢的には27歳で、まだ自由が効くと言えるところでもあったし。
と、いうわけで、今後は今まで以上にインフラ寄りにお勉強を深めていきたい所存。先日のJAWS-UGへの参加なんかも是非とも続けていきたいところでござんす。
34歳無職さん 1 (MFコミックス フラッパーシリーズ)posted with amazlet at 15.05.16いけだ たかし メディアファクトリー (2012-02-23)
売り上げランキング: 15,860
Amazon.co.jpで詳細を見る </description>
    </item>
    
    <item>
      <title>JAWS-UG初心者支部の立ち上げに行ってきた #jawsug_bgnr</title>
      <link>https://you.github.io/post/jaws-ug-for-beginners-at-first/</link>
      <pubDate>Fri, 15 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/jaws-ug-for-beginners-at-first/</guid>
      <description>JAWS-UG初心者支部【第1回】2015年5月14日(木) - JAWS-UG初心者支部 | Doorkeeper
AWS初心者具合がどれぐらいかと言うと、先日のJAWS DAYS 2015のハンズオンセッションでアカウント作り、それっきり一度もログインしていないぐらいの初心者なんですが、今回JAWS-UGの初心者支部というのが出来ると聞いたので飛び込むにはちょうどいいと思い行ってきました。しかし会場の内田洋行さん、初めて行ったけど大変ナウいオフィスでびっくりした。横に長い会場だったけど、6面スクリーン同時投影でどの席でも見やすいとかハンパない。
AWSは入口が山のようにある 初心者と口で言うのは簡単ですが、それを脱する手段も実は山のようにあるのがAWSで。そもそもがマウスでポチポチやればサーバー立ち上がっちゃうっていうサービス自体の簡易性を反映してなのかなんなのか知りませんが、入口といえる部分は非常に多いです。今回の勉強会では「入口」の紹介に非常に時間を割いていたなという印象。
自分が特に印象深かったのはセルフペースラボとWebiner。前者はウェブ上で無料で（有料コースもあり）使える実践的なラボ空間で、自学自習でAWSの使い方が学べるとのこと。自分はVMwareを今専門としているのだが、VMwareでも同様のHands on Labsがあったりして、使えるもん使わなきゃ損だなと。後者はオンライン・セミナーで、毎週開催されているそうです。火曜日18:00からが初心者向け、水曜日18:00からがBlack Belt Tech、すなわち「黒帯」ですので中級者向けのセミナー。
他にも技術書の読み方や推薦があったり、ちょっと上級者向けな気はしたけどRe:Inventの紹介があったり（まーこういうのは早いうちから知っておいて、アンテナ高めといた方が良いのだろう）。とにかく勉強しようと思ったらいくらでもAWSを知るための入口はあるし、しかもかなり敷居の低いところから始めることもできるので、やらないで指くわえてたらどんどん置いていかれてしまうだけだなと。
人と会うこと 勉強会界隈ではよく言われる話で、本番は懇親会ってのがあります。まぁこの集まり自体が「Users Group」であることからもわかる通り、エンジニア同士の会社の枠を超えたつながりってすごく重視されていて、今日のなかでも何度も話に出ました。というか「隣の人と話してみましょう」なワークセッションが設けられていたぐらいの。これはあれですね。勉強会自体の初心者が多いことも見越してのことだったんでしょうね。
自分はもうとにかくコミュ障というか人と話さず済むならそれが一番って感じの人間なんですけど、ここまで言われると話さないわけにもいかないんじゃないかなと思いつつあるし、てか勉強会参加の第一の目的が「コミュ障脱却」になってくるのではないかという話も。。
これまでの勉強会で他の参加者と話したことは皆無ではなくて、んでやっぱり社外のエンジニアだと技術との向き合い方だとか、会社環境における技術選択の方式だとかが全然違うことが多くて、そういうの聞いてるだけで確かに楽しいのは知ってるんですよね。んでユーザーグループとなるとそのつながりがずっと続いていくわけで、絶えず情報交換しながら自分の会社に持ち帰って試してみて、また勉強会に課題を持ってくるみたいなサイクルが出来てくるわけじゃないですか。会社内だとなかなか解決できないことを外に出すこともできるわけで、閉塞感を打破する矛先を持っておくことってすごい重要なんじゃないかって気がします。近所にもJAWSの支部あるっぽいんで、そっちにも顔出してみたい……ですね、なんとか。あー、初心者向けならコミュ障のための勉強会参加法も教えてほしいなぁ。
初心者の中での隔絶 セッションを聞きながらTwitterでハッシュタグ追っていたのですが、結構レベル高くないか？難しいよ？みたいな声もちょこちょこ聞こえていて、「初心者」とひとえに言ってもレベルの差があるのだと気付いた次第。
それを言ってしまうと多分自分は「なんちゃって初心者」です。オンプレミスのインフラエンジニアとしては数年の業務経験がありますし、VPS使ってるしAWSもアカウントは持ってるしで、完全な初心者かと言うとそうではない。一方で本当に「AWSってよく聞くけどなんなの？ 導入したらおいしいの？」ぐらいの人もいるのだし、敷居をどこまで下げていくかって案外むずかしい話なのかもしれないなと。初心者向けを謳っているのに「いやいや難しいでしょ」で人が離れてしまったら悲しいし、そこへのフォローってどうしたらいいのかなとか。
あと「わからないことを取りあえずスルーする力」ってのも必要な気がした。今日のセッションって結構具体的なAWSのサービス名も出たりして、自分も全然わからない言葉は少なくなかったんですけど、一つ一つの単語や一部の話はわからなくとも、全体として何を言っているのか掴めれば取りあえずOKってことも多いし、わからない部分にこだわりすぎず、ある程度スルーする力って必要だと思うのです。でもこれって日々「わからないこと」と向き合っているエンジニアならではの特性っていう部分もあると思うので、そうじゃない人も入ってくる可能性のあるこの初心者支部では課題になりそうとも思った。
やろうと思えばいくらでもやることあるし、やらないと置いて行かれるだけだってのが理解できたので、できることからガツガツやっていきたい所存。とはいえ「目的のない勉強」は行き詰まりやすいので、AWSで何ができるのか、自分は何をしたいのかをちょっと考えてみようかなと。とりあえず「紫本」買ってみるか。。。</description>
    </item>
    
    <item>
      <title>Hash与えるとGoogleスプレッドシートに入力してくれるRubyスクリプト</title>
      <link>https://you.github.io/post/google-spread-sheet-update-via-ruby/</link>
      <pubDate>Sun, 03 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/google-spread-sheet-update-via-ruby/</guid>
      <description>  つくった。動きとしては、項目名と入力値からなるハッシュを引数で与えてやることで、該当スプレッドシートの2列目に符合するキーがある場合、その値を一番右側の列に入力してくれる。符合するキーがない場合は、メッセージを吐いた上で最下行に新しい項目として追加する。
用途としては非常に個人的なもので、各種オンラインバンクの残高をスクレイピングしてハッシュで返してくれるスクリプトをいくつか作ってあったので、その戻り値を使ってオートで家計簿作れたらいいなーという思いによるものです。ハッシュのキーをそのままスプレッドシート内の項目名として使っているので、シンボルではなく文字列をキーとして使ってしまっているのがあまりよろしくないのかなぁとは思うのだが、いつか改善するってーことで、とりあえず動くものを作ることを優先させた。んで、これって結構汎用的に使えそうなスクリプトかもなと思って公開した次第。
Googleスプレッドシートをいじるのにはgoogle-drive-rubyというGemを使ってます。Githubはここ。def initialize内の処理は、このGemの初期設定によるものなので、Gemの方のReadme読んでもらえればよいかと。単純な話、GoogleのOAuth API使っているだけの話です。ただ、このコードだと叩くたびにブラウザからAPI使用許可を与えてやって、success codeをコピーしてコマンドラインで入力してやらなくちゃならないっていう手間があって、そこまで省く方法なにかありそうだけどまだ調べてない。あと気になっている点としては、このGemでセルの値を取ると、表示値しか取れないこと。式を入れているセルについては、式を取るか表示値を取るか選べるといいなぁと思ったんだけど、そこまではできないらしい。しかしまぁ、とにかくGoogleスプレッドシートという、APIで叩けるクラウドの表計算ソフトがあるというのは本当に便利なことですね、って感じ。
参考  RubyからGoogle SpreadSheet をいじるメモ - Qiita Google DriveのスプレッドシートにRubyでアクセスする方法 - Qiita  </description>
    </item>
    
    <item>
      <title>GTD環境 2015年春版（Workflowy最強説）</title>
      <link>https://you.github.io/post/gtd-2015-workflowy/</link>
      <pubDate>Wed, 29 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/gtd-2015-workflowy/</guid>
      <description>GTDという言葉もなんだか多義的に使われているような気がするが、自分の場合は「今あるタスクを『高度』の概念から見直して、きちんと前に進むことができているか確認すること」と、「やるべきことが全部可視化されていること」に重点を置いている。
前回GTD関連をきちんとまとめ直したのはどうもこの記事のようなのだが、さすがに古くてここからだいぶ変わっている。とかく面倒であるということが中心にあって、使うツールは少なければ少ない方がよいし、完全に管理しようとするよりもある程度割り切ってしまった方が上手く回る。現時点で使っているのはほとんどGoogle CalendarとWorkflowyだけだ。特にWorkflowyにはだいぶ助けられている感がある。
Workflowy オンラインのアウトライナーツールである。そんなのVimでいいんじゃねーかと思ったこともあったが、わりと使い勝手が良くて使い続けている。ポイントとしてはショートカットがかなり豊富で、TabやShift + Tabでインデント／アウトデントできるなんてのはもちろん、Ctrl + Enterで取り消し線引けたり、階層の折りたたみもできたりと、キーボードだけでほとんどの操作が完結できる点にある。あと貧相ではあるがiPadアプリとAndroidアプリがあるのでモバイル環境でも使えるし、@hogeとか#fugaって形式でタグを生成して絞込かけたりもできる。タグで絞り込んだ状態や一部階層だけを表示した状態にはそれぞれパーマリンクが与えられるから、アクセスも楽でよい。

自分がこれを何に使っているかと言えば、ほぼGTDの全部と言っていい。各高度の可視化、PJ（高度1000mにあたる）ごとの手順、細切れのタスク、などなど。GTDというのは単なるタスク管理の手法ではなく、高度のような文章化される内容まで管理しなくてはならなかったり、あるいは単純にタスクだけを目にするのではなくて、マイルールとか目標といったものも一度に管理したいなという思いもあったので、結果的にいわゆるタスク管理系のソフトを捨ててWorkflowyに辿り着いた。アウトライナーなのでフォーマットは自由だが、階層化できるので高度ごと、ジャンルごとなどでまとまった意味単位を作りやすいし、取り消し線機能を使えばタスク管理っぽいこともできる。フットワーク軽く使えるという点でこれはだいぶ重宝する。
あとWorkflowyでは1日の更新差分をメールで転送してくれる機能があって、これ使ってEvernoteに差分を貯めこんでいる。週次レビューでその週にやったことを見直すときに便利。
Google Calendar いわずもがなのカレンダー。これ以外にクラウドのカレンダーツールって選択肢あんのかもわからないぐらいのデファクトスタンダード。ただWeb版の使い勝手はあまり良いとは思えていなくて、代わりにSunrise Calendarを使っている。予定を追加するとき、Google Calendarみたいに別画面へ遷移することなく、バルーン上ですべての項目を登録できるあたりがお気に入り。あと全体的に使われている色の彩度が低めで見やすい。

時間の決まったタスクだとか、繰り返しやることなんかはすべてGoogle Calendar行きである。あとWorkflowy上で記録しているタスクでも、今日やれそうだ、明日やれるかなと思ったらGoogle Calendarにツッコんでリマインダーかけている。スマホだと予定確認にはCalを使っているので、リマインダーについてもピックアップしてくれて忘れにくい。ちなみにCalを使っている理由は、スマホでカレンダー見るときは先々の予定を考えるときより、その日の予定とタスクを確認するときの方が多かったから、CalのUIがしっくり来たってことです。というかスマホの小さい画面だと月間予定とか見るのはしんどい。
回し方 基本的にはこれらのツールを使って週単位でタスクを回している。1週間終えた金曜日の夜に週次レビュータイムを設けていて、さっき書いたWorkflowyの差分やらモレスキンやDropboxに溜め込んだメモを下に、KPTで週を振り返る。で、終わったタスクはWorkflowyから消して、予定見ながら空いている時間を確認しつつ、次週やることをNext Actionとしてピックアップする。長期間進んでいないプロジェクトなどがあれば要チェック。塩漬けのタスクがあれば「やる必要があるのか」再検討。週の行動目標をタスクとは別に何かしら定めてWorkflowyに記入。まぁ、そんな感じです。
問題点として、だいたい毎週進歩がないというか似たようなProblemが出てきちゃってるなぁというのが最近あるので、Tryにあたる内容はちゃんとタスクに落としこむことと、無理なくスモールスタートにできる範囲にしといた方がいいってことです。GTDは確かにやること、頭の中を可視化はしてくれるけど、結局それを進めていくのは自分だってことを忘れちゃいけないので。というか、そこがGTDで一番キモだし難しいところなんじゃないかなと思っている。</description>
    </item>
    
    <item>
      <title>Ruby基礎学習(10) Mix-in</title>
      <link>https://you.github.io/post/ruby-study-mix-in/</link>
      <pubDate>Fri, 24 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/ruby-study-mix-in/</guid>
      <description> Mix-inの話と、それに似たもろもろ。他に詳しい記事があるので、これを読んでおけばいいような気はした。
参考  requireとincludeとextendとmodule_function(1) : As Sloth As Possible requireとincludeとextendとmodule_function(2) : As Sloth As Possible  require  Kernelモジュールのモジュール関数。 引数に与えたRubyライブラリを1回ロード、というか実行する。 使う場面としてはgemだとか自作のライブラリ（クラス）を読み込むときに指定する。 指定した引数は$LOAD_PATHに探しに行き、カレントディレクトリは含まれないため、パスの指定には少し注意が必要。 同じファイルを複数回requireしようとしても、1回しか読み込まない。  load  Kernelモジュールのモジュール関数。 requireと同様に外部ライブラリを実行するが、同じファイルを何度でも読み込める。 requireは拡張子の自動補完を行うが、loadは行わない。  include  Moduleクラスのインスタンスメソッド。 Moduleを引数に取り、メソッドや定数といった対象Moduleの性質を取り込む。 ArrayやHashがEnumerableの性質を持っているのはincludeしていることによるもの。 継承とは異なるが、メソッドの探索対象としてはスーパークラスよりincludeされたModuleの方が先になる。 同じモジュールを複数回読み込もうとしても、2回目以降は無視される。 Rubyは多重継承を認めていないが、その代わりの機能を果たすという位置付けらしい。  extend  Objectクラスのインスタンスメソッド。 引数に取ったModuleのメソッドを特異メソッドとして取り込める。  </description>
    </item>
    
    <item>
      <title>Ruby基礎復習(9) Dirクラス</title>
      <link>https://you.github.io/post/study-ruby-dir/</link>
      <pubDate>Tue, 14 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-dir/</guid>
      <description>『パーフェクトRuby』p.208より。
Dirクラスは基本としてカレントディレクトリ情報を持っていて、それを元としてディレクトリ操作ができる。従って多くの操作を特異メソッドで行うことができる。
Dir.pwd # =&amp;gt; &amp;quot;/Users/chroju&amp;quot; Dir.chdir(&amp;quot;/tmp&amp;quot;) Dir.pwd # =&amp;gt; &amp;quot;/tmp&amp;quot; Dir.home # =&amp;gt; &amp;quot;/Users/chroju&amp;quot;  ディレクトリに含まれるファイルはDir.entriesで配列として返り、Dir.foreachでEnumerableとして返る。またDir.globにより、パターンにマッチするファイルパスを配列で返すこともできる。Dir.globはDir[]と同義である。引数のディレクトリが存在するか確認する場合はDir.exists?を用いる。
Dir.entries(&#39;.&#39;) # =&amp;gt; [&amp;quot;.&amp;quot;, &amp;quot;..&amp;quot;, &amp;quot;bar&amp;quot;, &amp;quot;foo&amp;quot;, &amp;quot;baz&amp;quot;] Dir.foreach(&#39;.&#39;) {|d| p d } # =&amp;gt; &amp;quot;.&amp;quot;, &amp;quot;..&amp;quot;, &amp;quot;bar&amp;quot;, &amp;quot;foo&amp;quot;, &amp;quot;baz&amp;quot; Dir.glob(&#39;ba*&#39;) # =&amp;gt; [&amp;quot;bar&amp;quot;, &amp;quot;baz&amp;quot;] Dir[&#39;ba*&#39;] # =&amp;gt; [&amp;quot;bar&amp;quot;, &amp;quot;baz&amp;quot;] Dir.exists?(&amp;quot;hoo&amp;quot;) # =&amp;gt; false  ディレクトリの削除、生成等も特異メソッドにて。
Dir.mkdir &#39;foo&#39;, 0755 # パーミッション0755でfooディレクトリを生成 Dir.rmdir &#39;foo&#39; # fooディレクトリを削除するが、対象ディレクトリは空である必要がある Dir.delete &#39;foo&#39; # Dir.rmdirと同義 Dir.unlink &#39;foo&#39; # Dir.rmdirと同義  Dir.</description>
    </item>
    
    <item>
      <title>Ruby基礎復習(8) Fileクラス</title>
      <link>https://you.github.io/post/study-ruby-file-i-o/</link>
      <pubDate>Fri, 10 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-file-i-o/</guid>
      <description>『パーフェクトRuby』p.196より。わりと苦手な分野。
まずはファイルをひらく。#openして変数に格納してもいいし、ブロックを引き渡して処理させることもできる。後者の場合は処理が終わると自動でクローズしてくれるので、こっちの方が楽っぽい。#readはファイルの内容全体を読み込む一方、#getsを使うと1行ずつ読み込むことができる。あるいは#each_lineや#each_charといったメソッドも。
file = File.open(&#39;example.txt&#39;) p file.read # example.txtの内容を表示 file.close File.open &#39;example.txt&#39; do |file| p file.read end File.read(&#39;example.txt&#39;) File.open &#39;example.txt&#39; do |file| while line = file.gets p line end end File.open &#39;example.txt&#39; do |file| f.each_line do |line| p line end end  書き込むときは#openの第二引数にファイルを開くモードを指定する。デフォルトは&#39;r&#39;、すなわち読み込みモードで、他は以下の通り。基本はrが読み込み、wが書き込み、aが追記で、+を付けると読み書き両用モードになる。またbを後置するとバイナリモードで開かれる。
| r | 読み込みモード | | r+ | 読み書き両用モード（読み書き位置は先頭から） | | w | 上書き書き込みモード | | w+ | 新規作成して読み書き両用モード | | a | 追記書き込みモード | | a+ | 追記読み書き両用モード（読み込み位置は先頭から、書き込みは追記形式） |</description>
    </item>
    
    <item>
      <title>Ruby基礎復習(7) Timeクラス</title>
      <link>https://you.github.io/post/study-ruby-time/</link>
      <pubDate>Mon, 06 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-time/</guid>
      <description>『パーフェクトRuby』p.190より。
Time#nowかTime#newで現在時刻が取得可能。
now = Time.now # =&amp;gt; 2015-03-16 23:25:32 +0900 new = Time.new # =&amp;gt; 2015-03-16 23:25:32 +0900 now.zone # =&amp;gt; &amp;quot;JST&amp;quot; now.getutc # =&amp;gt; 2015-03-16 14:25:32 UTC now.utc now.zone # =&amp;gt; &amp;quot;UTC&amp;quot;  現在時刻以外のTimeオブジェクトを生成するにはTime#atでUNIX秒を引数に指定するか、Time#utcかTime#localで直接時刻を指定する。
Time.at(0) # =&amp;gt; 1970-01-01 09:00:00 +0900 Time.utc(2015, 1, 1, 2, 30, 40, 100) # =&amp;gt; 2015-01-01 02:30:40 UTC (最後の100はマイクロ秒) Time.local(2015, 1, 1, 2, 30, 40, 100) # =&amp;gt; 2015-01-01 02:30:40 +0900  #to_i、#to_f、#to_rの戻り値はUNIX秒。#to_sで文字列表現が返る。#to_aは秒、分、時、日、月、年、曜日、その年の通算日数、夏時間の真偽判定、タイムゾーンの配列を返す。なお、この配列フォーマットを展開してTime#utcやTime#localの引数として与えることもできる。
now.to_i # =&amp;gt; 1426515932 now.</description>
    </item>
    
    <item>
      <title>Ruby基礎復習(6) Hash</title>
      <link>https://you.github.io/post/study-ruby-hash/</link>
      <pubDate>Sun, 05 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-hash/</guid>
      <description>『パーフェクトRuby』p.179より。
まず基本的なとこで。
hash = {hoge: 1, fuga: 2} hash.each do |key, val| p &amp;quot;#{key}: #{val}&amp;quot; end # =&amp;gt; &amp;quot;hoge: 1&amp;quot;, &amp;quot;fuga: 2&amp;quot; hash.each_key do |key| p key end # =&amp;gt; &amp;quot;hoge&amp;quot;, &amp;quot;fuga&amp;quot; hash.each_value do |val| p val end # =&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;2&amp;quot; hash[:hoge] = 3 p hash # =&amp;gt; {hoge: 3, fuga: 2} hash[:piyo] = 4 p hash # =&amp;gt; {hoge: 3, fuga: 2, piyo: 4} hash.delete(:piyo) hash # =&amp;gt; {hoge: 3, fuga: 2} hash.</description>
    </item>
    
    <item>
      <title>the world as code</title>
      <link>https://you.github.io/post/the-world-as-code/</link>
      <pubDate>Sat, 04 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/the-world-as-code/</guid>
      <description>世界を構成するのはテクストである、という考え方が好きだ。より正確な言い方をするならば記号論やミーム的な考え方になるのだと思うが、記述されたもの、意味を成して認識されたものだけが実在足りうる、というような世界観をなんとなく抱いている。『ニルヤの島』では個人の生が物語へと還元され、データとして外部記憶装置へ保存されるようになった。『from the nothing, with love』では、ジェームズが自らを「生起しつつあるテクスト」であると述懐する。あるいはヘプタポッドの言語は、未来をも決定論的に「記述」する。
特にことインターネットの隆盛により、世界はテクストの、ミームの満ちるものへと変容しつつあるように思う。インターネット上に存在する「個人」とは、すなわちミームに他ならない。インターネットへのアクセスをしていても、能動的にテクストを紡がない個人は存在しないに等しい。この世界では個人は、あるいはあらゆる事象はデータへと還元され、そして半永久的にミームの海を彷徨っていく。
上述したように、最近頓に多い「言葉」に関するSFのなかで、最も好きなのは『屍者の帝国』なのだけど、ここでは人間の魂自体が「言葉」によるものと解されており、そして屍者は「言葉」によってフランケンシュタインと化す。言葉は物質化する。書物がそうであるように。歴史上の人物がそうであるように。これが自らもまた「物質化した言葉」であるはずのヴァン・ヘルシングの言葉であるというのは皮肉でもあるのだと思うが、生きとし生けるものが言葉によりもたらされるというハッキリとした記述と、それに基づいて構成された世界観は実に興味深い。
Infrastructure as Code、物理的な世界の技術であったはずのITインフラが、近年言語により記述され、管理、構築されるフェーズへと転換したように。Internet of Things、家電や家具といった非電子的であったはずの「モノ」たちが、APIを提供して「言語」による働きかけを許すようになったように。我々エンジニアの一つの使命は、万物の情報化であると思う。言語が支配する世界にすべてを置き換え、言語を介した制御を可能とすること。それがエンジニアとしてやるべきことなのだと。
私はなぜ書くのか？という問いに対する答えはあまりに簡単で、それは生きるに等しい行為だからだ。語らぬ者は存在しないのならば、語る以外に選択肢はない。世界と関わりたいのならば、言語によって働きかけていくしかない。紡いだ言葉がミームの海を流れていき、対岸でやがて物語として物質化する日を夢見る。言葉が世界を構成し、言葉が万物を紡ぎ上げて、やがて物語と化していく。
the world as code.
世界は言葉で成り立っている。
あなたの人生の物語 (ハヤカワ文庫SF)posted with amazlet at 15.04.03テッド・チャン 早川書房 売り上げランキング: 10,379
Amazon.co.jpで詳細を見る </description>
    </item>
    
    <item>
      <title>JAWS DAYS 2015でAWS童貞捨ててきた</title>
      <link>https://you.github.io/post/jaws-days-2015/</link>
      <pubDate>Sun, 22 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/jaws-days-2015/</guid>
      <description>JAWS DAYS、前々からやってるのは知ってたんですけど、自分はAWS経験ないし行ってもわかんねーかなと思ってなんとなく行かずにいたんですが、今回タイムテーブル覗いてみたら初心者向けハンズオンもあったので意を決して行ってきました。
ハンズオンでAWSアカウント作り、とりあえずEC2のインスタンスを1つばちこんと立てて、もう1こ簡単なREST API使ったサービスをばちこんと立てたので、無事にAWS童貞捨てることができました。クリック1つでサーバーが立つってのは知ってはいたけど、実際やってみるとほんと楽だなと。ていうかこれがあるならインフラエンジニアって何のためにいんの？ってやっぱり思うのですよね。もちろん大規模に組むならどこにどのサービス使ってスケーリングの設定はどうでみたいのがいるし、サーバーとストレージとLB立てるってだけがエンジニアではないと思うけど、サーバー1つ立てんのにいちいち申請上げて手順書き出して何人日もかけてやってる自分と比べると、デプロイのスピードも容易性も、おまけに確実性も段違いなわけで。わかってる、わかってるつもりだったけど、こりゃもう無理だなというか、クラウドファーストってよりAWSファーストが前提にあって、オンプレミスはなにか制限がある場合の最終手段にしかならんよなということを改めて実感してしまった気がします。
セッションは結果としてわりとミーハーに聞いてしまって、ソニックガーデン倉貫さんの話だとかハンズラボ長谷川社長の話だとか、さくらインターネット田中社長がモデレーターをつとめるパネディスとかに参加してました。特に倉貫さんの「納品のない受託開発」の話、これまできちんと聞いたことなかったのですんごい興味を惹かれました。「受託開発」と言ってますけど、実質的には顧客との関係は受託開発よりも強固なもので。要するにビジネスモデルはあるけどエンジニアがいないようなスタートアップに対し、技術顧問を務めるような形で開発と運用を請け負うのですね。それはシステムを作って収めるというよりは、顧客の課題解決を一緒になってシステム開発によって実現していくこと。エンジニアの働き方の概念自体が変化する話。これをソニックガーデン社外の人間がすぐ真似できんのかと言ったらそうではないかもしれませんけど、現状の特に死に体になってる受託開発界隈に対して一石を投じるには十分過ぎる話だと思いました。
  「納品のない受託開発」の先にある「エンジニアの働きかたの未来」  from Yoshihito Kuranuki 
あとハンズラボの話に関してはこのツイートの内容に尽きる気がします。正直、羨ましいというか、今でこそ先駆的な一例に過ぎないけど、たぶんこういう例は徐々に増えていく、その一端なのだろうなと思っている。
ハンズやあきんどスシローのすごいところは、それまでtech companyっぽくない印象だった業態が、じつは #jawsdays で先進的な事例として講演できるようなことをやってのけたところだと思う。 クラウドだからできた。
&amp;mdash; Haruka Iwao (@Yuryu) 2015, 3月 22 
AWSのポイントはやっぱり、やろうと思えばすぐなんでもやれるって点だと思うんですよね。サービスやシステムを構築するにあたり、インフラをデリバリーするスピードがAWSによって格段に上がった。今まで何人日、何人月という工数をかけて、それでもヒューマンエラーで障害起こしてたようなインフラが意味を成さなくなった。じゃあその時代にインフラエンジニアは何をしなければならないのか？ってのは、ほんとちゃんと考えなきゃ死ぬな―これ。</description>
    </item>
    
    <item>
      <title>Ruby基礎復習(5) 配列(Array)</title>
      <link>https://you.github.io/post/study-ruby-array/</link>
      <pubDate>Wed, 18 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-array/</guid>
      <description>『パーフェクトRuby』p.171より。
まずは配列の生成をいくつか。#new(a,b)で生成したとき、各要素は同じオブジェクトとなるので注意。またブロックで受け取ると、インデックスを引数としてブロック内の処理を実行した結果が値となる。
a1 = Array.new(3,1) # =&amp;gt; [1, 1, 1] a2 = Array.new(3, &amp;quot;hoge&amp;quot;) # =&amp;gt; [&amp;quot;hoge&amp;quot;, &amp;quot;hoge&amp;quot;, &amp;quot;hoge&amp;quot;] a3 = [&amp;quot;hoge&amp;quot;, &amp;quot;fuga&amp;quot;, &amp;quot;piyo&amp;quot;] # =&amp;gt; [&amp;quot;hoge&amp;quot;, &amp;quot;fuga&amp;quot;, &amp;quot;piyo&amp;quot;] a4 = Array.new(3) {|i| i * 5} # =&amp;gt; [0, 5, 10] a2[0] &amp;lt;&amp;lt; &amp;quot;fuga&amp;quot; p a2 # =&amp;gt; [&amp;quot;hogefuga&amp;quot;, &amp;quot;hogefuga&amp;quot;, &amp;quot;hogefuga&amp;quot;]  基本的な操作系メソッド。演算子メソッドは直感的でほんといいなーと思う。なお、#&amp;lt;&amp;lt;は#concatと同義。#==と#eql?も同義。
array = [&amp;quot;hoge&amp;quot;, &amp;quot;fuga&amp;quot;, &amp;quot;piyo&amp;quot;] array.length # =&amp;gt; 3 array.size # =&amp;gt; 3 array.empty? # =&amp;gt; false array.</description>
    </item>
    
    <item>
      <title>Ruby基礎復習(4) EnumerableとComparable</title>
      <link>https://you.github.io/post/study-ruby-enumerable-comparable/</link>
      <pubDate>Tue, 17 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-enumerable-comparable/</guid>
      <description>『パーフェクトRuby』p.164より。
一部組み込みクラスは、EnumerableやComparableというモジュールがincludeされている。前者は聞き慣れない英単語だが、&amp;rdquo;can be counted&amp;rdquo;の意味らしく、HashやArrayといった一定の集合を表すクラスにincludeされていて、繰り返し処理や要素抽出に関するメソッドを実装する。Comparableはその名の通り比較演算、具体的には#&amp;lt;=&amp;gt;の実装であり、NumericやStringにincludeされているらしい。
特に実装されるメソッド数が多いので、Enumerableについてじっくり見てみたい。
Enumerable まず繰り返し系。これだけでもかなり。。#each_consのconsって何の意味ですかね。他はだいたい字義からイメージできる動作をしてくれる。あと#each_with_objectがいまいち飲み込めてない。
(1..4).each_cons 2 do |a,b| p [a,b] end # =&amp;gt; [1,2] [2,3] [3,4] (1..4).each_slice 2 do |a,b| p [a,b] end # =&amp;gt; [1,2] [3,4] %(hoge fuga piyo).each_with_index do |value, index| p &amp;quot;#{index}: #{value}&amp;quot; end # =&amp;gt; 0: hoge 1: fuga 2: piyo (1..4).each_with_object([]) {|i, result| result &amp;lt;&amp;lt; i*2} # =&amp;gt; [2,4,6,8] (1..4).reverse_each do |i| p i end # =&amp;gt; 4,3,2,1 (1..4).cycle {|i| p i} # =&amp;gt; 1,2,3,4,1,2.</description>
    </item>
    
    <item>
      <title>Ruby基礎復習(3) Numericクラス</title>
      <link>https://you.github.io/post/study-ruby-numeric/</link>
      <pubDate>Mon, 16 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-numeric/</guid>
      <description>パーフェクトRuby p.144より。
判定系メソッド、#nonzeroが#zeroの真逆の動きではなくてちょっと混乱しそう。あと#integer?はあるけど#float?はないとか。
0.zero? # =&amp;gt; true 3.zero? # =&amp;gt; false 0.nonzero? # =&amp;gt; nil 1.nonzero? # =&amp;gt; 1 1.integer? # =&amp;gt; true 1.real? # =&amp;gt; true  演算子系の話は割愛するが、宇宙船演算子だけ注意しとく。右辺（引数）の方が大きければ負。左辺（レシーバ）が大きければ正。#sortではブロック内の戻り値が負の場合は2要素をそのまま、正の場合は逆順にして返してくる。宇宙船演算子を利用して昇順or降順に並べ替えることができる。
1 &amp;lt;=&amp;gt; 2 # =&amp;gt; -1 2 &amp;lt;=&amp;gt; 1 # =&amp;gt; 1 1 &amp;lt;=&amp;gt; 1 # =&amp;gt; 0 %w(aaaa aa aaa).sort { |a,b| a.length &amp;lt;=&amp;gt; b.length } # =&amp;gt; [&amp;quot;aa&amp;quot;, &amp;quot;aaa&amp;quot;, &amp;quot;aaaa&amp;quot;]  丸め。#roundが四捨五入。#ceilで切り上げ。#floorで切り捨て。馴染みのない英単語で覚えにくい。
1.4.round # =&amp;gt; 1 1.4.ceil # =&amp;gt; 2 1.</description>
    </item>
    
    <item>
      <title>Ruby基礎復習(2) Stringクラス</title>
      <link>https://you.github.io/post/study-ruby-string/</link>
      <pubDate>Sun, 15 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-string/</guid>
      <description>パーフェクトRuby p.148から学習。文字列ことStringクラス。
まずは基本操作系。
s = &amp;quot;hoge&amp;quot; s.empty? # =&amp;gt; false s.length # =&amp;gt; 4 s.size # =&amp;gt; 4 s.bitesize # =&amp;gt; 8 s.include?(&amp;quot;og&amp;quot;) =&amp;gt; true  演算子での操作。
&#39;hoge&#39; + &#39;fuga&#39; # =&amp;gt; &#39;hogefuga&#39; &#39;hoge&#39; * 3 =&amp;gt; &#39;hogehogehoge&#39;  破壊的な文字列の追加。
s = &amp;quot;hoge&amp;quot; s &amp;lt;&amp;lt; &amp;quot;fuga&amp;quot; # =&amp;gt; &amp;quot;hogefuga&amp;quot; s.concat(&amp;quot;piyo&amp;quot;) # =&amp;gt; &amp;quot;hogefugapiyo&amp;quot;  切り出し。
s = &amp;quot;hogefuga&amp;quot; s.slice(3) # =&amp;gt; &amp;quot;e&amp;quot; s.slice(2,5) # =&amp;gt; &amp;quot;gefu&amp;quot; s.slice(-4,2) # =&amp;gt; &amp;quot;fu&amp;quot; s.</description>
    </item>
    
    <item>
      <title>Ruby基礎復習(1) 基礎文法</title>
      <link>https://you.github.io/post/study-ruby-grammer/</link>
      <pubDate>Sun, 08 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/study-ruby-grammer/</guid>
      <description>去年末から今年のはじめにかけてRuby Silver/Goldの再受験無料キャンペーンってのやってて、おーこりゃちょうどいいわーと思って取りあえず受けたら見事に落ちたんですけど、その後再受験申し込み期間あるの忘れてて棒に振るとかよくわからんことやりました。そのままなんとなーくやる気なくなってたけど、さすがにSilverクラスの知識はきちんと押さえるべきだろってことで、しばらく手元の『パーフェクトRuby』使って自分用チートシートっぽくまとめてみます。超基礎なので退屈な記事が続く予定。
今回は文法面で自分がつまずいたポイントまとめる。
変数のスコープ ローカル変数、グローバル変数、インスタンス変数、グローバル変数がまず基本にある。それぞれ文字通りではあり、ローカル変数は最も局所的なスコープ、グローバルはどこからでも参照可能（あまり使わない？）、インスタンス変数は個々のインスタンスに属する変数、クラス変数はクラス間で共有される変数。
def display_local puts hoge end def display_global puts $hoge end $hoge = &amp;quot;Hello, world!&amp;quot; display_local # =&amp;gt; NameError: undefined local variable or method `hoge&#39; for main:Object display_global # =&amp;gt; &amp;quot;Hello, world!&amp;quot;  class Hoge @@class_var = &amp;quot;Hello, world from class!&amp;quot; def display_class_var puts @@class_var end attr_accessor :ins_var end i = Hoge.new j = Hoge.new i.display_class_var # =&amp;gt; &amp;quot;Hello, world from class!&amp;quot; j.display_class_var # =&amp;gt; &amp;quot;Hello, world from class!</description>
    </item>
    
    <item>
      <title>『エンジニアのための時間管理術』読了</title>
      <link>https://you.github.io/post/book-review-time-management-for-engineer/</link>
      <pubDate>Sat, 07 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/book-review-time-management-for-engineer/</guid>
      <description>エンジニアのための時間管理術posted with amazlet at 15.03.07Thomas A. Limoncelli オライリー・ジャパン 売り上げランキング: 12,858
Amazon.co.jpで詳細を見る 先日のデブサミでオライリーの書籍販売があったわけですけど、5000円以上購入でトートバッグプレゼントというのに釣られて買った1冊です。本当は2冊で5000円届くはずだったんだけど、そのときはディスカウントがあったばかりに5000円に届かず3冊という、得したんだか買いすぎちゃったんだかよくわからない感じでした。まぁ、前々から気になって立ち読みしまくってた1冊なので、買うきっかけくれたことには感謝。
内容はGTDとか散々やってきてる自分にとってそれほど新鮮味のあるものではありませんでした。煩わしいタイムマネジメントに関することは全部頭の中から放り出して、その日のタスク、スケジュール、目標なんかはひとつの場所にまとめておこうねっていうのが全体の趣旨です。あとはエンジニアならではなのかな？と思えることとして、手順の文書化の話だったり、スクリプトによる雑事の自動化周りの話がちょっぴり載ってたりはします。
本書内ではタイムマネジメントに使う媒体（デジタルでもアナログでもなんでもいいけど、とりあえず何かしら「1か所」にまとめる）のことをオーガナイザーと読んでいるのですが、そこに書くことは大きく3つあり、1つは365日のタスク。GTDをはじめ、よく聞かれるタスク管理法だと「やるべきこと」は全部1つのリストにドバっと書き出していることが多いわけだけど、本書ではそういう「永遠に終わらないリスト」は「破滅のリスト」と呼んで回避するよう告げている。タスクリストを1日分でクローズドにすることで、その日にやったことを明示的にリスト内で「潰せる」ようにし、達成感を味わえるようにした方がよいと。2つ目はカレンダー。日時の決まった予定だとか、繰り返しのルーチンはカレンダーに書いちゃった方が早い。3つ目が長期的な目標。月単位や年単位の長いスパンで何を実現したいのか？を書き出しておく。その手順は細かく分割した上で、365日のタスクリストや、カレンダー上に載ることになる。そして目標リストは定期的に見直して、更新していく。
自分が今やってることとこれ、わりとよく似ていて。自分の場合、スケジュールがGoogle Calendarで、タスクリストはAny.do。以前todo.txtを使っているという記事を書いたこともあるのだけど、最近乗り換えてしまった。なぜかと言うとスマホアプリのCalを使うと、1日のGCal上の予定とAny.doのタスクがまとめて見られてすんごい楽だから。
Cal – Calendar for iCloud, Google &amp;amp; Exchange 開発元:Any.DO  無料 posted with アプリーチ          GTDが唄うような「すべてのタスクが網羅されたリスト」ってのはあまりに雑多で自分も嫌いで、なのでAny.doの中身は週に一度見直して、その週にやることを「TODAY」として扱うようにしている。日次リストが本当は理想なのだろうけど、残業も少なくない中で日次でできるタスクなんざたかが知れているので週次がちょうどいい。今日できなくても明日やればいい、みたいな調整もわりと楽に出来るし。
長期的なやりたいこととか取り組んでいることはプレーンテキストで残している。タスク管理アプリのカテゴリー機能などを使う人も多いようだけど、長期的な取り組みはだいたいがその途上で疑問が湧いてきたり、参考URLなぞを書きたくなるもので。だから柔軟なフォーマットで記録しておきたいなという思いが強く、今はプレーンテキストを使っている。
だいたいこれで上手く回ってるかなーと思ってはいるのだが、気にかかっていることが2つだけある。1つはルーチンの扱い。そのほとんどが家事ではあるのだが、定期的にやらなきゃならんことって案外多くて、Google calendarに全部記録するとなかなかにとんでもない量になって嫌気が差す。かといって他のリストを使うというのもあまり効率的には思えないので、現状は「日次タスク」というような大きな予定枠を取っておき、メモ欄にその内容を細かに書く、といった運用になっている。もうちょっとスマートにならぬものか。あとルーチンって定期的に決まったタイミングでやるもののみならず、普段はやらないけどたまに思い出さなくちゃならないよね！みたいなものもあって、そういうのまで「決まった日時に繰り返す」としてカレンダーに入れちゃうとちょっとしんどいなぁと思える。でも、そうした方が忘れずに済むのだろうなぁとも思うから、なるたけ割り切るようにしている。
もう1つ気にかかっているのは、クローズしたタスクや、長期的な取り組みの進捗はきちんと記録を残したいなということ。例えるならTaskChute。自分はこれを仕事で使っているのだが、完了したタスクは取り消し線を引いた上できちんと残るようになってて、今週どのPJに多くの時間を割いたのかとか、そういやあのタスクいつやったっけ、みたいなのを後から振り返れるようになっている。これをAny.doや、プレーンテキストによる長期タスク管理の中でも回したい。進んでいる感覚を持ちたい。暫定的には週次レビューでタスクの棚卸しをするとき、その週のやったことリストを残すことで運用している。自分が何かをやったんだ、前に進んでいるんだという実感を持つことはすごく大事。なんとなく頭の中がもやっとしてるときに、達成記録を読むとちょっとホッとするし、逆に最近取り組んでいない領域があると、ああやらなきゃなって思える。
んで、書評じゃなくて自分のタスク管理の話になってしまったが、毎日のライフサイクルをエンジニアリングするという思想は今後も重視していきたいなと思う。もうずっと迷いながら、やり方を模索しながらなのでときに嫌になったりはするのだけど、エンジニアリングすること自体が楽しくもあるし、それが「システム」である限りは常に保守運用は必要だよなとも思っている。とりあえず中心に置いている考え方は、「ルーチンを確実に回し、タスクを効率的に処理して、やりたいことをやる時間を増やす」こと。ここだけブレなければ大丈夫かなと思っている。</description>
    </item>
    
    <item>
      <title>はてなブログからの記事移行を完了した</title>
      <link>https://you.github.io/post/move-from-hatena-blog-to-octopress/</link>
      <pubDate>Sun, 01 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/move-from-hatena-blog-to-octopress/</guid>
      <description>旧ブログから全記事移行完了しました。ちょっとだけ疲れた。
もともとはてなブログで記事を書いてたわけですが、こちらは記事のエクスポート形式がMovableType形式にしか対応してないので、そこからMarkdownに変換し直したりだとか、はてなキーワードリンクを削除したりとか、地道にいろいろやらねばならず。たぶんスクリプトでガチャーンと一発でやれるんでしょうけど、力が足りないのでvimでファイル開いて変換ポイント確認してはポチポチコマンドで置換してやりました。
エクスポートファイルの分割 はてなブログからエクスポートすると、最初は全記事が1ファイルに連なった状態で吐かれます。自分はMT使った経験ないんでわかりませんが、これは「そういうもの」と考えていいんですかね。一方のoctopressはエントリーごとに分割されているので、まずはこのエクスポートファイルを1記事ずつ分割してやらなくてはならない。あとヘッダの形式も微妙に両者で異なるので、ここの変換も必要。
なんか変換ツール落ちてないかなと堕落した感じで探しまわってたら、一応ありました。
 名前がMT to Markdownなので完全にMarkdownへ変換してくれるものかと期待したのだけど、実際やってくれるのは先ほど挙げた「分割」と「ヘッダの書き換え」程度です。中身はHTMLタグのまま。一方で自分の環境だと&amp;lt;が&amp;amp;lt;に変わってしまうといった副作用もあり。あと元のヘッダにあったBASENAMEだとかCONVERT BREAKSといった文字列がそのまま本文内に残ってたり。。。ちょっと謎。ただ、変換が楽になるのは確か。
vimによる置換 スクリプト力弱いので、あとはvimによる力技です。今回初めて複数ファイルを一挙にvimで扱うってやったけど、便利ですね。
vimではバッファリストがあるのは知っていたけど、一方で引数リストっていうのもあって、:args hoge.txt fuga.txtとかでファイルリストを作れます。元々は名前の通り、vimを起動するときに与えた引数が入ってるリストらしいのだけど、任意に書き換えができるのであまり「引数」リストという感じはしない。バッファ内の全ファイルへのコマンド実行は:bufdoで出来るし、引数リストについても似た感じで:argdoが使える。このあたりを上手く活用すれば、今回のような複数ファイルを一挙に処理するのはたやすい。
例えば先の&amp;amp;lt;を&amp;lt;に全置換したり。他にも置換コマンドはh1タグを#に変換するだとか、いろいろな形で使いました。
:argdo %s/&amp;amp;lt;/&amp;lt;/g  CONVERT BREAKSと書かれた行を全部消したり。
:argdo g/CONVERT BREAKS/d  自分が今回初めて身に付けた知識でもっとも有効だったのは、検索でマッチした文字列を置換後の文字列内で指定する方法ですかね。検索パターンの括弧でくくった部分（vimなのでエスケープして\(と\)で囲った部分、ということになりますが）を、置換パターンから\1で指定できる。これははてなキーワードのリンクを一掃するのに役立ちました。
:argdo %s/&amp;lt;a class=&amp;quot;keyword&amp;quot;\(.\{-}\)&amp;lt;\/a&amp;gt;/\1/g  あー、あとこのコマンドで重要なのは{-}の部分ですかね。これで最短マッチになるらしい。他、細かなとこだと改行コードを入力するには&amp;lt;C-v&amp;gt;&amp;lt;C-m&amp;gt;と打つとか、&amp;lt;C-r&amp;gt;/で直前の検索パターンをコマンドラインに入力できるとか、いろいろこの機会に調べられてタメになりました。
参考  Vimで最短マッチと検索してから置換 - // Nice Catch! :) Vim で直前の検索パターンを部分的に再利用する - Xeebi 改行コード変換 vi で改行コード一括置換　-eightsee.net  ファイルのリネーム また先のコードで分割したMarkdownファイルは自動的にファイル名も振ってくれるんですけど、日本語の場合は漢字をなんとなく頑張って読んでローマ字に変換してくれた、気持ちはありがたいけど残念な感じのものになっちゃいます。なのでこれも一括変換。
ファイル名一括変換だとrenameコマンドってのがあるのですね。Linuxだと標準で入ってるけどUNIX系にはないだとか。なのでMacにも入ってなかったので、Homebrewでインストール。
$ brew install rename  使い方としてはrename &#39;s/hoge/fuga/&#39; *.markdown形式が使える、要はvimの置換コマンドっぽく書けるので、vimmerならこれが便利ではないかと。正規表現も当然使えます。ただし、vimで必要なあの面倒くさいエスケープの数々が要らなくなっていたり、先ほど書いた\1によるマッチ文字列の流用が$1に変わっていたり、若干の使用感の違いはあります。でも便利。
参考  Linuxでrenameコマンドを使おう - Qiita  そんなこんなで記事の変換が終わったら、あとはoctopressのレポジトリで/source/_postsの中に全ファイルブチ込んでrake generateかければ生成されます。最初はちょっと戸惑いもありましたが、やれてしまうと案外簡単に移行できるなという感じです。</description>
    </item>
    
    <item>
      <title>Developers Summit 2015に行ってきた</title>
      <link>https://you.github.io/post/developer-summit-2015/</link>
      <pubDate>Sat, 21 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/developer-summit-2015/</guid>
      <description>Developers Summit 2015の2日目に行ってきました。うちの会社とは少し毛色が違うイベントということもあり、これまで業務時間を利用して行くということはなかなかできなかったんですが、今年はたまたま休みが入ったので行けました。
マネジメント的な話から、かなり技術よりの話まで、幅広く聞くことができたのは面白かったです。一方でそれほどベンダーロックインされた話もなく、自分の業務に何かしらのフィードバックを持ち込みやすそうな構成になっているのだなという点もよくわかりました。人は多くてちょい疲れますけど。。あと会場の目黒雅叙園、IT系のイベントやってるとは思えないぐらい綺麗な場所ですね。。まぁ元は結婚式場などなどなわけだけど。渡風亭とか、屋内に茅葺きの一軒家があるもんでびっくりした。
以下、参加セッション別に簡潔に。
進化する！インテル RealSense テクノロジー インテルが売り出している、Kinectのような3Dカメラのデモを交えた紹介。990ドルで買えて、SDKキットは無料で落とせるらしいので、こういうのも試してみると面白いのかも。自宅での趣味開発ってこれまでコード書いてるだけに結局は終始してしまっていたのだけど、昨今の流行りに乗ってラズパイでなんかやってみたりっていうIoTっぽいのも楽しそうだし、Unityみたいな簡単に本格的なアプリケーションを作れるフレームワークもいいのかなと。RealSenseもUnityに対応しているみたいです。
関連  Engadget Fes：インテル「RealSense 3Dカメラ」をタッチ＆トライ。表情も認識 #egfes - Engadget Japanese  クラウドを活かす組織運営 ～クラウドガバナンス入門 すでにSlideShareが上がってますね。
  デブサミ2015 クラウドを活かす組織運営 ガバナンス入門  from Yuya Yoshida  様々なケーススタディを元にした、組織内でクラウドを導入する場合のガバナンスノウハウの紹介。クラウドの導入とは単なる技術的な変革にとどまらず、リソース調達のタイムスパンが飛躍的に短くなったり、管理手法としてスパイラルが適していたりと、ビジネスの枠組み自体が変化することを意識して、ガバナンスルールを変えなくてはならないという点が印象的でした。またスキルセットの話（属人化とか能力評価の方法）はクラウド運営にとどまらず、いろいろと応用できそうな話でした。 ## おさえておきたいモダンなチーム開発を支えるツール連携 こちらもSliedeShareあり。   おさえておきたいモダンなチーム開発を支えるツール連携【20-B-4】 #devsumi #devsumiB  from Tomoharu Nagasawa  システム企画にConfluence、BTSにJIRA、開発にGitHubといった形でそれぞれ個別のツールを使って管理しているところを、Atlassian社のツールを使って連携させていきましょうというような話。こういったツールとまったくもって無縁の仕事をしているので若干ぼんやり聞いてしまいましたけど、やっぱツールで仕事回せるのは間違いが少なくて良さそうだな、と。上がどうにもしてくれないなら、自分でなんとかせねばなぁ。 ## 実践！Infrastructure as a Codeの取り組みと改善 サイバード社の実例に基づいたInfrastructure as a Codeの現場レベルでのお話。正直言って途中から用語についていけなくなってしまったのだけど、オンプレミスでVMware上に仮想マシン立ててばかりいる自分とは隔世の感があることだけはよくわかりました。Chefはともかく、AWSには早いとこ業務で関わりを持たなくてはまずそう。AMI？ ChefとAWSの連携？ オートスケーリングの制御？ ## 情報革命時代における新しい多様性の共存と、これからのエンジニアのキャリア、評価制度について ビズリーチCTOの竹内氏によるお話。「多様性」とは内向型人間（イントロバート）と外向型人間（エクストロバート）のことであり、エンジニアには前者が多い一方で、営業には後者が多く、両者が相互理解していくこと、また近代資本主義下ではエクストロバート偏重文化があることを指摘した上で、イントロバートに対する配慮をどのように進めるべきかという内容でした。自分もイントロバートではあると思うので、結構興味深い内容。 外向型の振る舞い方を内向型に対して強要すること、またその逆はハラスメントに相当すると考え、双方の立場を理解して付き合っていかなくてはならないと。またイントロバートはビジネススキル、マネジメントスキルがなかなか理解できない側面もあるので、そういったスキルはきちんと理論的に分解して説き、「インストール」していくべきである。確かに自分も「なるべく社交的に振る舞わなくてはならない」という強迫観念のようなものがあるんですけど、一方で外向型の人が内向型に合わせてくれても良いはずなんですよね、多少は。 このあたりのお話はスーザン・ケイン氏の主張が下地になっているとのことなので、後で調べてみたいところ。  変革のSIer～挑戦者たち～ NRIと日立ソリューションズのSEによるパネディス形式でのセッション。ごめんなさい、本日寝不足につき正直半分寝てました。。
お話としては大きく二本立てで、保守的なSIerの内部で新しいものを導入していくにはどうすればいいのか？という話と、SIerが持つ技術をOSS化した事例の紹介。前者についてはリスク（悪い面の話だけではなく、将来予測が難しいことをリスクと呼ぶ）を可視化して検討を重ねていくことが大切であり、後者についてはオープン化の時代に対応していくために、OSSとして自社技術を公表することによる外部へのアピールの重要性と、またそれを実践するにあたっての困難が話の主軸でした。
大きなことをやる必要はない、世の中には解決すべき問題は多いので、小さなことからでも変えていくべきだという話がセッションの中でありましたが、自分としてはドラスティックな変革が必要だよなーという思いもあってちょっともにょもにょ。結局小さなことしかゲリラ戦を仕掛けていくしかないとしたら、なかなかに気の遠くなる話です。</description>
    </item>
    
    <item>
      <title>美術展検索サービスExhiBiを見切り発車しました</title>
      <link>https://you.github.io/post/2014-12-29-post/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-12-29-post/</guid>
      <description>ExhiBi
初めてウェブサービスをローンチしました。ネタとしてはすでにゴマンとある美術展の検索サービスです。既存のものがあんまりしっくりこなかったので、自分向けに対象を絞って、掲載情報も絞ったものを作ろうかと。ただし現時点だと絞りすぎてまだ出来てなくてMOTとMOMATしか範囲に入ってないッス。タイトル通り超見切り発車。なおExhiBiと書いてエキシビと読みます。
技術的な面では自分が「コードを書けないエンジニア（GUIによる設定ばかりに従事しているインフラエンジニア）」なので、何かしら書けるようになりたいよねということで、汎用性も高そうで文法的にもしっくりきたRubyを選びました（細々した理由は他にもいろいろとありますが……）。フレームワークには当然のごとくRails、中身のデータはMechanizeで美術館サイトから取ってきてます。あとはInfrastructure as Codeを意識したかったので、ChefとCapistranoを軸に構築してます。若干手作業が残ってしまったのが気掛かりなので、後々なんとかしたいところ。
しっかしRailsのデプロイ、めっちゃめちゃハマりポイント多かったんですがなんなんですかね……。開発自体は半月で終わったけどデプロイで1か月以上苦しんでました。自分が単に知見がないってだけの話なのか、元々そういうものなのか。オライリーもRailsのデプロイだけで1冊出してるぐらいなので、結構難しいものなのやも。
今後やりたいことは取りあえずこんな感じ。
 対象美術館を増やす 気になる展覧会をクリップしとく機能 気になる美術館を同上 レスポンシブ、とかデザイン面をもうちょっと コードきたねーので整える はてなブログのembed記法に対応させる（冒頭のリンクだとなんかダサい）  コードは一応GitHubに上げてますが、場当たり的に直しちゃったところもあるんで汚いです。ていうか仕事でコード書いたことないからもう少し作法とか身につけたい。
なんとか年内にあげようと思ってたんで、間に合ってホッとしました。技術的な話はおいおい少しずつブログにまとめていきます。</description>
    </item>
    
    <item>
      <title>『インフラ/ネットワークエンジニアのためのネットワーク技術&amp;amp;amp;設計入門』読了</title>
      <link>https://you.github.io/post/2014-12-01-post/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-12-01-post/</guid>
      <description>インフラ/ネットワークエンジニアのためのネットワーク技術&amp;設計入門posted with amazlet at 15.03.01みやた ひろし SBクリエイティブ 売り上げランキング: 8,231
Amazon.co.jpで詳細を見る 去年の末ぐらいに話題になってた本だと思うけど、読み終えた。よかった。
自分はネットワークスペシャリストを持っているんだけど、IPAの資格って基礎を満遍なくさらっておくには良いんだが、じゃあその知識の中で今現場で必要なのどれよ？ってのがなかなかわかりにくい面がある。この本はその「現場で実際どうネットワークは組まれるのか？」という部分に大きく焦点が当たっていて、知識を実践的なものに洗練できる。L3をコアスイッチにして構成していくのが主流ですみたいな話から、ケーブルにタグ付けてこういう情報書いておきましょうなんていう細かいことまで載っていて隙がない。
こういうことができる／できないという知識があったところで、じゃあそれがネットワークを組む上でどう活きるかというのはまた別の話であって、そこを弁えてないと車輪の再発明に手を煩わせてしまったり、組んだ構成が主流から外れてたりということはままある。自分がまさにそういう状態で、ベストプラクティスをきちんと押さえていないことがずっと気がかりだったので、今ネットワークを触る上でベターなやり方、注意点を体系的に学べたのはとても良かった。知識レベルとしては応用情報技術者があれば読めるレベルだと思う。1〜2年目ぐらいでネットワークに携わり始めたぐらいの社員が読むには本当に優れた1冊です。</description>
    </item>
    
    <item>
      <title>ブログ書けてない不安</title>
      <link>https://you.github.io/post/2014-11-10-post/</link>
      <pubDate>Mon, 10 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-11-10-post/</guid>
      <description>インプットに対してアウトプットの量が少ないなと前から思っている。ブログを書いてるとアウトプットの履歴が残るから「アウトプットしてない」時期が明確にわかり、それを見て不安になったりもするんだけど、結局書けてない、ということが続く。
何かコーディングするときや構築するとき、わからないことがあるとついついググって探して実装して、そのままにしてしまうことが多いんだけど、これだと一過性で終わってしまって、結局記憶に残らず終わったりする。せめてはてブしたりEvernoteにクリッピングしたり、ぐらいはするときもあるのだが、これはこれでクリップしたまま記憶の底に埋もれていくので、やっぱり身になった気がしない。きちんと知識の定着を図るなら、何度も繰り返し試してみるか、インプットした内容を頭の中で咀嚼して、その経緯を自分なりにアウトプットし直すのが一番いい。
なんで書けないのか？というと怠惰という理由が最も強いように思う。特に壁にぶつかったときは山のようにググって比較検討して答えを見つけようとしてしまうから、結局どのページを見て正答に至ったのかが曖昧になったりしやすい。まずは公式ドキュメントにあたるだとか、試行錯誤した結果でさえ一つ一つ丁寧にログを残すだとか、そういうことをできていない。
ちなみにメモツールとしてはアナログだとモレスキンのラージ、デジタルだとVimプラギンのQfixHowmを主に使っている。QfixHowmを使っているのは正確にはプライベーとのPC上だけで、要はDropboxにプレーンテキストでメモを書き溜めている形になる。アナログについては、先月までは常に1日1ページ手帳のEDITを携えてメモ代わりに使っていたんだけど、ページが足りなくなることも多かったし、そもそもスケジュール管理はGoogle Calendarに全部移行していたので、もう手帳を持つ意味もないなと思い、自由度の高いモレスキンに変えた。職場の関係でDropboxは使えないし、ノートPCを叩きながら打ち合わせに臨むのも好ましいとされていないので、アナログのメモはどうしても捨てられずにいる。あと思考を巡らせるときなんかには自由度の高いフォーマットの方が好ましくて、iPad miniにタッチペンとか使うよりも広めのノートの方が脳と手が直結しやすくて楽。
基本的にはこれらに「なんでも書く」方針でいるのだが、散漫な思いつきなどは書けていても、本を読んだり勉強しながら書く習慣がない。tmux開いてコーディングして、ウィンドウを切り替えてメモしてという手順を踏めばいいはずなんだけど、ついつい怠ってしまう。プログラマーの三大美徳に「怠惰」が含まれてはいるが、究極の怠惰は生産性を持たないものだと思う。
書くためにどうしたらいいのか？という問いに対する答えは自分でもまだ見出だせていない。こういうのはもう性格レベルの問題なので、「意識する」とかそういう類の対策じゃなんの意味もなくて、もっと根本的にメモせざるを得ない状況を生み出すしかないのだと思う。あるいはメモの効用をより強く自覚すること。怠惰とはいえ、本当に必要だと思っていることはそれなりにこなせてはいるので、メモを書いてないことで痛い目見るような経験がもっと増えれば変わるような気はする。</description>
    </item>
    
    <item>
      <title>Rakuten Technology Conference 2014に行ってきた</title>
      <link>https://you.github.io/post/2014-11-02-post/</link>
      <pubDate>Sun, 02 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-11-02-post/</guid>
      <description>イベント終了から1週間後にレポートとか遅すぎますね。これの前の週のオープンソースカンファレンスがせっかく土日開催だし行こうかなと思っていたんだけど、土曜に徹夜仕事があったんでさすがにしんどくて行けなくて、じゃあこっち行こうかな、と行ってみた具合。
中嶋聡氏→三木谷氏→OpenStackの話→Matz氏→Chefの中の人という大変ミーハーなタイムテーブルで回ったけど、著名なハッカーの話を聞く機会というのは何度か設けておくべきなんだろうと思う。当然ながらウェブや雑誌で何度も著名な人の話は読んでいるわけだけど、実際目の前で話してもらうと記憶への残り方とか、影響の受け方とかやっぱし違ってくる。
エンジニアとしての哲学 特にMatz氏の話を聞いていて思ったのが、エンジニアとして哲学というか思想というか、そういうのはきちんと持っておきたいなということ。
エンジニアというのは知的生産、クリエイティブワーカーだという人もいるが、意識によっては単なる流れ作業の駒になりかねない。実際、自分の今の仕事というのはそちらに近くて、顧客からの要件をいかに盛り込むか、いかに期日までにリリースして検収をもらうか、というところにばかり目がいってしまっていて、全体のデザインが出来ていないことはままある。短期的に売りを上げるだけであれば別にそれでいいのだが、思想のないシステムは運用上のトラブルが多いだとか、更改や拡張に対する思慮に欠けているだとか、長期的には利を失うことにつながりやすい。
キャリアデザインとしても、思想がないエンジニアはブレる。というか、単に売り上げて金回すサラリーマンエンジニアになりたいのか、それともいわゆるハッカーとしてやっていきたいかの境界線の一つがそこなんじゃないかなと。どちらが良し悪しではないし、どちらでも人生で「やっていく」上では困りはしないとは思うのだが、どちらが自分にとって楽しいかが問題だ。自分は結構収入面って後回しに考えていて、自分が思うような価値を生み出せる人間になれればいいと思うし、その対価として金が得られればそれでいいのかなと思う。
リクルーティングとしてのイベント いろいろと縁があって、ビジネス的なつながりがない企業の中では楽天がおそらく一番多く訪問してるんだけど、今回はだいぶ中の方まで入り込めて、ちょっとだけ実態が理解できたように思う。噂の楽天ランチは大変楽しみにしてたのだが、思ってたより薄めの味付けで社員の健康を考えている会社って素晴らしいなぁと感心してしまった。でもその後、カフェテリアで開かれる午後のセッションに行ったら無料でピザやらポテトやら唐揚げやらコーラやら配られてたんで、「あかん、これ罠や」と思いました。
あと周知の通り社内は英語公用語化していて、イベント自体も英語が基本なんだけど、実態としては会場内の案内は日本語遣うスタッフもいたし、本当に英語しか喋らない外人スタッフもいるし、日本語喋る外人スタッフもいるしでわりと脳味噌混乱した。参加者側も外国籍の人が半分ぐらいいたんじゃないかという感じ。
こういうあたり、単純にエンジニアイベントというよりは、楽天という会社の在り方をアピールする機会として本当にうまく活かしていたなと思う。夜のBeer Bashの裏でHiring Partyも開かれているのはちょっとびっくりしたけど。</description>
    </item>
    
    <item>
      <title>『実践Vim』でVimの思想を身につける</title>
      <link>https://you.github.io/post/2014-10-02-post/</link>
      <pubDate>Thu, 02 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-10-02-post/</guid>
      <description>実践Vim 思考のスピードで編集しよう!posted with amazlet at 15.03.01Drew Neil アスキー・メディアワークス 売り上げランキング: 15,673
Amazon.co.jpで詳細を見る 何ヶ月か前にKindleでアスキー本のセールをやっていたことがあって、そのときに買ったのをやっと読み終えた。技術書を電子書籍で読むというのは、感覚の問題なんだけどどうも身が入らない。ドッグイヤー付けたりとか書き込んでみたりとか出来なかったり、あるいは満員電車の中とかでも手軽に読めすぎてしまって、腰を据えて勉強する感覚がなくなるからなのかなぁという気がしている。なお、これまでKindle読むのにはXPERIA Z1fを使っていたが、さすがにしんどくてiPad miniを買った模様。もちろん、他にも理由はあったけど。
自分はVimを使い始めて1年ちょいというところで、これまでVim本をきちんと読んだことはなかったのだが、読んでよかったと思う。
Vimの情報はネットにありふれすぎていて、素のVimから触り始める人ってあんまりいないような気がする。自分も最初からKaoriya版を使っていたし、使い始めてまもなくGitHubやQiitaから他人のvimrcを拝借してきて、NeoBundleでプラギン入れて使っていた。特にプラギンまわりが楽しすぎて、ほうほうVimはこんなことができるのかー！とｗｋｔｋしながら1か月ぐらいはvimrc触りまくってたように思う。自分のGitHub見るとまーよくわかる。
でもこの本にも書いてある通り、まずは素の状態で試してみるべきなのだ、本来。Vimは最初からできることが豊富にある。テキストを扱う上で必要な操作がいくらでも揃っているので、まずはそれらを味わってみて、足りなければvimrcで味付けしていけばいい。そうじゃないとVimの設計思想というか、Vimによってテキストをどう扱うべきなのかという原則論が見えてこない。
本書はプラギンの話は皆無で、Vimが最初から備えている機能を中心に解説されている。.を始めとした繰り返し操作を多用する考え方だとか、テキスト対Vimというありがちな考え方だけではなく、ファイル対Vimという考え方もしなくてはならないとか、学べることは本当に多かった。あまりに分量が多いので一気にすべてのことを実践できるわけではないが、これはと思ったとこから取り入れていきたい。おそらくVimはツールではなくて思想なのだ。親指シフトやHHKBなんかと同じように、最速でテキストを編集するための思想。もちろんここでいう「Vim」には「Emacs」も当てはまってくるのだろうけど。あと正規表現ちゃんと覚えなあかんなと思った。後半の検索、置換のあたりは当然ながら正規表現を使える前提の話が多くて、きちんと使えてない自分には少ししんどかった。</description>
    </item>
    
    <item>
      <title>awesomeウィンドウマネージャーの見栄えを良くする</title>
      <link>https://you.github.io/post/2014-09-28-post/</link>
      <pubDate>Sun, 28 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-09-28-post/</guid>
      <description>Arch Linuxで使っているデスクトップマネージャ、awesomeのテーマに若干手を入れてみたので備忘録。あんまり日本語ドキュメントないので、このあたりのカスタマイズしんどかったです。
テーマファイルの構成 awesomeの設定はいくつかのLuaファイルを使って書き換えていく。基本的には全体設定を司る~/.config/awesome/rc.luaと、外観やテーマを司るライブラリである、Beautifulの設定ファイル~/.config/awesome/themes/default/theme.luaの2つを覚えておけばいいのかなと。いずれも初期状態では配置されてないので、デフォルトファイルをコピーしてきて使う。
# cp /etc/xdg/awesome/rc.lua ~/.config/awesome/rc.lua # cp -r /usr/share/awesome/themes/default ~/.config/awesome/themes/default  Beautifulについてはrc.luaの中で設定ファイルのパスが指定できるので、defaultという名前が嫌だったら任意で変えてもOK。あとは正直awesomeのWiki見るのが手っ取り早いとは思うのだが、設定したところだけ書いておく。ちなみに前回記事で書いたが、awesomeのデフォルトターミナルの設定もrc.luaを使うので、外観変える必要なくてもrc.luaだけは確実に要ると思う。
タグリストの書き換え awesomeでは仮想デスクトップを&amp;rdquo;tag&amp;rdquo;と呼んで扱っていて、デフォルトの状態だと左上に1から8まで（だったかな？）の数字がタグの番号として並んでいる。あまりわかりやすいものではないし、そんなに多くタグも使わないので書き換える。
tags = &amp;lt;span class=&amp;quot;synType&amp;quot;&amp;gt;{}&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;for&amp;lt;/span&amp;gt; s = &amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;1&amp;lt;/span&amp;gt;, screen.count() &amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;do&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synComment&amp;quot;&amp;gt;-- Each screen has its own tag table.&amp;lt;/span&amp;gt; tags[s] = awful.tag(&amp;lt;span class=&amp;quot;synType&amp;quot;&amp;gt;{&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;&amp;quot;Firefox&amp;quot;&amp;lt;/span&amp;gt;, &amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;&amp;quot;Terminal&amp;quot;&amp;lt;/span&amp;gt;, &amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;&amp;quot;Vim&amp;quot;&amp;lt;/span&amp;gt;, &amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;&amp;quot;other&amp;quot;&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synType&amp;quot;&amp;gt;}&amp;lt;/span&amp;gt;, s, layouts[&amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;1&amp;lt;/span&amp;gt;]) &amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;end&amp;lt;/span&amp;gt;  あとウィンドウを開いているタグは小さな正方形が表示されたりしていて鬱陶しかったのと、あまり見栄えも良くなかったので、正方形を表示されないようにした上で、フォーカスしているタグは文字色を変えることにした。これはbeautifulの方で設定する。
theme.taglist_fg_focus = &amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;&amp;quot;#f15c22&amp;quot;&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synComment&amp;quot;&amp;gt;-- theme.taglist_squares_sel = &amp;quot;/usr/share/awesome/themes/default/taglist/squarefw.png&amp;quot;&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synComment&amp;quot;&amp;gt;-- theme.</description>
    </item>
    
    <item>
      <title>Arch Linux &#43; awesome with Windows 8.1 in VAIO Pro</title>
      <link>https://you.github.io/post/2014-09-23-post/</link>
      <pubDate>Tue, 23 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-09-23-post/</guid>
      <description>先日VirtualBoxでArch Linux入れてみたところでしたが、引き続いてVAIO Proへのインストールが完了したのでまとめます。なお、Window8.1とのデュアルブートです。上のスクリーンショットが完成形。
事前準備 まず起動前の状態でASSISキーを押してBIOS設定を呼び出し、Secure Bootをdisabledにしておきます。Arch LinuxではSecure Bootをサポートしてないです。あとここではExternal MediaでのBootを許可したりとかしておくべきなんですけど、怠惰なんでASSISTメニューからUSBメモリでの起動を選ぶことでインストール作業を済ませてしまいました。Arch Wikiには「External MediaいじってUSBブートしろ」って書いてあるんで、そっちに従った方がいいと思います。
続いてWindows側での設定。
 コントロールパネル &amp;gt; 電源オプション &amp;gt; 電源ボタンの動作の選択 &amp;gt; 高速スタートアップを有効にする をオフ スタートボタン右クリック &amp;gt; ディスクの管理 &amp;gt; パーティションを縮小  既存パーティションを縮めることでLinuxインストール用の領域を作るのが通例ですけど、自分の場合これだ空き容量が小さすぎたので、回復パーティションを削除することにしました。そのために16GBのUSBメモリを買ってきて、回復ドライブを作ります。
VAIO Care  その他の機能  リカバリーメディアの作成 作成が無事に終わったら、その後一度シャットダウンしてASSISTキー押して起動。以下の手順でリカバリーパーティションが消せます。
トラブルシューティングを開始  Microsoft IME  トラブルシューティング  VAIOのリカバリー機能  Windows 8.1  ツール  リカバリーパーティションを削除 が、自分の場合これだとエラーが出て結局ダメだったんでdiskpartで削除しました。これもあまり褒められたやり方ではないと思うので推奨はしません。256GBモデルをケチらず買っときゃよかったなと初めて思った次第。
インストール インストール用のUSB指して再度ASSISTキーを押し、USBメディアから起動。そこからは前回のエントリーを参考に、通常通りインストール進めていきます。ポイントは以下の点かなと。
 ディスクパーティション、ESPはWindowsのものを使うので新たな切り出し不要。自分は/mnt/boot(200MB)と/mnt(残り全部)だけ切りました。 実際にインストールしていく段階ではネット接続が必要だが、wifi-menuでWi-Fi接続できる。意外に簡単。 インストール後の設定作業でもwifi-menuを使えるようにするため、pacstrapするときにdialogとwpa_supplicantも入れておく。  だいたいブートローダーの設定前までは特にハマるポイントはないはず。
ブートローダーの設定 grub-installまで完了すると、/boot内は次のような感じになりました。
/boot/efi/EFI/Microsoft/Boot/bootmgfw.efi /boot/efi/EFI/grub/grubx64.efi  ……efiがダブったパスになったのあれなんですけど、とりあえずここで言いたいのはもともとのWindowsのブートローダーとgrubがダブりますねという話で、勝手にgrubの方を上げてくれればいいんですけど、このPCの場合なのかWindows8の場合なのかわかりませんが、bootmgfw.efiの方がどうしても呼ばれてしまうので、パスを変える必要があります。
$ cd /boot/efi/EFI/Microsoft/Boot/ $ mv bootmgfw.</description>
    </item>
    
    <item>
      <title>RubyHiroba 2014に（少しだけ）行ってきた</title>
      <link>https://you.github.io/post/2014-09-22-post/</link>
      <pubDate>Mon, 22 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-09-22-post/</guid>
      <description>本当に少しだけというか2時間だけ居られました。LTほぼずっと聴いてて永和システムマネジメントさんの生活発表会を聴いて帰ってきました。
アンテナの広さとか インフラエンジニアである自分がRubyに興味持ったのは、最近インフラ界隈でもRuby使う機会多いなと思っていて。ChefやCapistranoがそうだし、あとVagrantfileもRubyの文法で書かれてるし。あと何かしら言語も身につけるべきだよなと思った時、汎用性が高そうだよなとか日本製だからというのもあって選んでます。でもRubyで何ができるんだろ？っていまいちちゃんと定まってなくて、最近はWebスクレイピングを少しやってたりしたんだけど。
LT聴いてると話題の幅が広くて驚く。何がやれるんだろ？なんて悩んでるのがそもそもおかしくて、コンピュータ上で情報を扱うのであれば言ってしまえば何でもいいのであって。きちんとアンテナ広げて何やりたいのかとか、何がやれるのかとか探って書いてかないと嘘だよなと。何が一番印象に残った？となるとRubicureなんだけど、手のつけやすいとこからとにかく書くって重要だと思う。他だとロジバンとか言語学かじってた人間としては興味をひかれた。
LTはきっかけ 「勉強会」って言われるとその場でいろんなことめちゃくちゃ吸収して文字通り「勉強するんだ」と意気込むような感じするけど、ぶっちゃけ5分のLTで得られる情報量は「勉強」って程のものではない。あれはこんな界隈があるんだとか、あんなことやってるエンジニアがいるんだって知るきっかけで、そこから自分で興味持って更に調べるなり、発表者に聞くなりするきっかけにすべきもんなんだなって今回遅まきながら気づいた。一方で発表者の側にとっても自分を知ってもらったり、マイナーな領域を広めたりするチャンスでもあって、コミケじゃないけどあまり主客を分けた意識になってしまうのは危険だと思った。流動的に自分も「主」の側に回れるようにならなきゃいけない。
みんなずっとコード書いてる。。。 常にみなさんMBAなりMBPなり開いてコード書くかプレゼン資料修正するかしてた気がする。そもそもうちの会社だと自分用ノートPC持ってるとセレブ扱いされる（商売道具なんだから値段云々言ってる場合じゃねーだろと個人的には思うんだが）んだけど何なんだよこの違いとか思った。とはいえ自分もノートPC持ってるけどコードはあんま書いてないので書かなきゃなと思う。MBAやっぱり羨ましいけど、VAIO ProにArch Linux入れたんで頑張る。入れた経緯は今度書く。
とにもかくにも自分、視野狭いなーと思いました。視野の狭さ故に自分がやれることとか、今後の働き方の選択肢も勝手に狭めている気がして。情報集めてコードに落としこむことをもっと意識したい。とりあえずるびまはちゃんと読もう。</description>
    </item>
    
    <item>
      <title>VirtualBoxでArch Linuxのインストール練習</title>
      <link>https://you.github.io/post/2014-09-07-post/</link>
      <pubDate>Sun, 07 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-09-07-post/</guid>
      <description>VAIO Proにcygwin入れたりして頑張っていたのだが、そろそろしんどくなってきたので、まともに開発に使える環境作るかーってことでデュアルブートを試みることに。デュアルじゃなくてOS丸ごと入れ替えたら？という話もありそうだが、仕事で使うのでWindowsを潰せない。かと言って仮想マシンだとこのPCのスペックでは心許ない。。ってことでデュアル。
ではディストリは何を使うか？だが、以前Ubuntuを使ったデュアルブートは構築した経験があるし、GUIでポチポチインストールするんじゃあんまり勉強にはならなさそうだなーということで、思い切ってArch Linuxを選んだ。ミニマルを是とするディストリだが、ミニマルどころか最初はsudoすら入っていないという徹底っぷり。使うものだけ入れろ！というポリシーは大変に共感できるものではあるが、それは当然難易度の高さと引き換えなわけで。。
さすがにぶっつけ本番は怖すぎるので、まずはiMac(OS X Marverics)上のVirtualboxでデスクトップ環境が立ち上がるとこまでやってみた。難易度が確かに高いOSではあるが、Wikiが非常に充実しているので、案外迷うこともないとは思う。逆に言えば、ここのInstallation GuideとBeginner&amp;rsquo;s Guideにきちんと目を通した上でやるべき。こんなブログ記事だけに頼らずに。
なお、えっらい長い記事になった模様。
事前チェック VAIO ProがUEFI環境なんで、今回の練習でもUEFIを用いることにした。Virtualboxの場合は仮想マシンの設定から「UEFI」を有効化できる。他にも初期設定ではインストールメディアをつないだり、外部メディアからの起動準備を優先させたりして、準備完了。
起動するといきなりシェルが現れる。ここから全部自力でコマンド打って入れてけというストイックさ。ひとまずはUEFIで起動しているか確認するため、UEFI変数を表示。
# mount -t efivarfs efivarfs /sys/firmware/efi/efivars # efivar -l  続いてキーボードマッピング変更。 jp106とかusとか適宜。やらなくてもインストールぐらいはなんとかなるかもしれんが、viでコロンの位置違ったりすると不便。地味に。
# loadkeys hoge  事前チェックはこれぐらい。
パーティション 続いてパーティション分割。まずはfdisk -lによりハードディスクのデバイス名を確認し、そのデバイス名に対してcgdiskコマンドを実行。今回はGPTを用いるのでcgdiskだが、MBRならcfdiskでよい。あるいはgdiskやfdiskもあるけど、やりづらいので自分はcを頭に付ける。
なおMBRとGPTの選択に関してはwiki内に記述がある。
ブートローダに GRUB Legacy を使う場合、MBR を使うべきです。 古い BIOS を使う Windows (32ビット、64ビット両方) とのデュアルブートをするなら、MBR を使うべきです。 BIOS の代わりに UEFI を使う64ビットの Windows とデュアルブートをするなら、GPT を使うべきです。 [https://wiki.archlinux.org/index.php/Partitioning_(%E6%97%A5%E6%9C%AC%E8%AA%9E)](https://wiki.archlinux.org/index.php/Partitioning_(%E6%97%A5%E6%9C%AC%E8%AA%9E)) 分け方はわりと適当。swapはまぁ、なくてもいいかなと。
 ESP(EFI System Partition)用に512MB
 タイプコードはef00
 FAT32でフォーマットする (https://wiki.archlinux.org/index.php/Unified_Extensible_FirmwareInterface(%E6%97%A5%E6%9C%AC%E8%AA%9E)))
 デュアルブートの場合はすでにWindows側で作られているので要らないはず。
 /bootに200MBぐらい</description>
    </item>
    
    <item>
      <title>Webスクレイピング手法は何かしら身に付けると便利そう</title>
      <link>https://you.github.io/post/2014-08-24-post/</link>
      <pubDate>Sun, 24 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-08-24-post/</guid>
      <description>最近Webスクレイピング勉強会が開かれましたが、自分これは「補欠」になってしまった上に別の予定が入ってしまって行けなくなり、なので自分なりにスクレイピングやってみました。人気の勉強会ってすぐ満席になりますよね。。ホントは勉強会情報とかすくれーぴんぐして逃さずゲトできるようになりたい。まぁこの分野は先人が作ったサービス等いろいろありますが、push型の配信してくれるものってまだない気がするのよね（あったっけ？）
Ruby好きなので使ったのはMechanizeです。nokogiriではない。ネットバンクから毎月残高拾ってきて記録してって手でやってたんだけど、いい加減自動化できないかと思いまして。なのでログイン処理とかまで任せるためにMechanize。
 で、使ってみた結果、これめちゃくちゃ便利というか、Web上に掲載されているあらゆるデータを抽出して使い回せるようになるので、やり方一つぐらい身に付けといて損はない気がしますね。
冒頭に挙げたWebスクレイピング勉強会の第2回でkimonoの中の人がしゃべっていたみたいですが、その中でも「セマンティックWebは失敗だった。だからkimonoが必要なんだ」という考え方にはわりと共感できるところがあります。Webってこれだけ情報に溢れたのに、そこから情報を抽出する手立てが限られているのは勿体ないですよね。
ただ、kimonoが最終解のままで良いとも思っていなくて、セマンティックWebは失敗したのかもしれないけど、それに代わるアーキテクチャは何かしら必要なんじゃないかと。Web標準への準拠という、フロントサイドの思想がだいぶ一般的になってきた今日、次に考えていくべきはアーキテクチャの標準化ではないかという気がします。</description>
    </item>
    
    <item>
      <title>何を勉強したらいいのかわからなくなってきた</title>
      <link>https://you.github.io/post/2014-08-10-post/</link>
      <pubDate>Sun, 10 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-08-10-post/</guid>
      <description>このブログ始めて1年経ったけど、まー振り返ってみるとだいぶ話題の幅が広いっつーかまとまりがないっつーか、俺はいったい何エンジニアなんだって感じの内容になりつつある。絶賛迷走中、何を勉強してるのかよくわからない、感じに。
ここまでは完全に興味関心に頼ってそのときやりたいことを取りあえずやってみる、という感じの勉強をしてきてしまった。いや、正確に言えばそこまで何も考えていないわけでもない。自分はインフラ、それもWindows方面にかなり偏ったエンジニアなので、Linuxを触る機会を増やすだとか、アプリ寄りの技術も触れてみるだとか、そういうことは意識してきたつもりだ。とはいえ基礎から積み上げるような学習ではないし、そうやって身につけてきたことが実際の仕事で役立ったためしも、今のところない。実際のところ仕事で学べるところはOJTで学べばいいやと思っている節があったりして、趣味の勉強の目的が迷子になりつつあるように思う。
プログラミングとか学ぶことは趣味にもなり得るのだが、一応この業界にいる以上、勉強の目的は「金を生めること」であるのが健全だろうと思っている。言い方はよろしくないかもしれないけど、付加価値のない技術なんざ単なる雑学に過ぎないわけで。なのに実業務につながるような勉強ができていないあたり、自分のやっていることは自己満足に過ぎなかったのかもしれんなぁなどと最近考える。もちろんまったくの無駄と言うわけではないけれど、それを学んで今後どうしたいの？ということをあまり考えていなかったような。
今業務でやっていることに対してモチベーションが上がらないのであれば、自分がやりたいことをやるにはどうしたらいいのか考えるべきだし、今の業務の延長線上で食べていく気があるのであれば、もっと業務に結びつくことを学ぶべきだ。要するに、エンジニアとしての生き方を自分は考えきれていない。なんとなく不安だからとか、なんとなく面白そうだからとか、そういう理由でつまみ食いをしてきてしまったなと思う。たぶん、これでは生きていけない。
で、じゃあ何をやろうか？と。スキルマップみたいなものを描かなくてはならないのではないか。計画的に狡猾に、自分の人生がどこに向かっていて、そのために何が必要なのかを考えなくてはならんのではないか。
もっと社外のエンジニアに会ったりしないと、選択肢は狭まる一方だよなーと思う。実のところ、この3か月ぐらいはそこそこいろんな人に会う機会を設けてみたのだけど、ユーザーへのデリバリを考えずひたすら作りたいものがあるって人もいれば、何よりもお客様のためだみたいなことを話す人もいるし、1人保守のチームに投げ込まれて気付いたらアプリもインフラもやれるようになりましたみたいな人とか、まーホントいろいろいる。社内のキャリアパスにしたってあんまりルートは多くないように見えていたけど、知らないところでだいぶ自由なことをやれている人もいた。結局、やりたいことはやったもん勝ちだし、案外やりたいことをやれるようになるまでのハードルは高くないのだと思う。だから、まずは自分がやりたいことを、もちょっと見定めてみようと思う。</description>
    </item>
    
    <item>
      <title>tmuxをなんとなく使ってたのできちんと使うようにしてみた</title>
      <link>https://you.github.io/post/2014-07-20-post/</link>
      <pubDate>Sun, 20 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-07-20-post/</guid>
      <description> tmux、なんとなく入れてなんとなく使ってたけど、改めてそれなりに形にしてみたのでメモ。
tmuxとは たーみなるまるちぷれくさー。ターミナル操作が便利になる系の、まぁ今更自分が説明する必要はないですね。せいぜいVagrantとVPSいじりながらローカルでもVim開くみたいなことしかやってないので宝の持ち腐れ感はハンパねーですが、とりあえず知っといた方がいいやってのとなんとなくカッコいいっていう理由だけで使ってる。
なお、インストール自体はだいぶ前に実行したのでやり方忘れた。たしかHomebrewで入れられたと思う。
.tmux.conf ルートディレクトリ上に生成される設定ファイル。tmuxに関する設定は基本的にここに書く。
 自分の設定ファイルを張ってみたけど、よく見られる設定の寄せ集めだとは思う。tmuxはなんかESCキーの反応が悪いらしく、その調整用の設定。ペイン分割したときに各ペインごとでマウススクロールができるようにする設定。文字コードの設定。色の設定。これは上手くやればもうちょっと見やすくなるんだろうなとは思ってるけど、面倒なので半端なとこで終えている感がある。カラーパレット見て色名探してみたいのめんどいよぉ。
あとキーバインド。デフォルトのプレフィックスがC-bなんだけど、使いづらいのでC-zに当て替えてる。他にやってるのは.tmux.confをすぐ読み込むための設定と、ペイン移動や分割のショートカット、あとはウィンドウやセッションのkill。
コピーモードについてはよくあるviライクなキーバインドが使えるってやつをそのまま使っている。コピーモードに入るときのデフォルトキーが[なのが気に食わんので、これもyに変更してある。でもよく考えたらvの方が相応しいなこれ。。。
powerline 残りはpowerlineに関しての設定。ステータスラインがカッコよくなります的なやつ。これも入れ方忘れたけど、公式のREADMEによればgit cloneするみたい。
これ、入れたはいいけど天気とか上手いことでなくてしばらく放置していた。今回改めて先のREADME読んでみたところ、GNU grepが要るみたいで、インストールしてgrepコマンドを置き換えたら上手くいった。やっぱりREADMEはちゃんと読むべき。自戒。他にもCPUとメモリ使用状況のグラフィカルな表示のためにはthewtex/tmux-mem-cpu-loadが必要だったりする。
設定は主に3ファイルを用いる。
 ~/.tmux.conf ~/tmux-powerline/themes/default.sh ~/.tmux-powerlinerc  まず.tmux.confにpowerlineを使うってことを宣言しなくては使えない。先の.tmux.confで言えば一番下の方の設定。上からステータスラインを表示する設定、更新インターバルの設定、UTF-8の使用設定、ウィンドウリストを左側に配置（これは好みによってcenterにする人もいるみたい）、ステータスラインの長さの設定。で、最後の2行でpowerline.shを読み込ませている。
実際にpowerlineに何を表示させるか、という設定は~/tmux-powerline/themes/default.shを変更して行う。
 デフォルトで表示できるものについてはすでに入っているので、これをコメントアウトしたりして好きなものを表示できるよう変える。各.shの後ろについている数字は表示部分の文字色と背景色。~/tmux-powerline/color_palette.shを実行すると、設定できる色と色名の一覧が表示できるので、これを使って設定する。まぁ、おこのみで。あと各表示部分の境界をカッコいい感じにするための設定が入ってたりするが、このへんはググれば出てくるので割愛（適当）。なお、default.shを直接触りたくない場合は、コピーして名前変えて使えばOK。
さらに、表示内容の細かな設定に.tmux-powerlinercを使う。このファイルは元々置かれていないので、./tmux-powerline/generate_rc.shを実行して生成する。よくいじるのは天気の設定で表示する地域を入れたり、時間や日付の表記を好きに変えたりってところかと。これについてはコメントでどういじればいいのか書いてあるので、見ればわかると思う。
tmux-powerlineについては自作のshellを使って好きな内容を表示させたりもできるようだけど、現状そこまでの情熱は傾けていない。
tmuxinator tmuxを立ち上げたあと、ペイン分割をいちいち手でやるのが手間なので、自動化しちゃいましょうというツール。単に分割するだけではなく、各ペインでコマンド実行させることもできるので、例えば最初からVim開いておいたりディレクトリ変えておいたりgit pullさせておいたりとか、いろいろできる。工夫次第。自分は今のところよく使っていたペイン分割を自動化させることしかしていない。以下のページが大変わかりやすかった。
tmuxのwindow, pane設定を一発で再現できるtmuxinatorが便利 | TACTOSH
以上、超適当かつ簡潔なまとめでした。まぁ、あまりにもこの分野は先駆者が多いので。。
参考
 tmuxを使い始めたので基本的な機能の使い方とかを整理してみた - 完熟トマト tmux-powerline - Qiita tmux-powerlineを使う - memo63 Ruby - tmuxinatorで一瞬で開発環境を起動する - Qiita  </description>
    </item>
    
    <item>
      <title>chroju.netを開設してみた</title>
      <link>https://you.github.io/post/2014-06-05-post/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-06-05-post/</guid>
      <description>前回の記事でchefでVPS構築したと書きましたが、そのサーバーに静的なHTML置いてドメインも取得してみました。
まだ何もないですけど、ここをハブにしていろいろやってみたい。
http://www.chroju.net/</description>
    </item>
    
    <item>
      <title>chefでさくらVPSの初期構築を全部自動化してみた</title>
      <link>https://you.github.io/post/2014-05-30-post/</link>
      <pubDate>Fri, 30 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-05-30-post/</guid>
      <description>Chefでサーバー構築を全部自動化して、借りているさくらVPSに当ててみた。意地でも手作業はまったく入れない完全自動構築設定。これはよい。毎回手作業やらずに済むというのは、抜け漏れをなくすという点でも、構築時間を短縮するという面でも本当に楽。
前提 今回構築するサーバーの前提は次の通り。
 CentOS 6.4環境で試行 vagrantを使って試験後、さくらVPSに対して適用 基本の設定として以下を実施
 構築用のユーザーを作成
 構築用ユーザーにsudo権限を付与
 構築用ユーザーを秘密鍵認証でsshログイン許可
 rootによるsshログインを禁止
 パスワード認証によるログインを禁止
 sshのポート番号を変更
 ssh, http以外のアクセスをiptablesでシャットアウト
 nginxを導入
 将来的にunicornを入れるための設定を準備
 極力サードパーティクックブックは使わない（ブラックボックス化が嫌）
  レシピ まず「どんなサーバーだろうとまず実行するだろうセキュリティ上の設定」はdefault_tasksというレシピにまとめた。
default_tasks # sshdサービスの有効化 service &amp;quot;sshd&amp;quot; do supports :status =&amp;gt; true, :restart =&amp;gt; true, :reload =&amp;gt; true action [ :enable, :start ] end # sshd_configの配置 template &amp;quot;sshd_config&amp;quot; do path &amp;quot;/etc/ssh/sshd_config&amp;quot; source &amp;quot;sshd_config.erb&amp;quot; owner &amp;quot;root&amp;quot; group &amp;quot;root&amp;quot; mode 0600 notifies :restart, &amp;quot;service[sshd]&amp;quot; end # iptablesの設定 iptables_rule &amp;quot;iptables&amp;quot;  sshdの有効化と、templateを使ったsshd_configの配置、そしてiptablesの設定。このうちiptablesについてはサードパーティ・クックブックを使っている。</description>
    </item>
    
    <item>
      <title>Chef soloはじめの一歩</title>
      <link>https://you.github.io/post/2014-05-06-02-post/</link>
      <pubDate>Tue, 06 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-05-06-02-post/</guid>
      <description>Capistranoでいろいろやったので今度はChef、正確にはChef solo。サーバー立てて最初にやること（ユーザー作成、sshd_configの設定、もろもろインストールとか）はだいたい決まっていると思うので、Chefで全部コード化してしまえたらやっぱり楽だなぁという思い。なおChefとCapistranoの境界線については、システム全体をまかなうのがChefであり、Capistranoはあくまでアプリ単位のデプロイに使うイメージでいる。rbenvやRubyを入れるのはChef。/var/www/RailsApp/配下（仮）をごにょごにょするのがCapistrano。
参考としたのはお馴染みのアレです。ただ、すでに出版から1年近く経っているので若干事情が変わっている箇所もあったりした。
入門Chef Solo - Infrastructure as Codeposted with amazlet at 15.03.01伊藤直也 (2013-03-11)
売り上げランキング: 2,821
Amazon.co.jpで詳細を見る 基本構成 chefの基本構成を取りあえず押さえる。
 chef: インフラ自動化のフレームワーク chef solo: 本来クラサバ構成で扱うChefをスタンドアロンで使えるようにしたもの knife-solo: リモートからchef soloの実行に必要なツール Berkshelf: サードパーティクックブックの管理に使う  インストール 上記のものを全部インストール。Gemfile書いてる場合はそっちに追加で。また最後にrbenv rehashを忘れずに。
gem install chef -v 11.10 --no-ri --no-rdoc gem install knife-solo --no-ri --no-rdoc gem install berkshelf rbenv rehash  ちなみにChefの最新はv11.12.xなんだけど、どうもバグがあるっぽく上手くいかなかったのでバージョン指定で古いの入れてる。
 参考：Chef 11.12.2のknife configureが失敗する - Qiita  セットアップ 初期設定。
knife configure # 対話は全部デフォでOK knife solo init chef-repo # chef-repoフォルダがレポジトリとして作られる cd chef-repo  chef soloを使うリモート側へもchefをインストール。</description>
    </item>
    
    <item>
      <title>自分のことを「ペルソナ」から見返してみる</title>
      <link>https://you.github.io/post/2014-05-06-post/</link>
      <pubDate>Tue, 06 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-05-06-post/</guid>
      <description>自分の状況を客観的に評価する（タスク、財務管理、スケジュール管理など）とき、「ペルソナ」を考え方として導入するとわりと捗ることに最近気付いた。
このご時世、人格が一面的という方はあまりいないのではないかと思う。要は家庭での顔、仕事での顔、趣味での顔とかそういうのだ。これを「ペルソナ」と呼んだのだが、自分にはどんなペルソナがあるのか把握した上で、各ペルソナごとに状況を振り返ってみると漏れが少なくなる。
使える場面としては、例えばGTDの週次レビュー。ToDoリストに漏れがないか考えるとき、各ペルソナごとに数分ずつ考えてみる。あるいは「高度」の概念を取り入れるとき、ペルソナごとの高度の視点を持ってみる。
あるいは家計簿。どのペルソナに対してどれだけお金を使っているかで出費のバランスを管理する。時間の使い方を振り返るときも、案外趣味の時間がすげー多いなとか気付けたりして有意義。
自分の場合は今のところ、生活、仕事、趣味、開発、家庭・友人、教養と6つのペルソナに分けてみている。なかなかMECEというわけにはいかないのだが、「生活」は自分自身の身の回りのこと（病気や健康、衣食住）であるのに対して「家庭・友人」は実家の事情やプライベートな友人関係の話という具合に分けていたり、仕事上必要な技術知識は「仕事」に振り分け、家で勉強している知識は「開発」に入れたりとか、そういうことをしている。
まぁ要するに、自分なりに状況整理できる手がかりがあれば良いのではないか、という話。</description>
    </item>
    
    <item>
      <title>エンジニアの転職において「技術」とは目的か手段か</title>
      <link>https://you.github.io/post/2014-05-01-post/</link>
      <pubDate>Thu, 01 May 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-05-01-post/</guid>
      <description>ヒカリエ綺麗すぎる。。。
えー。こちらのイベントに参加してきました。こういう社外での勉強会チックなものに参加するって初めてですげービビってたけどわりと楽しめた。
登壇者の方はSIerを経てWeb系に行かれたということでしたけど、その理由が「コードを書けないことへの危機感」だとか「3.11を目の当たりにして、やりたいことやっとかないといつ死ぬかわからん」みたいな話だったのはだいぶ自分とも似ている点でした。というかSIerを脱出して別のIT系に転職する人って2パターンあって、1つがこのように「より技術的なスキルアップをしたい」という人、もう1つが「ユーザー企業の側に行ってみたい」という人。少なくとも自分の周囲を見回しているとそういう感じで、比較的安定的でよく狙われてるのは後者なのかなという印象を持っています。ただ、最近のトレンドとして手に職をつけるじゃないですけど、自分で出来ることの幅を増やしたいという人は多い気もしている。イベント後の懇親会で話していてもそんな感じでした。
自分としても技術的な危機感というのはあって、だからこそのこのブログなのだけど、最近より強く感じているのは「長期的な目的なしに転職したくない」ということ。
突然の話になりますが、自分は今のインターネットを巡るこの国の状況って実にダサいなと思ってます。光回線はだいぶ普及したし、ポケットに入るコンピュータを誰もが持ち歩いているし、Wi-Fiサービスだってそこここにある。確かにインフラ面では多大なる普及を終えているんだけど、じゃあそれを使ってみんな何をしてるの？というと、今ってソシャゲで金銭巻き上げられたりSNS疲れが社会問題化したり、あるいは歩きスマホしてて階段から落ちて死ぬみたいな感じじゃないかと。なんてーか、まだ十分に社会が、ソフトウェアの方がインフラの状況に追いついていないように思うのですよ。ネットを通じて社会を良くしよう！みたいな話がよくあるけど、その前にネット自体を何とかしないとヤバくね？ってのが自分の実感です。
だからキャリアプランとしては、そういう方面に進んでいきたい。ITと社会がうまく調和していくような、そういう動きを促せるような働きかけをしていきたいという思いが強くある。となると、自分の手で技術的なこと出来るか？ってのは二の次になるんですよね。手段と目的の違いです。もちろん、今挙げた「目的」を達するために技術力という「手段」を使えれば自分にとって最も望む結果だと思うし、その方向ってのは目指したいと思ってる。ただ、じゃあ実際どういう職種がその方向の先にあるんだろうってのがあまり具体化できてなくて。考えたのはUIデザインとかは少し近いなと思うんですけど、あまりしっくりはきていない。むしろ法整備したりだとか、あるいはMIAUのやっているようなことが自分の目的には近いのだけど、そうなると技術者じゃなくて良くなっちゃうなぁってのもあって悩ましい。
世の「技術やりたい！」というエンジニアのみなさんは、もうそれ自体が目的化しているのか、それとも技術を通して実現したい目的が別にあるのか、というのは大変に気になるところです。今日聞けばよかったかな。まあ、もうちょっといろいろ考えてみます。</description>
    </item>
    
    <item>
      <title>Capistrano3を最後にもう一度だけ懇切丁寧にまとめてみる</title>
      <link>https://you.github.io/post/2014-04-12-post/</link>
      <pubDate>Sat, 12 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-04-12-post/</guid>
      <description>いろいろエントリーを上げながら苦しんでいたCapistranoだが、ようやっとそこそこ落ち着いてきた気がするのでそろそろ完結編といく。Capistranoの基本とかはすでにこちらのエントリーで書いたので、今回は各設定ファイルの書き方とか、その他ハマったポイントを中心に。
今回作成したファイル 以下4ファイルを作成した。
 Capfile config/deploy.rb config/deploy/staging.rb lib/capistrano/tasks/unicorn.cap  基本的にCapistranoを使う場合「必須」なのは上2つのファイル。deploy/hoge.rbも確実に必要にはなるが、デプロイ先の環境が本番なのかステージングなのかでproduction.rbとstaging.rbを使い分けることになる。もちろん、ステージング環境を用意していない場合はstaging.rbは不要。また4つ目のファイルだが、自分の場合はunicornをデプロイ先で使っているので、デプロイ後にunicornを再起動する目的で独自タスクを作成している。説明が遅れたが、今回デプロイ先となる環境は大雑把に以下の通り。
 Vagrant 1.4.3 CentOS 6.4 Postgresql 9.1 Ruby 1.9.3(rbenv) Nginx + unicorn 位置付けはステージング環境  Capfile  ここはだいたい環境を問わず同じになってくる箇所。まずcapistrano/setupとcapistrano/deployはrequireが必須。RailsをCapistranoで扱う場合は、capistrano/bundlerからcapistrano/rails/assetsまでも必須となる。rbenvを使っている場合はcapistrano/rbenvが必要。rbenvをどこにインストールしたのかにより、set :rbenv_typeを指定する。あとはデプロイするアプリケーションで使うRubyのバージョンも指定してやる。rvmを使う場合も似たようなcapistrano/rvmを使うみたいだが、そちらはよくわからんので割愛。
で、ここまでがrequireということなので、デプロイ元サーバーにGemfile書いてインストールしておくことを忘れずに。
group :development do gem &#39;capistrano&#39;, &#39;~&amp;gt; 3.1.0&#39;, :require =&amp;gt; false gem &#39;capistrano-rails&#39;, &#39;~&amp;gt; 1.0.0&#39;, :require =&amp;gt; false gem &#39;capistrano-rbenv&#39;, &#39;~&amp;gt; 2.0&#39;, :require =&amp;gt; false gem &#39;capistrano-bundler&#39;, &#39;~&amp;gt; 1.1.2&#39;, :require =&amp;gt; false end  最終行は独自カスタムタスクを読み込むための行。デフォルトで入っているのでそのままにしておけばよい。先のunicorn.capのように、何か独自タスクを作った場合はこのパスに入れれば読み込むよーということでもある。
なお、以前書いたエントリーではcapistrano3/unicornもrequireしていたのだが、これは撤回した。詳細はunicorn.capについて説明するときに後述。
staging.rb  ステージング環境、というかデプロイ先環境の設定を書く。vagrantを使う場合はだいたいこれと同じように書いておけば通るんじゃないかと思う。Vagrantはデフォルトではvagrantユーザーによるsshが可能になっているので、それをそのまま使わせてもらっている。何か他のユーザーで入りたい場合などは頑張るしかない。
一点だけ注意すべきは、RAILS_ENVがstagingに設定されるということ。Capistrano3ではcap installを叩くとデフォルトでstaging.rbとproduction.rbが作られ、RAILS_ENVもこのファイル名に倣うことになるのだが、Railsの動作環境は通常test, development, productionの3種類であり、stagingは存在していない。従ってこのままデプロイを始めてもうまくはいかない。面倒であればstaging.</description>
    </item>
    
    <item>
      <title>Capistrano3がわからんので今一度イチから考えなおしてみる</title>
      <link>https://you.github.io/post/2014-04-06-post/</link>
      <pubDate>Sun, 06 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-04-06-post/</guid>
      <description>前回Capistranoが上手くいかないというエントリーを上げてから1か月。いまだにハマってしまっている……。何が悪いの皆目検討もつかない、というほどではないのだが、なんというか、雲を掴んでいるような状態ではある。一旦Capistranoについて整理してみるべきなんだろう。
Capistranoは何をしてくれるのか そもそもCapistranoとは何をしてくれるツールなのか？
Capistrano3のデプロイフレームワークの使い方 - Qiita
Capistranoは2まではRailsのデプロイツールだったけど、3は汎用的なデプロイツールに変わっている。したがってデフォルトの状態ではRails用のデプロイタスクは特に含まれていない。このあたりが上の方のリンク先で語られている内容になる。
ではデフォルトでのデプロイタスクは何をするのかと言えば、およそサーバーへのデプロイとして一般的に行われるようなものが組み込まれている。capistrano/lib/capistrano/tasksあたりを探るとデフォルトタスクがよくわかる。
 releases、sharedなどディレクトリの作成 レポジトリからgit clone linked_files、linked_dirsの存在確認とシンボリックリンク作成 currentディレクトリへのシンボリックリンク作成 最古世代の削除、クリーンナップ  ざっくり見てしまうと、単純にサーバーへファイルを上げて自動的に世代管理をさせるだけであればこれだけでもなんとかなりそうなところではある。逆にこれらが煩わしいのであれば、デフォルトタスクを無効化して使う手もある。デフォルトタスクの内容がよくわからないのであれば、一旦無効化してすべて自分でタスクを書くというのも手だと思う。
Railsをデプロイする場合はこれだけでは足りないので、capistrano/railsやcapistrano/bundleを追加で読み込むことになる。
Capistranoのディレクトリ構成 Capistranoでデフォルトのデプロイを行った時、デプロイ先サーバーに作成されるディレクトリについてもよくわかってないのでまとめておく。deploy.rbのdeploy_toで指定したディレクトリ内に、次の3つのディレクトリが作られる。
 releases : デプロイした内容を世代管理する shared : bundleとかdatabase.ymlとかGitで管理してないファイル置き場（多分） current : releasesの最新世代とsharedのシンボリックリンクが置かれる  基本的に「デプロイ」される先はreleasesだ。releases配下にはデプロイ時のタイムスタンプから生成された名前のフォルダが作られ、その中にごっそりデプロイしたファイルが入っている。タイムスタンプが付くということは当然世代管理されているわけだが、保持される世代数はdeploy.rbのset :keep_releasesで好きに指定できる。で、最新世代についてはcurrent配下にシンボリックリンクが自動的に貼られる。
sharedは正直よくわからない。デプロイのたびに更新するわけではない、すなわちGitで管理をしていないファイルやディレクトリを置いておくみたいなのだが、ではどうやって配置すれば良いのか？がわからない。配置したファイルのうち、必要なものについてはlinked_dirsとlinked_filesで指定すればcurrent配下にこれもまたシンボリックリンクが貼られる。しかし、どうやって配置すれば良いのかわからない。
関連：[自分用メモ]Capistrano3のlinked_filesって自分でuploadしなきゃダメですか？ - Qiita
で、これを書いていて気付いたのだが、アプリの中身がcurrent配下に展開されるということは、ウェブサーバーの設定もdeploy_toではなくてcurrentに飛ばすようにしておかないといけないわけだ。俺だけかも知れんが、案外罠ではないかという気がする。
capistrano/rails 取りあえずsharedの話はほっといて、Railsをデプロイする話に戻る。冒頭でデフォルトタスクにはRailsに対するものは何もないと書いたが、それではRails用のデプロイタスクはどこで生成されているのか？ 答えから書けば、capistrano-railsを読み込むことによって初めて生成される。
Capistrano3におけるRailsのデプロイタスクの内部実装 - Qiita いわく、追加されるタスクは主に3つ。
 assets compile(deploy:compile_assets) assetsのタイムスタンプ更新(deploy:normalize_assets) db:migrate(deploy:migrate)  逆に言えば、ここで追加される以外のタスクはデフォルトでは行われないということになる。その点を配慮せずにただググって適当なdeploy.rbをコピーしたりしただけでは、思ったとおりのデプロイは出来ない。
一例として自分がハマったのが、db:createは行われないということ。schema.rbを使って初回デプロイのときに上手いこと云々なんてことは一切してくれないので、このあたりは自分で書くか、サーバーを作ったときに予めDBをこしらえておく必要がある。
capistrano/bundle もうひとつ、RailsをCapistranoでデプロイする場合に必要なのがcapistrano/bundler。Rails使っててGemfileがないなんてことはないだろうし、というかcapistrano/railsの中で明示的にrequireされている。何をやっているのかと言えば、簡単な話しである。
 before &#39;deploy:updated&#39;, &#39;bundler:install&#39;  デフォルトではshared配下にbundleディレクトリを作ってbundle installをかけてくれる。パスについてはset :bundle_pathオプションで自由に指定ができる。
capistrano/rbenv あとよく使われるものとして、capistrano/rbenvがある。デプロイ先のサーバーで、Rubyをrbenvを使って入れてる場合には必須になるもの。中身はよく知らないが、そこまで気にしなくても良いのではないかという感覚がある。入れたらdeploy.rbにset :rbenv_ruby_versionで使うRubyのバージョンを指定すればOK。
同じ類のものとして、capistrano/rvmももちろんあります。
ソースにあたることの必要性 以上がCapistrano3でRailsをデプロイする場合の主なデフォルトタスクの内容になる。繰り返しになるが、これ以外に何をやりたいのならタスクは書く必要がある。よく言われるのがデプロイ後のウェブサーバーの再起動で、unicornを再起動するタスクを追加している例はよく見かける。ただ、これもcapistrano3-unicornという便利なものがすでに作られていたりはする。</description>
    </item>
    
    <item>
      <title>Rails環境構築（5）Capistranoによるデプロイ ※未完</title>
      <link>https://you.github.io/post/2014-02-20-post/</link>
      <pubDate>Thu, 20 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-02-20-post/</guid>
      <description>だいぶ間が空いてしまった……。Rails環境構築シリーズ、あとサーバーにはDB（postgresql）、unicorn、Railsを入れれば終わりなのだが、いずれもアプリ側のGemfileを使ってローカルにインストールするつもりなので、まずはアプリのデプロイが必要となる。そしてRailsのデプロイといえばCapistranoだ！と、いきたいところだったのだが。
Capistrano 3.0を試しているのだが、どうにもハマっている。ハマっているというのはのめり込んで楽しくなっちゃった的な意味ではなく、上手くいかずにっちもさっちもいかない的な意味である。とりあえず頭からやり方おさらいしつつ、ハマった箇所をまとめてみたい。
3.0についてはまだ登場してから時間も経ってないためか、日本語で有益な記事は少ない。が、まずは本家をよく読むこと推奨。一応これに則れば出来るようになっているはずではある。他に個人の方がまとめたハウツーで参考になったのは以下の記事。すでに2.0を使っている人ならよりわかりやすいのだろうが、これがCapistrano初体験となる自分にはちょっとわかりにくかったりも、した。特に3番目の記事は完全に2.0からの移行組に向けて書かれた内容。
 capistrano 3.x系を使ってrailsをデプロイ | iii ThreeTreesLight 入門 Capistrano 3 ~ 全ての手作業を生まれる前に消し去りたい | GREE Engineers&amp;rsquo; Blog Capistrano 3への手引き - 今日のごはんは素麺です capistranoでステージングとか本番環境とか使い分ける - リア充爆発日記 Capistrano3 で Vagrant で構築したVMにデプロイする - kakakakakku blog  Capistranoとは？ そもそも論。開発完了したアプリをサーバーにデプロイするときの手順をRubyで書いて自動化しておくためのもの。デプロイ先はステージング環境とか本番環境とか何種類か存在する場合もあるが、環境ごとにデプロイ手順は別々に設定したりできる。なお、Rails用のツールだと思われがちだが、3.0からRails以外でも使える汎用的なツールになったらしい。自分の場合はRailsで使っているのであしからず。あと、SCMはGitを使っている必要がある。
インストール インストールはGemで行う。デプロイ予定のRailsアプリで、Gemfileに以下追記してbundle install。
group :development do gem &#39;capistrano&#39;, &#39;~&amp;gt; 3.1.0&#39; gem &#39;capistrano-rails&#39;, &#39;~&amp;gt; 1.0.0&#39; gem &#39;capistrano-bundler&#39;, &#39;~&amp;gt; 1.1.2&#39; end  capistranoが本体。先の本家によれば、Railsで使う場合はcapistrano-railsも必要になるとのこと。他にもいろいろ便利なプラギンがあったりするっぽいけど、わかんないので今はここまで。
初期作業 $ bundle exec cap install  初期ファイルの生成。手元の本にはcapify .コマンドだと載っていたのだが、capistrano 3.0から変わったらしい。ほあ。これによりCapfileと、config配下にdeploy.rb、さらにconfig/deploy配下にproduction.rbとstaging.rbが作成される。それぞれの役割は次の通り。
 Capfile : 他のGem（capistrano-railsとか）の読み込みなどを記述 config/deploy.</description>
    </item>
    
    <item>
      <title>突然だけどVimperatorrcを晒してみる</title>
      <link>https://you.github.io/post/2014-02-09-post/</link>
      <pubDate>Sun, 09 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-02-09-post/</guid>
      <description>非常に唐突ではあるけどvimperatorrcを晒してみようと思う。2014年の今日、あんまりvimperator関連の日本語記事って見かけない気がしていて、少しでもそのへん貢献できればなぁという思い。
vimperator自体は何年か前から使っていたのだが、カスタマイズはあまりせずにデフォルトの状態で使っていた。が、操作によってはマウス使った方が便利だったり、ちょっと中途半端な状態にあったので、一念発起してちょこちょこいじってみた。まだいじれる余地はありそうなんだけど。
&amp;quot; vim: set filetype=vim :  Vimで編集するとき、vimperatorrc用のハイライトがないので、似たところでfiletypeをvimとして認識させてます。
&amp;quot;============================ &amp;quot; General &amp;quot;============================ &amp;quot; 入力欄に自動フォーカスしない set focuscontent &amp;quot; ビジュアルベルを表示しない set visualbell highlight Bell display:none &amp;quot; :oなどでの補完候補をサーチエンジン、履歴に限定 set complete=sl &amp;quot; Hintモードでアルファベット使用 set hintchars=jfnvurhgytbkdmcielsxoe &amp;quot; ツールバーはアドオンとタブのみ表示 set gui=addons,nobookmarks,nomenu,nonavigation,tas &amp;quot; commandモードでIMEオフ style! -name=commandline-ime chrome://* #liberator-commandline-command input {ime-mode: inactive;}  ここまでは基本、一般的な設定しかないかと。set complete=slは地味に効く。で、なぜか現状set hintcharsが動いてないれす。。。何がいけないのかわからん。
&amp;quot;============================ &amp;quot; Key mapping &amp;quot;============================ &amp;quot; google検索を手早くする noremap s :open&amp;lt;Space&amp;gt;google&amp;lt;Space&amp;gt; noremap ,s :tabopen&amp;lt;Space&amp;gt;google&amp;lt;Space&amp;gt;  ここからキーマッピング。:openで検索もURL開くのも両方いけるのはいけるんだが、vimperatorでの自動判別がうまいこといかない（例えば「twitter おすすめ」とかで:openすると、Twitter検索で「おすすめ」を探した結果が表示されるとか）ときがあるので、明示的に:open googleを一発で開けるようにしてます。
&amp;quot; OS分岐 &amp;quot; dをブラウザ標準の「タブを閉じる」にマッピング &amp;quot; vimperatorrcのリロード js&amp;lt;&amp;lt;EOM if(liberator.</description>
    </item>
    
    <item>
      <title>$ vagrant sshで仮想マシンに入れなくなった場合の対処法</title>
      <link>https://you.github.io/post/2014-02-02-02-post/</link>
      <pubDate>Sun, 02 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-02-02-02-post/</guid>
      <description> やらかしました。Rails環境構築シリーズ、次回でセキュリティ周りの設定をまとめようと思い、普段の要領でiptablesをいじったりsshのポート番号変えたりしてたんだけど、なんか設定しくじったっぽくてvagrant sshできなくなった。やべえ、仮想マシンだしこれもう詰んだんじゃねと思ったんだけど、なんとか元に戻せたので対処法まとめとく。
といっても簡単な話です。
 Virtual BoxをGUIで開く 該当仮想マシンがリストに入ってるので強制終了し、Virtual Box上で起動し直す rootでログイン（パスワードはデフォルトでvagrant） おかしい箇所を直す  コマンド叩いて操作してるので忘れがちだけど、vagrantはVirtual Box上（有料版ならVMware上も有り得るが）に仮想マシンを立ててるので、普通にGUIから入ればいい話です。rootで入ってぐいぐいヤバそうなとこ直して、直したらターミナルからssh試してみてってのを交互に繰り返して直しませう。
rootで入り直す前に一度VM落とす手順を入れましたが、こうしないとGUIから入れないみたいです。僕の場合だとターミナルでiptablesとかいじって、よしじゃあsshログインできるか試してみようってことで一旦ログアウトしたら入れなくなった（アホだ）のでVMが立ち上がりっぱなしになっちゃったわけなんだけど、この状態で取りあえずVirtual Box開いてみたらコンソールを表示するボタンがグレーアウトされてました。なので一旦強制終了するしかないです。
余談 ちなみに今回何をやらかしたのかだけど、ポイントとしてはsshのポート番号を変えた（/.ssh/configをいじった）のと、iptablesをいじったのの二点。これを普通のVMの感覚でやっちゃったのがマズかったみたい。
まず、vagrant sshで使われているポート番号はデフォルトで22ではなく、2222なのです。これはホスト側で$ vagrant ssh-configコマンドを打ってみるとわかる。
Host default Hostname 127.0.0.1 User vagrant Port 2222  一方、ssh以外の各vagrantコマンドについても、いずれもsshを使っている模様。vagrant upもvagrant haltもssh経由で命令しているみたいです。ただ、こちらは通常通りPort 22を使っているのでややこしい。このあたりの事情を踏まえた上でポート番号あれこれしないと、必要なはずのポート塞いじゃってうああああああってなっちゃうっぽい。
まぁvagrantで作ったサーバーを表に公開することはあんまりないと思うんで、このへん触らないのが吉かも。練習でやりたいのであれば、vagrant以外でサーバー用意してやった方が良さそうな気がします。
参考
 Vagrant で SSH の接続ポート番号を変えると、けっこう複雑になるという話 | girigiribauer.com Vagrant のベースBOX作成手順 (Scientific Linux 6.1) - エンジニアきまぐれTips  </description>
    </item>
    
    <item>
      <title>Rails環境構築（4）nginx導入</title>
      <link>https://you.github.io/post/2014-02-02-post/</link>
      <pubDate>Sun, 02 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-02-02-post/</guid>
      <description>nginxの導入はyumを使ってサクッと。まずリポジトリを登録して、その後yumを使ってインストール。
$ sudo rpm -ivh http://nginx.org/packages/centos/6/noarch/RPMS/nginx-release-centos-6-0.el6.ngx.noarch.rpm  リポジトリが正しく登録されているか確認。
$ yum list nginx --disablerepo=* --enablerepo=nginx （中略） Installed Packages nginx.x86_64 1.4.4-1.e16.ngx @nginx  yum install
$ sudo yum install nginx  インストールは以上。EPEL様々ですね。あとは仮想ホスト設定用にディレクトリを作っておくという作業を入れる。
$ sudo mkdir /etc/nginx/sites-available $ sudo mkdir /etc/nginx/sites-enabled $ sudo rm -f conf.d/*.conf  nginx.confを開いて、sites-enabledを読み込ませるための設定を追加する。具体的には以下の1行。
include /etc/nginx/sites-enabled/*;  以上、終わり。最後にサービスを立ち上げて、自動起動を設定。
$ sudo service nginx start $ sudo chkconfig nginx on  </description>
    </item>
    
    <item>
      <title>Rails環境構築（3）PostgreSQLインストール</title>
      <link>https://you.github.io/post/2014-01-26-post/</link>
      <pubDate>Sun, 26 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-01-26-post/</guid>
      <description>ポスグレのインストール。yumで入るようなんだけど、聞くところによるとバージョンが古いらしい。念のため確認してみる。
$ yum list | grep postgresql （中略） postgresql.i686 8.4.18-1.el6_4 base postgresql.x86_64 8.4.18-1.el6_4 base postgresql-contrib.x86_64 8.4.18-1.el6_4 base postgresql-devel.i686 8.4.18-1.el6_4 base postgresql-devel.x86_64 8.4.18-1.el6_4 base postgresql-docs.x86_64 8.4.18-1.el6_4 base postgresql-ip4r.x86_64 1.05-1.el6 epel  うん、確かに古い。この記事を書いている時点では9.3.2とか出てるけど、yumで用意されてるのは8.4である。最新を入れればいいというわけでもないが、さすがにちょっと古いような気が。（ところでyumに入ってるパッケージの調べ方、これであってんのかな？）
てわけで新しいバージョンをWebから持ってくる。PostgreSQL RPM Repository (with Yum)にいろいろあるので適切なリンクURLをコピー。1コ前のバージョンでPostgresql 9.2にしときましょか。自分のOSがなんだかわかんなくなったらunameコマンドで確認。
$ uname -a Linux xxx 2.6.32-358.23.2.el6.x86_64 #1 SMP Wed Oct 16 18:37:12 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux  CentOS 6 x86_64であることがわかったので、wgetでダウンロード。
$ wget -P /tmp http://yum.postgresql.org/9.2/redhat/rhel-6-x86_64/pgdg-centos92-9.2-6.noarch.rpm  続いてRPMにインストール。
$ sudo rpm -ih /tmp/pgdg-centos92-9.</description>
    </item>
    
    <item>
      <title>Rails環境構築（2）Rubyのインストール &#43; α</title>
      <link>https://you.github.io/post/2014-01-23-post/</link>
      <pubDate>Thu, 23 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-01-23-post/</guid>
      <description>環境構築エントリーその2。まっさらなOSにRubyを入れていきます。
まずはいろいろパッケージのインストール。ぶっちゃけ参考書通りにやってるだけなので、どこまで必要なのかはよくわかってない。wgetとgitが必須であるのは言わずもがなだし、openssl-devel、make、postgresqlなんかもまぁ必要だろう。gccとかは要る……のか？
$ sudo yum -y install gcc gcc-c++ make autoconf openssl-devel readline-devel libyaml-devel postgresql9.1-devel wget git  続いて意気揚々とRubyを入れたいとこだが、その前にrbenvを入れる。以前にエントリーで書いたこともあったけど、rbenvを使ってRubyをインストールすると、複数のバージョンのRubyを切り替えて使えるようになるので大変便利。インストールもwgetでダウンロードしてビルドしてなんてやり方をせず、rbenv installコマンドで簡単にできるようになる。
$ git clone git://github.com/sstephenson/rbenv.git ~/.rbenv $ git clone git://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build  rbenv自体のインストールもgithubからのクローンで済むのでスマートですね。ついでにもう1個git cloneしているのはruby-buildって奴で、これがないとrbenv install（rbenv使ってRubyをインストールするためのコマンド）が使えないらしい。で、入れたらPATHを通します。
$ echo &#39;export PATH=&amp;quot;$HOME/.rbenv/bin:$PATH&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile $ echo &#39;eval &amp;quot;$(rbenv init -)&amp;quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile  PATHを通したらbash_profileを再読み込みして、それからtype rbenvコマンドを打ってインストールの正常完了を確認する。ちなみに自分は最初、bash_profileの再読み込み忘れて慌てました。
$ source ~/.bash_profile $ type rbenv # rbenv is a functionと表示されればOK  ここまでの手順はGitHubでrbenvのREADMEにも書かれてるから一読を推奨。
rbenvが入ったので、早速Rubyをインストール。
$ rbenv install --list #インストール可能なバージョンが一覧表示される $ rbenv install 1.</description>
    </item>
    
    <item>
      <title>Rails環境構築（1） Vagrantで仮想マシン構築</title>
      <link>https://you.github.io/post/2014-01-19-post/</link>
      <pubDate>Sun, 19 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-01-19-post/</guid>
      <description>Vagrantで仮想サーバー構築してみたのでメモ。構築するだけなら、楽。大してコマンドも要らない。
インストール まずはGemでインストールしてみたけど、バージョン古いっぽくてダメだった。なので、ふつうにウェブからダウンロードしてインストールします。URLは以下。ついでに「GET STARTED」からいろいろ読んどくといいと思う。公式のRead meに勝るものはないです。
http://www.vagrantup.com/
インストールしたらバージョン確認。
$ vagrant -v  boxの追加 まずはboxと呼ばれる仮想マシンイメージをvagrantに追加する。
$ vagrant box add hoge url  hogeは適当なイメージ名、urlはウェブで公開されているboxのurl。イメージはココに大量にあるので、任意のサーバーのURLをコピってくればOK。ほとんどがLinux、ちょこっとだけBSD。
vagrantの初期化 $ mkdir vagrant $ cd vagrant $ vagrant init hoge  vagrant initでカレントディレクトリをvagrant用に初期化する。なのでmkdirは好きなとこに好きなフォルダ作ればよい。そこに仮想マシンの設定ファイルが作られるので。vagrant initの引数にはさっきvagrant box addで追加した仮想マシンイメージの名前を渡してやる。渡さないとデフォルト値で設定ファイルが作られてしまう。これで最初ハマった。ちなみにどうハマるかというと、vagrant upしたときにこんなエラーが出る。
There are errors in the configuration of this machine. Please fix the following errors and try again: vm: * The box &#39;base&#39; could not be found.  「base」という名前のboxがないというエラー。どうもデフォルトでbaseという名前が使われるらしいのだが、さっきbox addした名前と異なるので、そんなboxはないから立ち上げられませんよということ。もしこの状態になってしまった場合の対処としては、vagrantfileのconfig.vm.boxの値を該当の仮想マシンイメージ名に替えてやれば上がるようになる。
話が飛んでしまったが、Vagrantfileはvagrant initしたディレクトリの中に出力されている。で、仮想マシンを上げる前にちょっとコイツの編集が必要。</description>
    </item>
    
    <item>
      <title>Ruby on Rails環境構築はじめます</title>
      <link>https://you.github.io/post/2014-01-18-post/</link>
      <pubDate>Sat, 18 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-01-18-post/</guid>
      <description>Ruby on Railsのアプリ構築の勉強もそこそこに進んできたので、そろそろ環境構築始めてみようかと。（あんまりブログで書けてないけど……）。てわけで、この本買いました。
Ruby on Rails環境構築ガイドposted with amazlet at 15.03.01黒田 努 インプレスジャパン 売り上げランキング: 44,343
Amazon.co.jpで詳細を見る これまで環境としてはMax OS X 10.8にVMware FusionでCentOS浮かべて、その上でコード書いてWEBlick立ち上げてってやってたんだけど、考えてみればWEBlickでやるなら手元のMacで良かったわけで。いちいちコード書くのにSSHつなぐとかまぁ無駄なわけで。ということで、環境は次のように変えていく。
 開発環境：Mac OS X 10.8（物理）
 Ruby 2.0.0
 Rails 4.0
 MySQL
 ステージング：CentOS 6.4（Virtual Box）
 Ruby 2.0.0
 Rails 4.0
 MySQL
 nginx
 unicorn
 vagrant + chef soloで構築予定
 本番環境：CentOS 6.4（VPS）
 構成はステージングと同様
  Apacheは仕事でも使ったことがあるんで、流行りに乗ってnginx + unicornでやってみます。んでさらに流行りに乗ってvagrantとchef solo使おうかと。vagrantはちなみにもう試してみたけど、さっくり仮想マシンが立ってしまってビビる。あとでブログにまとめる予定。ただしchefはちょっとしっかりやんないと使えなさそう。
もともと自分はインフラをメインで飯食ってるんで、こころへんはあまり迷わずにいきたいところ。</description>
    </item>
    
    <item>
      <title>bundle installはどのパスに対してすべきなのか問題</title>
      <link>https://you.github.io/post/2014-01-14-post/</link>
      <pubDate>Tue, 14 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-01-14-post/</guid>
      <description>今まであまり意識していなかったのだが、bundle installコマンドを打ったときにGemはどこにインストールされるのか？という問題がある。オプションなしにこのコマンドを実行した場合、Gemはシステム側（すなわちusr/lib/ruby/gems/云々）に入ってしまうわけだが、一つの環境で複数アプリを構築している場合はこれだと困るし、何より各アプリごとにGemfileを用意している意味が無い。というわけで、bundle installはパスを指定すべきである、ということ。
% bundle install --path vendor/bundle  Railsの場合はvendor/bundleディレクトリがあるので、ここを明示的に指定してやれば良い。これでGemも含めてアプリ内で「閉じた」状態にすることができる。
まぁこのへんの話は今更自分が書くまでもなく、いくつか参考になるエントリーがあった。ただ、自分が使っていた有名なオンラインのRoRチュートリアルにはこのことが書かれてなかったので、書き留めた次第。
 Rails開発環境の構築（rbenvでRuby導入からBundler、Rails導入まで） - Qiita [キータ] Bundler再履修: bundle execって何？ gemはどこに入るの？ - memo.yomukaku.net  
で、先日のエラーについてもこれが原因ではないかと思ったのだが。。。そんなことはありませんでした。</description>
    </item>
    
    <item>
      <title>twitter-bootstrap-railsでundefined methodのエラーに嵌る</title>
      <link>https://you.github.io/post/2014-01-09-post/</link>
      <pubDate>Thu, 09 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-01-09-post/</guid>
      <description>完全に嵌った。さっぱりわからん。
undefined method `to_css&#39; for nil:NilClass  twitter-bootstrap-railsを試してみているのだが、インストールしていざブラウザからアクセスしてみると、undefined methodのエラーが出てしまう。どのページにアクセスしてもダメ。
エラーメッセージ読んでも、そもそも#to_cssメソッドがどこにあるのかもわからんのだが、名前からしてLESSのコンパイルでトチってそうだという想像をしている。でもそれ以上はわかんない。ググっても情報はほとんどないし。
試しにrails newから全部やり直してみたりしたのだが、それでも尚ダメだった。READMEの通りにやってるはずなんだけどなぁ……。使ったコマンドは以下のみ。
rails new TestApp (Gemfileを編集) bundle install rails g bootstrap:install less rails g bootstrap:layout application fluid rails g Scaffold User name:string mail:string rake db:migrate rails g bootstrap:themed Users rails s  仕方ないので他のGem使おうかと。んー……なんなんだろ。</description>
    </item>
    
    <item>
      <title>今年はほぼ日からEDITに浮気しました</title>
      <link>https://you.github.io/post/2014-01-05-post/</link>
      <pubDate>Sun, 05 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2014-01-05-post/</guid>
      <description>title=&amp;ldquo;ほぼ日→EDIT by chroju, on Flickr&amp;rdquo;&amp;gt;
浮気しました。
就職2年目から2年間ほぼ日手帳を使い続けたけど、今年はEDIT。ライバル側に浮気してしまいました。理由はいくつかある。ほぼ日手帳に愛想を尽かした点が多いかな。
 紙面サイズが小さい。EDITの方が書くスペースが多い。 「今日の一言」が邪魔。手帳を書くときに脇目を逸らしたくない。 価格が高い。一番安いタイプだと、ポリエステルカバーのザラザラした手触りが苦手。  これはほぼ日が悪いというより、単に好みの問題だと思ってる。いわゆる「ほぼ日らしさ」が好きな人であれば多分たまらないんだろうし、俺も当初はそうだったんだけど、仕事で書くことが増えたり、もっといろんな用途で使いたくなってくると、徐々に物足りなくなってきた。よりシンプルで、スマートな印象があるEDITの方が魅力的に見えてしまった。
まだ使い始めて3日なのだが、EDITにも当然いまいちだなぁという点はある。
 ペンを挿すパーツがチープで心許ない。ほぼ日のバタフライストッパーは素敵。 紙質がすべっすべで気持ちいいんだけど、心持ちめくりにくい。 ほぼ日の月ごとにインク色が違うのは、パラパラめくるときにすごいわかりやすかった。  このあたりはトレードオフなので、両方使ってみて「より良い方」を選べばいいだけの話。ただし、手帳は1年ものなので、なかなか「気に入らないから買い直そう」というのがすぐ出来ないのが痛いところ。本来あるべき姿としては、あまりツールが何であるかにこだわりすぎず、出来るやり方でやってみることこそが重要なんだろう。物理的なガジェットだけじゃなく、タスク管理サービスとかでもそうだと思う（Wunderlistがいいか、Google Tasksがいいかで1か月悩んだことへの自戒。。。）</description>
    </item>
    
    <item>
      <title>『Webを支える技術』読了</title>
      <link>https://you.github.io/post/2013-12-28-post/</link>
      <pubDate>Sat, 28 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-12-28-post/</guid>
      <description>Webを支える技術 -HTTP、URI、HTML、そしてREST (WEB+DB PRESS plus)posted with amazlet at 15.03.01山本 陽平 技術評論社 売り上げランキング: 13,031
Amazon.co.jpで詳細を見る 言わずと知れた名著、なんですかね。元はと言えば山本陽平氏のREST解説記事をWebで見かけて、えっらいわかりやすかったもんで著作にも手を出してみたという感じ。あとRails触り始めたというのもあって、一度は読んでおくべきかな、と。
全体の印象としてはかなり平易な言葉で書かれてて読みやすい。ただ、多少の前提知識は必要とされるので、ウェブサービスを初めてこれから作ってみようみたいな人が読んでも厳しいと思う。自分はRailsかじってたので、それになぞらえて読み進めることができた。というか、この本読むとRuby on Railsが本当にRESTfulに設計されているんだってことが追認できる。
目次
 第1部 Web概論
 第1章 Webとはなにか
 第2章 Webの歴史
 第3章 REST ―― Webのアーキテクチャスタイル
 第2部 URI
 第4章 URIの仕様
 第5章 URIの設計
 第3部 URI
 第6章 HTTPの基本
 第7章 HTTPメソッド
 第8章 ステータスコード
 第9章 HTTPヘッダ
 第4部 ハイパーメディアフォーマット
 第10章 HTML
 第11章 microformats
 第12章 Atom
 第13章 Atom Publishing Protocol</description>
    </item>
    
    <item>
      <title>Windows 8(VAIO Pro)買って最初にやったことと注意点</title>
      <link>https://you.github.io/post/2013-12-22-post/</link>
      <pubDate>Sun, 22 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-12-22-post/</guid>
      <description>本家ブログの方に書いたけどVAIO Pro買った。1週間ちょいほど、主に仕事で使ってるが、やっぱ軽いは正義。持ち歩いてもあんまり負担にならない。スペック的にも申し分ないし。不満点といえば、マットな触感のボディが案外傷つきやすいことぐらい。これタッチパネル有りのモデルだと質感違うんだっけ。そっちのが良かったりするのかも。
で、Windows8はこれまでMac上に載せた仮想マシンで遊んでただけで、今回初めてきちんと初期設定とかしたので、いろいろまとめてみる。Win8が使いづらいってのは、要するにタブレット用のMetro UIを何も考えずデスクトップ用UIに統合させちゃったからで、この点上手く付き合えばそこまで悪くないOSかなとは思う。動きは軽い印象。
インストール インストールは対話型に順序良く進めれば別に迷うこととかはないんだけど、一点だけ注意すべき点がある。ユーザーアカウント作るときにMicrosoftアカウントがあるなら入れてくれと確か言われるんだが、これ、入れない方がいい。というのも、入れるとMSアカウントに登録している名前がそのまま勝手にユーザーネームとして採用されるからだ。当然ながらユーザーディレクトリ名もMSアカウントの「名」部分が使われる。すなわちMSアカに「山田太郎」と登録していると、C:\Users\太郎フォルダと永劫付き合っていくことになる。これは頭抱える。
はっきり言って糞仕様というか、こういうアカウントの「意味」が範囲を広げてしまうような昨今の風潮が自分は好きじゃない。例えばGoogleアカウントはもともとGmail目的で、つまりプライベートで使うものとして7年ぐらい前に取得したんだけど、いまや「ソーシャル」サービスであるGoogle+と勝手に結び付けられてしまって、当初こちらが意図していた「意味」を逸脱してしまっている。俺の持ってるMSアカウントも元を正せばSkypeアカウントだったんだが、それがまさかMSに買収され、さらにはOSのユーザーアカウントにまで流用されるとは思ってなかった。まぁアカウントの統合はわかりやすくて良いことかもしれないが、個人的には大変気持ち悪い。
MSアカウントを使うのが気持ち悪い人や、ユーザーディレクトリに2バイト文字使いたくない人はローカルでアカウント作りましょう。MSアカウントには後からでも紐付けられるんで。
アップデート インストールしたら早速8.1に上げた。上げていいこともあんまなかったけど、悪いこともそんなない気がする。スタートボタン復活が8.1の目玉らしいけど、あれ押しても例のタイル型UIのスタート画面が出てくるだけで、何の意味もないです。強いて言えば右クリックしてみると真価を発揮する、かも。
title=&amp;ldquo;スクリーンショット_2013-12-11_18_55_36 by chroju, on Flickr&amp;rdquo;&amp;gt;
見たまえ、このよく使うはずなのにどこ行ったかわからん機能をとりあえず寄せ集めました感。美しくない。
ちなみにアップデートはWindowsストアからです。Windows Updateじゃないです。いまだにWindows Updateがどこにあるかわかんないんだけど、どこにあんの？ MS公式のヘルプ見たら「検索から探せ」とか酷い答え返ってくるんだけど。。。
あとハードウェアによっては独自のドライバアップデートツールとかあるんでそのへんも忘れずに。VAIOならVAIO Updateね。
設定変更 適当にコンパネとか。
 フォルダオプションで不可視ファイルの表示と拡張子の表示。必須。 タスクバーとナビゲーションの設定でサインイン時にデスクトップを表示させる。 マウスの速さ適切化。あとCtrl押下でマウス位置表示させる。すぐいなくなるので。 電源オプションで無操作スリープ時間の変更と、休止状態の有効化。  ソフトウェアインストール ここまできてやっとソフト入れます。Win8アプリは普通にPCとして使う場合は不便そうなので入れてない。
 Google IME Firefox Evernote Dropbox Kaoriya版Vim(GVim) Avast! Janetter Keepass  ウイルスソフトとTwitterクライアントは今何が流行りなんですかね。よくわかんないので取りあえずAvast!とJanetter。もうリーマンやってんだからウイルスソフトぐらい買えって話だけど。
テキストエディタは迷ったけどMacと同じくKaoriya版Vim入れました。慣れるともうVimから離れられんね。OS選ばず同じ使い方できて楽だし。ちなみにGVimとしてしか使ってないです。コマンドプロンプトからVim開くのはなんかテンション上がんなくて。
ここまでやったらEvernoteとDropboxの同期でしばし待ち。
ソフト設定移行 ソフトの設定連携。というか、Dropboxの中に設定はほぼエクスポートしてるんで、DBの同期が終わった時点で半分移行は済んでいる。
Firefox FEBEっていうアドオンが便利なので良いです。アドオン、ブックマーク、設定とかそのへんまとめて自動的に吐いてくれる。さすがにGreasemonkeyは無理っぽいけど。
FEBE
dotfiles vimrcとvimperatorrcはdotfilesとしてDropboxにまとめているので自動同期。あとはシンボリックリンク貼っちゃえば終わりです。
mklink _vimrc Dropbox/dotfiles/.vimrc mklink _vimperatorrc Dropbox/dotfiles/.vimperatorrc  vimrc だいたいはMacで使えてたvimrcそのままでイケるんだけど、一部設定変更が必要だったり。例えばファイルパス指定しているような箇所はOSによって当然異なるので、if has(&amp;ldquo;Win64&amp;rdquo;)で分岐させる。あとGVimだとcolorschemeの設定がgvimrc優先になるとかそのへん注意。
vimperator vimperatorrcの同期は上に書いたDropboxでいいとして、pluginの同期まんどくせって思ってたんだけど、plugin_loader.jsで解決した。これ便利すぎてヤバイ。
let g:plugin_loader_roots=&amp;quot;~/.vimperator/vimperator-plugins/ ~/vimperator/vimperator-plugins/&amp;quot; let g:plugin_loader_plugins=&amp;quot;_libly,direct_bookmark,sbmcommentsviewer,caret-hint,appendAnchor,evernote-clearly-vimp,stella,feedSomeKeys_3,commandBookmarklet,copy&amp;quot;  g:plugin_loader_rootsにプラグインが入れてあるフォルダのパスを指定し、g:plugin_loader_pluginsに読み込むプラグイン名を列挙するだけでプラグインを読み込んでくれる。シンボリックリンクなんて不要。プラグインについてはたいていのvimperator遣いはvimpr/vimperator-pluginsをクローンしてると思うんで、組み合わせれば最強に近い。他から持ってきたプラグインについては別途パスを指定してやればいいだけだし。</description>
    </item>
    
    <item>
      <title>Bundle execサボってエライ目にあった</title>
      <link>https://you.github.io/post/2013-12-08-02-post/</link>
      <pubDate>Sun, 08 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-12-08-02-post/</guid>
      <description>今までよくわからなくてほっといたけどこれ使わなきゃダメだったわ……
bundle exec rails g devise:install Railsのログイン機構作るためにDevise入れてみてたんだけど、本来上のコマンドを入れるべきところでふつーにrails g devise:installとしてしまった。そしてrails g devise User ゴニョゴニョ。すると何が起きたか。
title=&amp;ldquo;スクリーンショット_2013-12-08_22.57.54-3 by chroju, on Flickr&amp;rdquo;&amp;gt;
い……いねぇ……。確かにgenerateしたはずなのにurbはあるけどコントローラーもヘルパーもねぇ……。でもね、この状態でブラウザ開くとなんかログインできるんだよ。動くんだよ。幽霊かと。お前ソースはどこにあんのかと。んで探ってみたらあった。
title=&amp;ldquo;スクリーンショット 2013-12-08 22.57.19 by chroju, on Flickr&amp;rdquo;&amp;gt;
なんかすげえとこにあった！！！！
あー、要はこれがbundle execの有無による違いね。bundle execプレフィックスを付けると今の環境のGemfileからインストールしてくれるけど、付けない場合はシステム側のRubyにdeviseをインストールしてしまう。だからこんなとこにいろいろ入ってしまったと。あー。失敗だ……。
なんかbundle execを省略する方法もあるらしいけど、よくわかってないものをよくわかってないままに省略するのも危険なので、しばらくは自戒込めてプレフィックス付けるようにします。</description>
    </item>
    
    <item>
      <title>SSHでサーバーリモートログインする際のあれこれまとめ</title>
      <link>https://you.github.io/post/2013-12-08-post/</link>
      <pubDate>Sun, 08 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-12-08-post/</guid>
      <description>うちのには仮想開発環境としてCent OSが立っているんだけど、この前コイツの中で作っているプロジェクトをGithubに突っ込もうと思ったらGithubで登録してるRSA認証のキーフレーズ忘れてて軽く詰んだ。で、仕方ないのでもっかい鍵作ってやり直そうと思ったら、SSHの鍵認証のやり方とかそのへんも全部吹っ飛んでたので改めてまとめ直しとくことにする。やったことまとめないのダメ、絶対。
環境 今回はいわゆる「リモートのサーバーにSSHを使ってログインする」場合と「GithubにSSHでpushする」場合を想定する。使用するクライアントはMac OS X、サーバーはCent OS。ちなみにシェルはbash。
そもそもSSHってなんぞや セキュアなリモート通信用プロトコル。セキュアな、と言っているのはTelnetあたりが平文でパスワード贈っちゃうのに対して、SSHが暗号化に対応している点を指す。で、暗号化には今多くの場合RSA（公開鍵暗号方式）を用いているらしい。だから秘密鍵と公開鍵を作ってローカルとリモートにそれぞれ配置して云々という設定が必要になる。あとGitHubもリモートレポジトリにつなぐ時にSSHが使える。
リモートサーバー接続手順 まずリモートサーバーにSSHでつなぐときの手順。
1. 認証鍵の生成 Linuxには鍵生成のコマンドがあるので、それを使う。
ssh-keygen -t rsa -tが鍵のタイプを示すオプション。ここではrsaを使うのでそのまま。あとは促されるままに保存先ディレクトリとパスフレーズを入力してやれば鍵が生成される。パスフレーズは忘れるとどうにもならんので絶対控える。絶対控える。
作成された鍵はデフォルトだと~/.ssh/配下に置かれる。id_rsaが秘密鍵でid_rsa.pubが公開鍵。
2. 公開鍵の転送 鍵が出来上がったので、早速だが公開鍵をリモートサーバーに転送してやる。転送の方法はいろいろ考えられる（一番アレな手段だと中身コピペしちゃえばいいだけだったりする。単なるテキストだし）けど、scp使うのが個人的には楽かな、と。まぁscpもSSH利用したファイル転送なので、SSH使うためにSSH使っているという矛盾っぽいところはあるんだが。細かいことは置いといて。
scp ~/.ssh/id_rsa.pub hoge@fuga.com:~/.ssh/ SCPコマンドの第一引数が送るファイル、第二引数が転送先のユーザー名@ホスト名:ファイルパス。通信するときに指定したユーザー名のパスワードを聞かれるので答えてやる。SSHの認証の方式はいくつかあって、デフォルトではパスワード認証の設定になっているので、こういうログインの仕方になる。今このエントリの作業でやっているのは、パスワード認証から公開鍵認証に変えるための手順。
ちなみにホスト名は当然ながら名前解決出来なきゃアウトなので、ローカルの開発環境とかだったらhostsに入れとくとかなんとか忘れずに。あと俺みたいにSSH1回設定したんだけどいろいろ忘れちゃったんでもっかい鍵作ってまーす(ﾉω・)ﾃﾍ)みたいな人（あんまいないだろうが）はすでにリモート側のsshd_configが書き換わってたりするので注意。ていうか俺が少しやられた。
例えばセキュリティ面の配慮でポート番号を22から変えてると当然ながら通信できない。-Pオプションでポート番号を明示的に指定する。あるいはすでにSSHをパスワード認証で使うのをNGにしてる場合もある。これについては後述。
3. 公開鍵の登録 今度はリモートサーバー側に入って、送られた公開鍵のファイル名を替えてやる。
mv ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys authorized_keysが公開鍵登録用ファイルの名前。複数の公開鍵を登録することもできる。その場合は改行挟んで追記でいいのかな。アクセス元を制限してやったりとかできる記法もあるらしいが、ここでは割愛。
あとアクセス権も変える。
chmod 600 ~/.ssh/authorized_keys 4. sshd_configの設定 リモートホスト側の/etc/ssh/sshd_configファイルがSSHの設定ファイルになっているので、公開鍵認証が出来るよう、設定を書き換える。
RSAAuthentication yes PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys 読めば字の如くだが、上からRSA認証許可、公開鍵認証許可、公開鍵ファイルパスの指定。デフォルトでは全部コメントアウトされていると思うので、#を外して設定を有効化する。おそらく一番下のオプション書き換えでauthorized_keysファイル以外のファイルも公開鍵に指定できるんだと思うけど、やったことないのでわからん。
逆にパスワード認証（SCPでさっき使ったヤツ）はこれで要らなくなるので無効化する。
PasswordAuthentication no ChallengeResponseAuthentication no 上がパスワード認証の不許可。下はチャレンジレスポンス認証の不許可。こちらもnoにしとかないとパスワード認証が完全にオフにならないとどっかで聞いた。うろ覚え。。。
あと気になるようであれば、port行を変えればポート番号も変えられる。22はウェルノウンポートなので、変えておいた方が無差別な攻撃は防げるはず。
書き換えたらsshサービスを再起動してフィニッシュ。
/etc/init.d/sshd restart これでローカル側からsshコマンド打ってやればつながる。パスワードを聞かれたら、公開鍵作る時に入れたパスフレーズを答えればOK。
5. Githubにつなぐ場合 Githubにつなぐときは公開鍵の中身を全部コピーして、GithubのAccount Settings &amp;gt; SSH Keysにベタッと貼り付けてやればそれでOK。はじめはリモートホストにつなぐときと別の鍵を生成してやった方がよいのかな？とか思ってしまったが、公開鍵認証の意味を考えてみると、別の鍵を作ってやる必要はなかったので特にこれで問題はない。ちなみに俺の話をすると、これまで.ssh配下にcentosとかgithubとかフォルダ作ってそれぞれの鍵ファイルを管理してた。うん、無駄だったんやね……。
リモート側のサーバーで環境共有したい場合はgit cloneで持ってくる。</description>
    </item>
    
    <item>
      <title>GTD環境 2013年秋版</title>
      <link>https://you.github.io/post/2013-11-10-post/</link>
      <pubDate>Sun, 10 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-11-10-post/</guid>
      <description>先日GTDの聖典を読み込み、ちょっと久しぶりにGTD環境を再構築した。今年に入ってからCatch Notesを使ったGTDを試すも、PCを使ってる時にわざわざブラウザからタスク追加するのが面倒になり、初夏あたりでtodo.txtに乗り換えた。しかしこれもソートがしづらいだとか、複数のリスト持つのが面倒（たぶん俺のVimスキルがアップすればなんとかなるんだけど）などがあり、晩夏あたりでストップしてしまっていた。それから先々週頃までGTD環境は空白状態。そして2ヶ月ぶりぐらいの再構築に至ったわけだが、やっぱしGTDやっとくとなんとなく気が楽ね。
で、どうにもツールに振り回されすぎてる嫌いがあるので、ここらでもう一度自分がやっていることをまとめておく。やりたいことを一つに定めれば、どんなツールを使っていようとも迷うことはない。そういう状態に持っていきたい。
仕事とプライベートで環境を分ける 入社当初、Toodledoで仕事もプライベートもすべてのタスクを管理していたことがあったのだが、これは無理だった。自分の職場の事情故の話ではあるが、会社PCではクラウドサービスの使用が禁じられているし、また仕事の資料を家に持ち帰ることはできない。つまり仕事のタスクをいつでも見られるようにとToodledoに入れておいても、会社ではそれを確認できないし、家では仕事の状況を確認する資料がないからタスクレビューができない。
よって今は仕事とプライベートのGTD環境を完全に分けている。仕事ではTaskChuteを使い、TaskChuteはGTDには向かないなどという声もあるが、まぁ仕事のタスクなんて今すぐやるか、いつかやるかのほぼ2択なので、降ってきたタスクを適当な日付にぶちこんでおしまいである。「いつかやる」リストに行くはずのタスクは1週間後の日付でとりあえず入れておけばいいだけだし、「連絡待ち」リストに行くタスクは適したタイミングで「○○さんに例の件確認」と入れればいい。正直、仕事はこれで本当にうまく回っている。
仕事とプライベートのGTD環境を分けることには利点もある。TaskChuteは会社のファイルサーバーに置いてあるので家からは確認のしようがなく、ON-OFFのメリハリがつくようになった。家に帰ってタスクリストを開いたら、仕事の予定がいっぱい入っているなんて状態は、精神によろしくない。
軌道修正を図りやすくする ではプライベートのGTD環境とは何のためにあるのかということだが、軌道修正を図りやすくするためのものだと考えている。
プライベートのタスクとは何か。主には家事、勉強、娯楽に分けられる。このうち家事は細切れのタスクがほとんどだが、娯楽と勉強はPJのような形になることが多い。それは例えばRailsを習得する、だったり、秋の気になるアートイベントに行きまくる、だったり。
ただ、プライベートで抱えるPJを毎日コツコツ回せるかというと、なかなか時間は取れないもので、中には1ヶ月近く放置になってしまうものもある。そんなときに、「あれ？そういえばあれやりたくてやりかけにしてたけど、どこまでやったっけ？」とならないよう、フローの洗い出しと経過の記録を怠らないようにする。また進んでないPJにすぐ気付き、対策を打てるようにしておく。
そして長く時間を費やす「PJ」は、往々にして相応の理由がある。例えばRails習得はWeb系の転職を考えているからであり、ではWeb系になぜ転職したいのか？といえば、、、といった具合に、理由はより大きな理由に遡及できる。自分の立ち位置を見失わないためにも、理由の記録と記憶は必要だ。これを管理するには、GTDでいう高度の概念がふさわしい。
ユビキタスキャプチャーを重視する 週次レビュー以上に自分が意識しているのがユビキタスキャプチャー。いくらトリガーリストがあると言っても、やはり2時間うんうん唸ってタスクを書き出したところで、一度忘れたタスクは思い出しづらい。思いついたこと、インプットされた情報はとにかくすぐメモる。この方が確実だ。
ただ、自分はどうもこれが苦手で。あまり深く考えない性格なのか、気付けば新たな情報を右の耳から左の耳へ聞き流していることがある。どうすりゃいいんだろうな、これ。
ToDoリストに手間をかけすぎない ToodledoはGTDに最適だとよく言われる。自分も使っていたが、そのことは肯定できる。だが、ちょっとガチガチに設定できすぎるような気もする。アレに凝りすぎると、ToDoリストを管理するというToDoに1日相当な時間を使いそうになる。
だからToDoリストは手間をかけすぎないのがいい。先に言及したTaskChuteはその点すごく楽で、開いたらAlt+s -&amp;gt; aで行を追加し、日付とタスク名と見積時間を入れれば登録完了だ。プロジェクトの入力も可だが、そのへんはお好みで。
プライベートでは今、Wunderlistを使っている。これもまた実にシンプルなツール。次点でTodo.txtだが、これはすでに述べた通りちょっと単機能すぎる。Google Tasksもまた然り。
「いつやるのか」を明確にする GTDは元来、タスクのスケジューリングは必要としない。GTDにおいては、ToDoリストとは「時間があるときにやることのリスト」であり、やる時間が決まっているものはカレンダーに登録するべきだからだ。でも現実、そう上手くいくか？ この1週間のどこか空いた日で必ずやるとか、そういうタスクもあるだろう。
自分はWunderlistのスターを「スケジューリング」として使っている。スターが付いたタスクとは、「今週やるべきタスク」だ。「時間があるときにやること」の中でも、最優先でやるべきことにスターを付けている。
タスクのスケジューリングは重要だ。いつまでもリストに眠り続けるタスクを産まないためにも。
現在の環境 以上の方針を踏まえた上で、次のようなGTD環境（プライベートver）を回している。
 Wunderlist =&amp;gt; タスクリスト
 すべての「やること」がここにある。
 リストはNext、Someday、Want＝やりたいこと、各PJ。
 PJはノートを使って目的と〆切を書いている。タスクには最初マイルストーンだけを置いておき、実際に進めながら細かいタスクへ分解している。
  title=&amp;ldquo;スクリーンショット 2013-11-10 21.20.34 by chroju, on Flickr&amp;rdquo;&amp;gt;
 Google Calendar =&amp;gt; 予定管理
 アナログ管理とすべきかだいぶ悩んだけど、繰り返しの予定とかが管理しやすいのでデジタルに軍配。
 基本はプライベートの予定。仕事や休日や深夜のものだけ入れる。
 「やる日が決まっているタスク」もここに入れる。
 Dropbox&amp;amp;txt =&amp;gt; ユビキタスキャプチャーその1
 デジタルのユビキタスキャプチャー管理。メモはやっぱプレーンテキスト最強。
 Macで扱うときはQFixHowmを使っている。
 Androidで扱うときはDraft。これ最強。このエントリーもDraftで書いてます。</description>
    </item>
    
    <item>
      <title>『RailsによるアジャイルWebアプリケーション開発』読了〜「設定より規約」なら、規約をまず知りたい</title>
      <link>https://you.github.io/post/2013-11-04-post/</link>
      <pubDate>Mon, 04 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-11-04-post/</guid>
      <description>RailsによるアジャイルWebアプリケーション開発 第4版posted with amazlet at 15.03.01Sam Ruby Dave Thomas David Heinemeier Hansson オーム社 売り上げランキング: 138,293
Amazon.co.jpで詳細を見る Rails入門の定番書。このブログで何度か読んでいる途中経過を報告してきたか、ようやく読み終えることができた。読了というか、読み終わったのはもうだいぶ前ではあるんだけど。しかし予定していた以上に時間をかけてしまった。。。最初は写経しながら読んでいたのだが、どうにも時間がかかりすぎてイライラしていて、途中からはザーッとRailsの概略を掴むためだけのような読み方になってしまった。写経した方がやっぱり頭には入ると思うのだけど、それでサンプルアプリを作ったところで、応用してすぐ自分のアプリを作れるとは限らなくて、なんとももどかしい。結果としてはまだ消化しきれてなくて、行ったり来たり読み返しながら自分なりにアプリを作り始めているのが今の段階。
この本は表題通り、架空のアジャイル開発案件を進めるかのように、徐々にRailsアプリを作る過程をなぞっていくことで、Railsで出来ること、Railsでアプリを作ることを教える構成を取っている。なので実践的でわかりやすい反面、解説なしにいきなりソースを提示されたりもするので「なぜこういうコードで動くのか？」が掴みにくいことも少なくなかった。アプリの完成まで読み進めると、その後に「Rails詳説」としてRailsの規約やらフレームワークを解説するページが現れるのだが、個人的には正直逆の構成の方が掴みやすい。あるいはアジャイル開発を1段階進めていくごとに、そこで書いたソースの詳細な解説を挟むようにしてほしい。
というのも、Railsはコードを省略したり、フレームワークによって自動生成されたりするものが非常に多い。だから初見ではどこまで書けば良いのか、どこからオートで作り上げてくれるのか、感覚が掴めないのだ。俺はウェブアプリケーションとしては初歩的なJavaアプリの経験しかないのだが、それと比べてRailsのアプリ構築は遥かに簡単な印象を受ける。正確に言えば、自分で書かなくてはならない部分が少ない。しかしそれ故に、手で書かなかった部分がどう動いているのかがわかりづらく、技術を手中に収めた感覚がない。Javaのウェブアプリはプラグインなどを使わない限りは「書いたものが動く」感覚だったので、手とアプリが連動しているという錯覚があった。
本書のような「実際に作る過程を見せる」という技術書は世の中に数多くあるし、基本的にはその方が「わかりやすい」本になるのだと思う。が、ことRailsに関しては作る前に「Railsは何を成してくれるのか」を解説してくれる本が必要なんじゃないかと。もちろん、本書においては「Rails詳説」の章がその役割を果たしているわけで、もう少し読み込まないとRailsをモノには出来そうにない。
次の本としては、これまた定番であるレシピブックを買った。これとRails詳説を片手に、Railsがどう動くのか？を学びながら、まずはローカル環境でアプリを作ってみる。外観まで含め、年内にきちんと形になるところまで持っていきたい。
Rails3レシピブック 190の技posted with amazlet at 15.03.01高橋 征義 松田 明 諸橋 恭介 ソフトバンククリエイティブ 売り上げランキング: 57,121
Amazon.co.jpで詳細を見る </description>
    </item>
    
    <item>
      <title>短時間に限定して図書館で情報をザッピングするという勉強法</title>
      <link>https://you.github.io/post/2013-10-20-post/</link>
      <pubDate>Sun, 20 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-10-20-post/</guid>
      <description>新しい分野の勉強をするときって、ネットの情報だけでは不十分だったり信頼性に欠けたりするし、だから書籍に頼ろうと思うんだけど、どんな本 を選べばいいのかわからなかったりもして悩ましい。かといって時間をかけて何冊も読む暇もないし、そんなにかけるお金もない。そういうとき、 基本に帰って図書館に行ってみたら良いのですよね。
NISAのこともあって、投資とか金融について少し勉強しようと思い立ったのだけど、特にこの分野って一人の言い分だけを鵜呑みにすると恐ろしく 危険。じゃあ時間を決めて、5冊ぐらいをザックリとザッピングしてみようということで、図書館に行きました。本を選び、時間を決めてザッピン グ。とにかく多くの情報に触れることを重視したので、細部はバンバン読み飛ばす。わからない単語があったら取りあえずメモだけ取っておいて、 あとでググるなり調べることにする。そんな感じで約3時間で計7冊。A4見開きいっぱいに情報を書き留めて、それなりに情報を取り入れられた感じ がした。
図書館を使うメリットとしては、
 お金がかからない（貧乏性でサーセン） 手元の情報じゃ足りないと気付いたとき、すぐ追加の本を探せる とにかく静かで勉強するには快適 ハズレの本を引いたときのガッカリ感が少ない 逆にもし「これは！」という一冊があれば借りて帰れる 後から「あのとき読んだ本もっかい読みたい」と思っても楽に後追い可能  というあたりか。あ、ただ小さな図書館で勉強スペースを長時間独占したりすると迷惑だったりもするので注意ね。ちゃんと勉強が許されてるとこ で。
ちなみに俺が使ったのは日比谷図書文化館。大学によくあるような勉強スペースが設けられてる上、館内のPRONTOで買ったコーヒーを飲みながら本 を読んでもOKだし、平日は22時まで（！！！）やってるから仕事帰りにも寄れて超オススメ。蔵書量はちょっと少ないかもしれんけど、オライリー 本がかなり置いてあったり面白い品揃えではある。あ、当然ながらわりと混んでます。あーやっぱ東京東部で働きてぇな。。。</description>
    </item>
    
    <item>
      <title>GTDの原典読んでみたら意外にいろいろ勘違いしてたという話</title>
      <link>https://you.github.io/post/2013-10-14-post/</link>
      <pubDate>Mon, 14 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-10-14-post/</guid>
      <description>GTDというかタスク管理についてはもうだいぶ長いこと悩んできたのだが、もうなんともにっちもさっちもいかなくなってきたので、ついに原典を紐解いてみた。といっても本当の原典ではなく、実践編の方。まぁWebでつまみ食い程度とはいえ、一応GTDの理念は知った上でタスク管理してたので、実践編の方が役に立ちそうかな、と。でも実際読んでみて、案外GTDの基礎を勘違いしてることに気付いたり。
ひとつ上のGTD ストレスフリーの整理術 実践編――仕事というゲームと人生というビジネスに勝利する方法posted with amazlet at 15.03.01デビッド・アレン 二見書房 売り上げランキング: 14,427
Amazon.co.jpで詳細を見る GTDは頭の中を書き出すことがすべてではない GTDってなんとなく「2時間かけて頭の中を書き出しましょう」「そのリストを1週間おきに週次レビューで見なおして最新の状態を保ちましょう」っていうのが核だと思っていたのだが、それがすべてではなかった。というより、それはあくまで始まりである。書き出して意識の外で保存するだけでは中途半端で、それを「どう処理するのか？」という点で見極めた上で、適切なリストに並べておかなくてはならない。例えば時間の決まっている行動であればカレンダーに書き込むことになるし、誰かに頼めることであれば、依頼した上で「連絡待ち」リストに入れる。今は使わないが、いずれ興味が出そうなことであれば、数日後に思い出せるようリマインダーに仕込んでおく。
GTDとはすなわち「絶え間なく入ってくる情報を適切に整理し、すぐ行動に移せるようリスト化しておく」ことに他ならない。単なる「タスク管理」とはその意味で異なる。
「高度」という考え方を取り入れる 目の前のタスクを処理する、整理する他に、GTDには高度という考え方がある。一つ一つの具体的な行動を高度0メートルとして、それを1000メートル、2000メートルという名前の「より広い視点」から位置づけるのだ。例えば「GTDの本を読んだ感想をブログにまとめる」は、より広い視点から言えば「タスク管理方法を考えなおす」というプロジェクトの一部だ。さらにこのプロジェクトは、「時間を効率的に使う」という目標の一端を担うものとなる。という具合。
今気になっていることをリスト化して行動に移すという水平思考と同時に、GTDにはその各行動がどう位置づけられ、どのような優先順位とすべきなのかという垂直思考の考え方がある。この点ってあまりWebのGTD系記事では触れられていない気がして新鮮だった。そして特に「プロジェクト」や「目標」の管理に手を焼いていたので、ちょっと助かった思い。
とはいえ、この本はプロジェクトのリストとNext Actionリストの結びつけについては明確な答えを用意していない。きちんと整理された状態であれば、自ずと各リストの関連性が見えてくるといった回答が成されていたのだが、そんな曖昧なことに任せてられるかｗというのが正直なところ。プロジェクトには進行する上でのフローがあり、各フローをいつ、どのようにこなしていくかという視点から考えて、まとめておくことが必要なんじゃないかと思う。
ゴールは「常に最善の行動を取れるようになる」こと 何度も書くが、GTDはタスク管理の手法、ではない。今気になっていることと、それらの位置づけ、見通しをリスト化することにより、「今すべきこと」を明らかにすることだ。単なるタスク管理では、突発タスクを割り込ませる余裕がなくなったり、先々への見通しを持たず、目先の「やること」だけに集中してしまいがちだったりする。
GTDでは常にすべての行動がリスト化されているので、例えば突発的なタスクがあっても、それを既存のリストの中に位置づけ直した上で、いつどのように行動すべきかを捉え直すことができる。仮にその余裕がないほどの事態であったとすれば、すぐ元の状態、正常な状態に戻れることが重要だという。リストは完璧である必要はないが、信頼に足るもの、心の中の「気になっているもの」をすべて書き出したものである必要はある。そしてその状態を保つからこそ、常に最善の行動を取ることができる。
今後この本をどう活かすか GTDの基本概念を捉え直すことはできた。その上で今悩んでいるのは以下のようなこと。
 様々なリストが必要なようだが、何で管理するのか？（Google Tasks？、プレーンテキスト？、Evernoteは重いので論外） リストを見る気力もないときはどうしよう？（本書では「やる気がないことにすることリスト」も作っていたが、それすら見る気にならない時もある） 「次にやること」を実際いつやるのか、スケジューリングする必要はないか？  GTDというのはあくまで、自分の日々の状況を「最善」にするための補助ツールに過ぎないわけで、それに長々と時間をかけてしまうのは本末転倒。早いとこやり方を決めて、自分なりに進めていきたいところなのだが、はてさて。</description>
    </item>
    
    <item>
      <title>RoRでログイン機能を実装する</title>
      <link>https://you.github.io/post/2013-10-06-post/</link>
      <pubDate>Sun, 06 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-10-06-post/</guid>
      <description>sessionに情報を保存することで「ログイン」状態を実現する 逆に言えばsession破棄で「ログアウト」扱いになる Railsにはsessionオブジェクトが用意されており、これを使えば簡単にsessionへアクセス可能  ……というわけで。
1. sessionコントローラを作成する $ rails g controller sessions new create destroy   コントローラ作成のときは複数形！ newはログイン画面表示時に、createはログイン処理時に、destroyはログアウト処理時に使用  コントローラを作成したらとりまログイン処理を書いていく
 &amp;lt;span class=&amp;quot;synPreProc&amp;quot;&amp;gt;def&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synIdentifier&amp;quot;&amp;gt;create&amp;lt;/span&amp;gt;↲ user = &amp;lt;span class=&amp;quot;synType&amp;quot;&amp;gt;User&amp;lt;/span&amp;gt;.find_by_name(params[&amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;:name&amp;lt;/span&amp;gt;])↲ &amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;if&amp;lt;/span&amp;gt; user &amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;and&amp;lt;/span&amp;gt; user.authenticate(params[&amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;:password&amp;lt;/span&amp;gt;])↲ session[&amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;:user_id&amp;lt;/span&amp;gt;] = user.id↲ redirect_to tasks_url↲ &amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;else&amp;lt;/span&amp;gt;↲ redirect_to login_url, &amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;:alert&amp;lt;/span&amp;gt; =&amp;gt; &amp;lt;span class=&amp;quot;synSpecial&amp;quot;&amp;gt;&amp;quot;&amp;lt;/span&amp;gt;&amp;lt;span class=&amp;quot;synConstant&amp;quot;&amp;gt;無効なユーザー名／パスワードです。&amp;lt;/span&amp;gt;&amp;lt;span class=&amp;quot;synSpecial&amp;quot;&amp;gt;&amp;quot;&amp;lt;/span&amp;gt;↲ &amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;end&amp;lt;/span&amp;gt;↲ &amp;lt;span class=&amp;quot;synPreProc&amp;quot;&amp;gt;end&amp;lt;/span&amp;gt;↲   フォームで入力した値など、URLパラメータを受け取るときはparamsを使う authenticateメソッドは、引数とuserのパスワードダイジェストを比較して論理値を返す タスク管理アプリを作っているので、ログイン成功時のリダイレクト先はtasks_url ちな、hoge_urlは絶対パス、hoge_pathは相対パスになるらしい 302 Redirectでは完全修飾URLに飛ぶのが仕様なので、redirect_toではhoge_urlを使う 失敗時はlogin_urlに返し、エラーメッセージを渡す  2.</description>
    </item>
    
    <item>
      <title>RoRでScaffold作成を間違えたときの対処法が知りたい</title>
      <link>https://you.github.io/post/2013-09-21-post/</link>
      <pubDate>Sat, 21 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-09-21-post/</guid>
      <description>generate scaffoldはMVC全部作ってくれて便利なんだけど、うっかりカラム名間違えたりしたときの対処法ってあるのかしら。後から変更したい場合は全部逐一手動で変更してね☆ってことだとさすがに面倒臭すぎるんだが……。
とりあえず今はscaffoldを削除してイチから作り直す形で対処しているが。
$ rails destroy scaffold hoge このとき、db:migrateすでにしちゃってるならsqlite入ってDROP TABLEするのも忘れずに。
後からアプリ改修して新しい属性付け足したりとかって場面もあるだろうし、何かしらscaffoldに変更をかける手段はあるんじゃないかと思って探してるんだけど見つからない。ぐぬぬ。</description>
    </item>
    
    <item>
      <title>バージョン管理に注意しながら、Ruby on Railsの動作環境を構築する</title>
      <link>https://you.github.io/post/2013-09-16-post/</link>
      <pubDate>Mon, 16 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-09-16-post/</guid>
      <description>資料によっていろいろ言っていることが違うので、自分なりに調べながらまとめる。こういうのchef?とかで書いとけば環境構築楽になるんだろうな。
必要なもの  Ruby ……当然のごとく Rails ……当然のごとく
 rbenv ……RubyとRailsの管理システム
 ruby-build ……rbenvとセットで使うっぽいがよくわかってない
 Git ……バージョン管理。必須ではない気もするがあった方がいい。rbenvとかのインストールにも使う。
 RubyGems ……Rubyのパッケージ管理システム
 Bundler ……Gemパッケージの管理システム
  こんなところだろうか。直接的に必要なものの他に、それぞれを管理するためのツールが必要だったりしてなかなかに複雑であった。あとhomebrewとかのあたりまで遡るとキリが無くなりそうなのでさすがに割愛。というかOSによっても違ってきちゃうし。readlineとかopensslとかも依存関係があるっぽいんだけどよくわからないので割愛……。んー、意味あるのかこの記事。俺がRoRで使ってる環境はちなみにCent OSなので、その前提で書きます。
rbenv 本とか読んでるとRubyのバージョン管理にはRVM使わせているものが多いんだけど、rbenvの方が軽くてBundlerとの相性も良いらしい。よってrbenvを使うことにする。
参考：(http://passingloop.tumblr.com/post/10512902196/difference-between-rbenv-and-rvm)
インストール先のパスが迷う。。。ホームフォルダに不可視で入れるのが気持ちとしてスッキリするのでそれを採用。
$ git clone git://github.com/sstephenson/rbenv.git ~/.rbenv で、bash_profileにパスを追加。
echo &#39;export PATH=&#34;$HOME/.rbenv/bin:$PATH&#34;&#39;  ~/.bash_profile echo &#39;eval &#34;$(rbenv init -)&#34;&#39;  ~/.bash_profile source ~/.bash_profile 追記 実はこの記事書く前にすでにRVM入れちゃってたんだけど（今読んでる『RailsによるアジャイルWebアプリケーション開発』にはRVMが紹介されてたので）、RVM削除してrbenvを入れなおそうと思い立って調べてみた。そこで辿り着いたRVMのアンインストールコマンドがなかなかに衝撃的だったので追記。
$ rvm seppuku 切腹！！！ｗｗｗｗ
ruby-build よくわかってないけど入れる。わかっていることとしては、これがないとrbenv installコマンドが使えない。rbenvのプラグインとしての位置づけらしいので、.rbenvフォルダ配下に入れる。公式のREADMEにもそう書かれてます故。
$ git clone git://github.com/sstephenson/ruby-build.git ~/.rbenv/plugins/ruby-build Ruby Rubyはrbenvを使ってインストール。バージョンについては都度変わるので、$ rbenv install -lコマンドで落とせるバージョンを確認しつつ、必要なものを入れる感じで。まぁ使うバージョン選択できるんだから、そんなにセンシティブになる必要はないと思うが。</description>
    </item>
    
    <item>
      <title>『RailsによるアジャイルWebアプリケーション開発』第II部読了</title>
      <link>https://you.github.io/post/2013-09-04-post/</link>
      <pubDate>Wed, 04 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-09-04-post/</guid>
      <description>「仕事で忙しい」という最悪の理由により全然勉強してなかった件。Rails本、やっと第II部読み終わった……。
 基本的に rails generate hoge コマンドで色々作れる。scaffold、model、controllerとか、コマンドで作ればRailsが必要なファイルを用意してくれる。あとは作られたファイルをガチャガチャいじってアプリを作っていくというのが基本の流れ。
 特にscaffold。これをバチコンと叩き込むとコントローラ、モデル、ビュー、マイグレーションファイルから何から全部作られる。英語で「足場」という意味らしいがまさにそんな感じ。
 scaffold作ると編集画面（edit.html.erb）とか部分テンプレート（form.html.erb）まで作られるという至れり尽くせり感ビビる。
 create()やupdate()などの基本的なアクションも作られるので、実装したい動作によっては新しいコントローラをわざわざ作るまでもなかったり。
 テスト用のファイルも自動生成される。コントローラの機能を試すファンクションテスト、モデルの動きを試すユニットテスト、そしてアプリのフローを試す統合テスト。
 テスト用ファイルの編集方法がいまいちよくわからず。このあたりは試してみないと実感わかないかも。
 複数モデル（テーブルと読み替えてOK？）間の関係を表すには、modelsファイルにhas_manyやbelongs_toを追加する。DBを直接弄ったりはしない。
 button_to()に:remote=&amp;gt;trueを加えるだけでAjaxが追加できる。……え？
 そもそもAjax使ったことないんでこのへんはもう少し見直したいが。。。Ajaxと言っても、再読み込みなしで画面の一部が再描画される程度の話か。
 ちなみRailsのCSSは標準でSCSSが使われている。SCSSって最近まで知らなかったけどクソ便利すぎて笑う。最初からCSSがこういう文法ならいいのに……。これもまた勉強が必要なり。
 コントローラのrespond_toメソッドは出力形式を色々替えられる。format.json、format.atomとか。
 メールの送信機能もある。。。（このへんになるともう色々出来過ぎてよくわからなくなってきた）
 Railsのコードは当然ながらRubyで書くわけだが、他にヘルパーメソッドというものがある。フォーム用にボタンリンクを作るbutton_to()とか、リンクを作るlink_to()とか。Rubyをそもそもまだちゃんと理解できていないので、どれがRubyの文法でどれがヘルパーメソッドなのかの見分けもいまいちついていない状態。
  わーすげー簡単にまとめてしまった。全体の感想としては、とにかく何かと簡単かつ便利。自分でコーディングが必要な範囲は最低限。Javaでウェブアプリを組んだ経験があり、そのときはMVCそれぞれのファイルを全部イチから手で作っていたわけだが、RORではrails generate scaffoldで一発である。なんてこったい。しかし逆に言えばアプリの構造自体は全部任せて、中心的な機能や外観のコーディングだけに集中できるんだから、効率が良いわけで。
SCSSとかAjaxとか、ウェブアプリを作る上でよく使われる技術が標準で備わっているというのも利点だろう。だから覚えることも多い。SCSS、Ajax、あとはDBもか。DBはSQL文を直接発行するようなことはないように組まれているけど、has_manyとか正しく書くためにはDBの構造あたりを正しく知っていなくてはならない。うむ、これからいろいろと学ばなくては。
しかしこの本、いきなり実例から入ってしまうので理解がしやすいやらしにくいやら。実際に何か作りながら説明していく本というのは少なくないが、この本は第III部でRails全体の詳説が入る。「ここで入れるなら先に解説してくれよ！」と個人的には言いたい。いきなり知らないメソッドをバシバシ盛り込まれてどんどんコーディングを進められるより、そもRailsとはどういったディレクトリ構造になっていて、どういった動作をするものなのかという中身を知ってからの方が実例を理解しやすいのではと思う。まぁこのへんは好みか。
第III部を読みつつ、そろそろとりあえず何か作ってみてもいいかもしれない。SCSSやAjaxを知らない以上、見た目にこだわったりとかはできないけど、とにかくアプリっぽいもの、データを追加して編集して表示させてっていうものをまずは作る。9月はそれが目標だなー。</description>
    </item>
    
    <item>
      <title>技術書のサンプル試す場合はバージョンを揃えるべきなのか</title>
      <link>https://you.github.io/post/2013-08-03-post/</link>
      <pubDate>Sat, 03 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-08-03-post/</guid>
      <description>『RailsによるアジャイルWebアプリケーション開発』、大変くだらないところでつまずいているというか、なんか知らんがエラー出まくりで先に進めず。
問題が起きたのはp.79。モデルクラスのrbファイルにvalidatesメソッドによる検証過程を書き入れ、test/functional/xxxに有効なテストデータ（とされるもの）を入れてrake testコマンドを発行したところ、エラーが出る。何度やってもエラーが出る。仕方ないので一旦先に進もうともしたのだが、次に編集対象になっていたtest/unitフォルダが見当たらない。念のため別プロジェクトを作ったりもしてみたが、やはりtest/unitは生成されなかった。さすがにディレクトリ構造丸ごと違うとなると、自分のミスというよりはバージョンの相違によるものではないかと思い至る。この本ではRubyの1.9.2とrails 3.1.0を使っていたが、僕の環境はRuby2.0とrails 4.0.0だった（もっと他にも「バージョンが原因ではないか？」と思い当たった根拠があったはずなのだが失念。トラブルシュートは記録しながらやるべきだったなー……）。
というわけでRubyとrailsのバージョンを下げる。Rubyについてはrvmで管理しているので簡単に変更できたが、問題はrails。軽くググった感じでは、一度アンインストールしてから再度バージョン指定して入れなおせばよいという話だった。
 #gem uninstall rails #gem install rails -v 3.1.0 が、これだけで再びrake testを試すとまだエラー。Gemfile内のバージョン表記を変えろとのことだったので変更し、#bundle update。bundleについてはよくわかってないのでおいおい調べる予定。
しかし、まだエラー。エラーというより、SECURITY WARNINGとやらが表示された。どうもrails過去バージョンの脆弱性に起因するらしい。こういった事例がいくつかあり、最終的にはRails 3.2.13まで上げざるを得なかった。
popowa: SECURITY WARNING: No secret option provided to Rack::Session::Cookie と出たら
Rails emits warning: &amp;ldquo;Rack::File headers parameter replaces cache_control after Rack 1.5&amp;rdquo; - Stack Overflow
ここまででもうかなり疲れていたのだが、これでもまだ尚エラーが出るから嫌になる。Gemfileのcoffee-railsのバージョンがよろしくないらしく、以下サイトを参考にして編集。
Rails 3.1.3 -&amp;gt; 3.2.11 のアップデートで railties のdependencyが競合エラーになった - The longest day in my life
んで結論を言えばこれでもまだ上手く行かず、最終的にはRailsとRubyのバージョンを揃えた上で、イチからプロジェクトを作りなおしたらやっとエラーが出なくなった。もう自分のやってること何もかも間違ってる気がしてくる。
で、今回の一件からの考察というか帰結なのだが、技術書に沿って勉強していく場合、やはり技術書と同じバージョンの環境で進めていくべきなのだろうか。僕はそのへんわりと甘く見てしまっていたので、今回は「いや最新環境に慣れといた方がいいんじゃね？　Ruby2.0も最近出たとこで覚えといた方が得な気がするし」などという考えでRuby2.0とRails4.0.0を使っていたのだが、結果はこれである。Androidの勉強も似たような考え方でだいたい最新か、あるいは一つ前のOSで構築していたのだが、あちらは後方互換性が高いのか、それほど困ることもなかったので、Ruby on Railsでも安易に同じ事をしてしまった。
新しい環境に慣れておいた方がいい、というのはあると思う。が、技術書を読み進めていく上では、万が一バージョン相違によるエラーが出た場合、今回のように大きなロスを被ることになる。技術書、特にある分野での最初の一冊なんてのはとにかくまずは一冊流してみた方がいいわけで、最新バージョンへの対応なんてのはその後でゆっくり覚えればいいのかもしれない。旧バージョンの方がノウハウが多く蓄積されていて、エラー対処がしやすいという利点もある。とはいえ、Railsは旧バージョンを使っていると脆弱性の警告が出る場合があるので、新しいバージョンを使わざるを得ない場合もあるようなのだが。。。
あと、今回のトラブルシュートでいろいろググってみたところ、Stack Overflowが引っかかってくることが非常に多かった。やっぱ英語は必須。</description>
    </item>
    
    <item>
      <title>『RailsによるアジャイルWebアプリケーション開発』第I部 読了</title>
      <link>https://you.github.io/post/2013-07-25-post/</link>
      <pubDate>Thu, 25 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-07-25-post/</guid>
      <description> 第I部はRailsのインストールから、とりあえずウェブアプリケーションを作って公開するまで。あとRoRにおけるMVCモデルの概要と、Rubyの簡単な紹介が主。
第1章 Railsのインストール  apt-getを使って必要な依存関係をインストールしているけど、Cent OSではapt-getが使えず。yumで入れようとしたが、yumにはnodejsがないらしく入らない。。。やむを得ずwgetで落としてきて展開しました。 エディタはTextMateがオススメとあるけど、変えるのめんどいし取りあえずvimでいいか。。。  第2章 Hello, Rails!  ポート3000を使うってことでiptablesで開けてあげたりとか、そのへんは自分でやる必要があった。こういうのスムーズに出来るようにならなきゃなんだろうなぁ。 eRubyの&amp;lt;% %&amp;gt;でコード埋め込むのがなんか気持ち悪いと思ってしまう。同じ理由でJSPも苦手だった。……と思ったら、この後の章で似たようなことが書いてあった。元々HTMLとJavaScript書いたりしてたからなんだろうけど、こういうのはなるたけ分離したいと思ってしまう。  第3章 Railsアプリケーションのアーキテクチャ  ちょっと置いてけぼり感あったので、この後の章でアプリ作りながら再読する。  第4章 Ruby入門  一応Rubyの基礎はさらったので割愛しよう……と思ったら！！！　『作りながら学ぶRuby入門』より記述範囲が広かった！ そういえばあの本、例外の処理とか出てこなかったな……。もう1冊、きちんとRubyの基礎をさらえる本を読んだ方が良さそうな気がしてきた。  キーワードreturnは省略可能です。returnがない場合、最後に評価した式の結果が戻り値になります。（p.39）  はあ！？
 Rubyの文法ってなんか慣れない。。。
  </description>
    </item>
    
    <item>
      <title>Kaoriya版Vimでtxtファイルの自動改行が解除できない件</title>
      <link>https://you.github.io/post/2013-07-23-post/</link>
      <pubDate>Tue, 23 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-07-23-post/</guid>
      <description>2週間ぐらいハマってた。
プレーンテキストの管理やRuby書くときとかにKaoriya版MacVimを使っているのだが、.vimrcでset textwidth=0を設定しているにも関わらず、txtファイルのみ自動改行が解除できなくて困っていた。最近QFix_howmをよく使うようになっていて、機能としては申し分なかったんだけど、まったく関係ないこの一点のためにイライラしっぱなしだった。
で、結論。Kaoriya版Vimのデフォルト設定により、textwidthが上書かれてしまうらしい。
Kaoriya版Vimではデフォルトの設定ファイルであるvimrc_example.vimを読み込んでおり、その設定ファイル内でテキストファイルを開いたときにtextwidth=78の設定が上書きされてしまうらしい。 [Vimのtextwidth設定と.vimrc - 続・日々の雑感](http://d.hatena.ne.jp/WK6/20120606/1338993826) なので引用元の記事にもある通り、.vimrcに以下を挿入すればおｋ。
&amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;autocmd&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synType&amp;quot;&amp;gt;FileType&amp;lt;/span&amp;gt; text &amp;lt;span class=&amp;quot;synStatement&amp;quot;&amp;gt;setlocal&amp;lt;/span&amp;gt; &amp;lt;span class=&amp;quot;synPreProc&amp;quot;&amp;gt;textwidth&amp;lt;/span&amp;gt;=0  これでQFix_howm使って快適メモライフ！！！　他にもデフォルト設定いろいろ入ってそうなんで、見直した方がいいかもしれない。</description>
    </item>
    
    <item>
      <title>Ruby on Railsを学ぶ、とはどこまでの範囲を学ぶのか</title>
      <link>https://you.github.io/post/2013-07-21-post/</link>
      <pubDate>Sun, 21 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-07-21-post/</guid>
      <description>VPSを使ってRuby on Railsでウェブサービスを作るにあたり、「まずは足元から」ということでLinuxの勉強から始めてボチボチやっていたんだけど、「むしろ取りあえずアプリ動かしてみた方が良いのではないか」と思い立ち、方針を転換。Linux→DB→Railsなんてゆっくりやるより、まずは動かしてみることにした。購入したのは以下の本。
RailsによるアジャイルWebアプリケーション開発 第4版posted with amazlet at 15.03.01Sam Ruby Dave Thomas David Heinemeier Hansson オーム社 売り上げランキング: 138,293
Amazon.co.jpで詳細を見る 各所で絶賛されているのを見かけた上、立ち読みしてみた感じでもRubyのインストールという初歩から始まっていてわかりやすそうだった。候補としてはもうひとつ、インプレスの『基礎 Ruby on Rails』もあったんだけど、ザッと見た感じでは『Railsによる〜』の方が記載が細かそうだったのでこっちを選んだ。
さて、VPSでRuby on Railsを作るにあたり、どこまでの範囲を学べば良いのだろう。今回買った本だとおそらくはRailsとRuby、DBあたりのコーディングが内容の主なようで、インフラ側のチューニングの話なんぞは載っていなさそうだった。自分が想像するとこだとLinuxというかOSの準備、VPSであるならばOSに接続するためのSSHの使い方、ウェブに公開するのでそのあたりのセキュリティ、DBやApacheなどのアプリインストールなどなど、やることはかなりありそう。あとView側の問題として、Coffe ScriptやSCSSあたりもやんなくちゃいけないんだろうな。JavaScriptとCSSはそれなりに書いたことあるけど、せっかくだし挑戦してみたい。
……という感じのビジョンを描いているのだが、やはり全部まともにイチからやっていたらかなりの時間が必要だと思う。とりあえず作ってみて、あとは気になったところを徐々に手をつけていく感じでやっていきたい。あと、環境構築系だとこの本が良さそうだなーと目をつけている。
Ruby on Rails環境構築ガイドposted with amazlet at 15.03.01黒田 努 インプレスジャパン 売り上げランキング: 44,343
Amazon.co.jpで詳細を見る ……で、『Railsによる〜』はとりあえず1章読破したのだが、まさかのここでつまずくという事件発生。つまずくのはえーよｗｗｗ
CentOSにRVMを入れようとしたのだが、以下のコマンドが通らなかった。curlのエラーっぽいんだけど、今をもって解消できてない。一応他の方法でRVMは入ったので、しばらく保留にしとこうかと思っている。
 bash </description>
    </item>
    
    <item>
      <title>お勉強ブログ、はじめます</title>
      <link>https://you.github.io/post/2013-07-20-post/</link>
      <pubDate>Sat, 20 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2013-07-20-post/</guid>
      <description>SEという職に就いて2年ちょい経ったわけだが、最近はPJリーダーとか顧客との調整とかその手の仕事ばかりを回されるようになってきて、実機を触る機会がめっきり少なくなってきてる。まぁうちはいわゆる大手SIerというヤツだから、徐々に技術っぽい仕事をしなくなるのは必然、ではある。だがしかし、あまり自分はそういうキャリアを望んでいないし、さすがに3年目で管理系の道に入ってしまうのは、ちょっと早すぎる。
というわけで技術ブログである。会社で触れないなら家でやるしかない。エンジニアとして自負したいのであれば、もう精一杯足掻くしかないだろうと。指を咥えて与えられた仕事だけしてたら単なるダラダラリーマンになりそうなので、それは避けたい。そしてゆくゆくは、身につけた技術によって職を替えてもみたいのだが。
現在位置を確認する 自分はインフラ系のSEなので、日頃はスイッチのconfig書いたりVMwareでガリガリ仮装環境作ったり、そういう仕事が多い。ちなみに顧客環境の都合で業務上Linuxに触れた経験がないのは内緒である。
というわけで、自力で学ぶのは会社から離れた方向。アプリ系を中心に据えていきたい。これまで自分が納めた言語はJava、それもわりと簡単なことしかできていないので、Javaの腕を磨くことと、もう一言語使えるようになることを目標にしたい。
とはいえ現在の業務も重要ではあるので、同時にインフラ周りも学べるような道が吉。
これから行く先を定める てわけで、当面の目標は二つ。
 VPSを借りて、Ruby on Railsで動くウェブサービスをつくる Androidアプリをなんかつくる  Javaに加える「もう一言語」はRubyにした。最近流行っているようだし、出来る事の幅も広そう。Pythonにも興味は惹かれたが、まあとりあえずRubyで。
そしてVPS。Ruby on RailsだとHerokuあたりで建てるのが主流みたいなんだけど、勉強も兼ねて足回りから全部自分で構築する。CentOSをいじって、ApacheとかDBとか入れてガリガリつくる。これでインフラ系SEとしての勉強も一応できるはず。
2番目の目標の方はサブ。1個の目標だけずーっと続けるのもなかなかしんどそうなので、息抜きとしてAndroidアプリをいじりたい。作りたいもののイメージとしては、Tumblrの過去ログを漁りやすくするようなアプリが出来たらいいなーと思っている。
ブログをどう活用するか と、ここまで書いておいてなんだが、実はすでに勉強は始めている。本を読みながら作ったりなんだりとしていたのだが、どうにも身になっている気がしなかったので、アウトプットの場としてブログを始めることにした次第。
はじめはEvernoteとかqfix_howmあたりに勉強記録をつければいいかと思ったが、それだとモチベーションが上がりにくいし、整形もしない気がしたのでブログにしてみた。はてブロならMarkdownで書けるからちゃんとフォーマットも考えつつ書けそうだし、参考文献へのリンクの処理とかも楽。んでさすがに本家の方とは内容がかけ離れすぎてるので、別ブログとして開設。まぁひっそりとやっていきます。
書く内容としては、何を学んだのか、何がわかったのか、わからなかった点はどこで、次回はどうするのか、ということを1日の勉強が終わったときに書き留めていきたい。特に「次回はどうするのか」がすげー重要だと個人的には思っていて、本を読んだら読みっぱなしではなく、「この本ではこのあたりのことがよくわからなかったから、次はこういう本を買おう」というビジョンが必要。そういうビジョンを持ちながら勉強するためにも、学んだ内容をアウトプットして整理する必要があると思っている。
あ、あとは「うたうとき」の方でたまに書いてたライフハック的というか、情報整理環境みたいのについての記事もこっちに回してくるつもり。そういうのってこだわりすぎてもいけないんだけど、ある程度整備しとかないと色々支障をきたすものだとも思っている。
次回以降、勉強ブログとして書き進めていきます。ちなみに現時点までの記録として、以下に開発環境と読んだ本をば。
これまでの足跡 開発環境  メインマシン
 iMac late 2010
 Mountain Lion
 Java : Eclipse 4.2
 Ruby : vim（良いIDEとかあるなら教えてくだしあ＞＜）
 Editor : vim（出来ること多すぎてハマり気味。精神衛生上良くない）
 サブマシン（いずれもVMware fusion上）
 Cent OS 6.4（Linux勉強用）
 Windows 8（ほぼ使ってない。一応持ってるWindows）
 流行りのvagrantなんかも触ってみたいですね！
  これまで読んだ本  Android  楽しみながら作ってみよう! はじめてのAndroidアプリプログラミングposted with amazlet at 13.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://you.github.io/post/2016-05-14-pepabo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://you.github.io/post/2016-05-14-pepabo/</guid>
      <description>100行合ったmod_rewirteっをngx_mrubyで書き換えた話 @buty4649 mod_rewriteがかなり複雑になっていてルーティング追加時のデバッグが困難だった nginxとapache2とapache1が役割違うけど同居、apache2がc10kでボトルネックに nginxに等号しようとしたけどifのネストができないしconfigが長くなる。。。 ngx_mruby nginx.conf内でmrubyによる拡張が出来る Middleware as code テストが出来る
Nyah ペパボのプライベートクラウド OpenStack 4象限のペパボの技術的なロードマップええなあ 86%のサービスで使っている 558インスタンス（うおー） 開発メンバーがインスタンス起動やプロビジョニングをポチポチ行える Mackerelでモニタリング コストダウン（イニシャル、ランニングとも）にもつながっている =&amp;gt;移行前後で測っておいて比較したい アラート等の障害も減った but バージョンアップが半年ごとにあるので追随大変（できてない） 機能を有効に使い切れていない部分がある。 nyah-cli DVR バージョンアップテストは？ Terraformが通ればOKでは
高集積ホスティングによる略 ロリポ 高集積にこだわる リソース制御の自動化 mod_mruby mod_cgroup
インフラCI alotofwe アプリのCI =&amp;gt; drone.io（コンテナ利用のCI puppet serverspec docker-in-docker
linuxユーザー管理 pyama ldap? /etc/passwdの構成管理ツールによる配布？ rpm? deb? ldap中身が複雑に成る 連携の複雑化 アプリケーションデプロイもsshでやるからユーザーが増えていく stns simple toml name service Golangだ 名前解決、更改鍵取得、sudo認証だけやる deploy用ユーザーの運用「deploy」 公開鍵を並べる運用、面倒では =&amp;gt; link_usersで解決できる 組織構造の表現も出来る =&amp;gt; link_groups GH:Eからデータ取ってtomlのユーザー情報つくる GitHub Flowでユーザー管理 APIでいろんなDBとかから直接ユーザー取ってくるようにしたい passwdバックエンドを用意したい</description>
    </item>
    
  </channel>
</rss>